{"paper_id": "00b4046cfc7f94d35e4fcf5af4cac34323812034", "metadata": {"title": "RELATION/ENTITY-CENTRIC READING COMPREHENSION", "authors": [{"first": "Takeshi", "middle": [], "last": "Onishi", "suffix": "", "affiliation": {"laboratory": "iii iii RELATION/ENTITY-CENTRIC READING COMPREHENSION", "institution": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO", "location": {"settlement": "Chicago, Illinois"}}, "email": ""}, {"first": "David", "middle": [], "last": "Mcallester", "suffix": "", "affiliation": {"laboratory": "iii iii RELATION/ENTITY-CENTRIC READING COMPREHENSION", "institution": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO", "location": {"settlement": "Chicago, Illinois"}}, "email": ""}, {"first": "Yutaka", "middle": [], "last": "Sasaki", "suffix": "", "affiliation": {"laboratory": "iii iii RELATION/ENTITY-CENTRIC READING COMPREHENSION", "institution": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO", "location": {"settlement": "Chicago, Illinois"}}, "email": ""}, {"first": "Kevin", "middle": [], "last": "Gimpel", "suffix": "", "affiliation": {"laboratory": "iii iii RELATION/ENTITY-CENTRIC READING COMPREHENSION", "institution": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO", "location": {"settlement": "Chicago, Illinois"}}, "email": ""}, {"first": "Makoto", "middle": [], "last": "Miwa", "suffix": "", "affiliation": {"laboratory": "iii iii RELATION/ENTITY-CENTRIC READING COMPREHENSION", "institution": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO", "location": {"settlement": "Chicago, Illinois"}}, "email": ""}]}, "abstract": [], "body_text": [{"text": "In Chapter 1, we overview the history of the reading comprehension tasks and various styles of the tasks. We also differentiate the reading comprehension tasks from other question answering tasks. Then, we present entities and their relations in the context of reading comprehension tasks.", "cite_spans": [], "ref_spans": [], "section": ""}, {"text": "In Chapter 2, we present an original reading comprehension dataset. We used baseline systems and a sampling approach to control the difficulty of the dataset. As a result, the dataset achieved a high human performance and low machine performance, and the gap indicates the dataset provides questions that requires deep understanding of texts.", "cite_spans": [], "ref_spans": [], "section": ""}, {"text": "In Chapter 3, we analyze neural network models for reading comprehension tasks and iv show that the vector representations learned in the models can be understood as being composed of a predicate applied to entities.", "cite_spans": [], "ref_spans": [], "section": ""}, {"text": "In Chapter 4, we apply our findings in Chapter 3 to another reading comprehension dataset focusing on entities and their relations. We propose a transformer encoder-based model and show that the model achieves the higher development accuracy than other models with a similar number of parameters.", "cite_spans": [], "ref_spans": [], "section": ""}, {"text": "In Chapter 5, we present our work on relation extraction, a task for predicting a relation My special thank goes to the late Mr. Tatsuro Toyoda, the found of Toyota Technological Institute. I really appreciate the opportunity to study in Toyota Technological Institute at Chicago and Japan, and it is a great honor for me to be the first student to graduate from both Toyota Technological Institute in the world. Hermann et al. [1] , results marked II are from Chen et al. [2] , result marked III is from Kadlec et al. [3] , and result marked IV is from Dhingra et al. [4] . 34 Table 3 .2 Accuracy on Who-did-What dataset. Each result is based on a single model. Results for neural readers other than NSE are based on replications of those systems. All models were trained on the relaxed training set which uniformly yields better performance than the restricted training set. The first group of models are explicit reference models and the second group are aggregation models. + indicates anonymization with better reference identifier. . 49 ", "cite_spans": [{"start": 428, "end": 431, "text": "[1]", "ref_id": "BIBREF12"}, {"start": 473, "end": 476, "text": "[2]", "ref_id": null}, {"start": 519, "end": 522, "text": "[3]", "ref_id": null}, {"start": 569, "end": 572, "text": "[4]", "ref_id": null}], "ref_spans": [{"start": 578, "end": 585, "text": "Table 3", "ref_id": "TABREF1"}], "section": ""}, {"text": "In many areas of engineering, it is our dream to create a machine that is more productive than a human, and can tolerate working for a longer time and that releases workers from tedious tasks. In Artificial Intelligence (AI) we seek a machine that has intelligence of humans.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Here, intelligence might include the ability to understand images, understand speech, and read texts.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "The ability to read texts is studied in Natural Language Processing (NLP), a field of study to process natural language texts, and its ultimate goal is to create a machine that understands natural language texts. Although the ability is essential for the desired machine to communicate with humans like workers do, understanding texts is not a well-defined goal, and it is nontrivial to verify the ability.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "A legacy approach for verifying the ability is the Turing test [6] where a tester talks with a machine or human, and we see whether the tester can reliably tell the machine from the human or not. Although the test setting is convincing, there are two practical issues that we are concerned about. First, the test cannot compare the intelligence of two given machines.", "cite_spans": [{"start": 63, "end": 66, "text": "[6]", "ref_id": null}], "ref_spans": [], "section": "Introduction"}, {"text": "The test verifies if each machine has the intelligence or not and each machine makes a fairly independent conversation for each other, thus it is difficult to compare these test results.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "1 Second, the test only verifies the existence of the intelligence, and it does not help to explain how the machine understands given texts.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "A practical approach might be reading comprehension tasks, where machines answer a question about a given passage rather than making a conversation. Here it is important that answering the question requires information described in the passage. So we can see how much a machine understands the given passage by observing the answer the machine makes. In this setting, we can compare the abilities of these machines by simply counting the number of correct answers given by each machine.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Additionally, we are also interested in how the information in texts is represented in the machines, especially deep neural network models that are notoriously difficult to interpret.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "We challenge this question with focusing entities and their relations described in the texts and show here that the vectors of neural readers can be decomposed into a predicate and entities.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Thus, this dissertation shows studies of these reading comprehension tasks focusing on entities and relations. We believe that understanding how machines take care of entities and their relations in a given passage helps further the study of machine reading comprehension.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Then eventually, this study contributes to the ultimate goals of AI.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "A machine that understands human language is the ultimate goal of NLP. Understanding is a nontrivial concept to define; however, the NLP community believes it involves multiple aspects and has put decades of effort into solving different tasks for the various aspects of text understanding, including:", "cite_spans": [], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "Syntactic aspects:", "cite_spans": [], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "\u2022 Part-of-speech tagging: This is a task to find a syntactic rule for each token in a sentence. Each token is identified as a noun, verb, adjective, etc. Figure 1.1 shows an example of part-of-speech tagging.", "cite_spans": [], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "\u2022 Syntactic parsing: This is a task to find syntactic phrases in a sentence such as a noun phrase, verb phrase. Figure 1. 2 shows an example of syntactic parsing.", "cite_spans": [], "ref_spans": [{"start": 112, "end": 121, "text": "Figure 1.", "ref_id": "FIGREF3"}], "section": "Reading Comprehension"}, {"text": "\u2022 Dependency parsing: Dependency is a relation between tokens where a token modifies another token. Dependency parsing is a task to find all dependencies in a sentence. Semantic aspects:", "cite_spans": [], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "\u2022 Named entity recognition: This is a task to find named entities and their types in a sentence. Typical named entity types are \"Person\" and \"Location\". \u2022 Coreference resolution: This is a task to collect tokens that refer to the same entity. For example, Donald Trump can be referred by \"he\", \"Trump\" or \"the president.\" Figure   1 .5 shows an example of coreference resolution.", "cite_spans": [], "ref_spans": [{"start": 322, "end": 332, "text": "Figure   1", "ref_id": "FIGREF3"}], "section": "Reading Comprehension"}, {"text": "A reading comprehension task is a question answering task that is designed for testing all these aspects and probe even deeper levels of understanding. (1) Robbie Keane and (2) Dimitar Berbato, in the passage with named entity recognition and coreference resolution, and then it might find \"Dimitar Berbato\" is the best answer. then returns an answer. Hence, a supervised training instance is a tuple of a passage,", "cite_spans": [], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "question, and answer. The passage is a text resource that provides enough information to find the answer, such as a news article, encyclopedia article, or multiple paragraphs of these articles. The question is also a text resource, but it is much shorter than the passage. The answer style is different depending on the style of each reading comprehension task. Here, we divide existing reading comprehension tasks into three styles depending on their answer type.", "cite_spans": [], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "\u2022 Multiple choice: In this style, a list of candidate answers is given along with each passage and question. Hence the answer is one of the candidate answers. On the example The performance of a machine is evaluated by the accuracy; the number of correct answers over the number of all questions.", "cite_spans": [], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "\u2022 Span prediction: In this style, the answer is a span in the passage, i.e., the answer is a pair of a start token and end token. This style is also referred to as extractive question answering. On the example question in Table 1.2, there are two occurrences of COVID-19 in the passage, but the answer is the second one.", "cite_spans": [], "ref_spans": [{"start": 222, "end": 258, "text": "Table 1.2, there are two occurrences", "ref_id": "TABREF0"}], "section": "Reading Comprehension"}, {"text": "The performance of a machine is evaluated by span-level accuracy by exact matching (EM) and/or an F1 score. EM is the same as the accuracy where the predicted span is correct if and only if the sequence of words specified by the predicated span is the same as the sequence of words specified by the gold span. This matching scheme might be called string matching. The F1 score is a harmonic mean of precision and recall that are computed between the bag of tokens in the predicted span and the bag of tokens 6 in the gold span.", "cite_spans": [], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "where P and G are the bag of tokens in the predicted span and that in the gold span, respectively.", "cite_spans": [], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "\u2022 Free-form answer: In this style, the answer can be any sequence of words in a vocabulary; thus, a machine generates the sequence to answer the given question. On the example question in Table 1 The goal of other question answering tasks is to appropriately answer questions posed by humans, and reading comprehension skills are less considered. Thus the machine may use any kind of information resources, including structured knowledge such as knowledge bases and unstructured knowledge texts such as encyclopedias, dictionaries, news articles, and Web texts. Additionally, the unstructured knowledge texts are longer than a passage and typically web-scale. These information resources require less reading comprehension skills described in Chapter 1. For example, given an access to a large text corpus, a simple grammatical transformation and string matching will likely suffice to answer the question like \"who is the president of the U.S.\" Here the question can be grammatically transformed into a declarative sentence, \"*** is the president of the U.S.\" Then, the machine more likely finds a sentence that matches the declarative sentence.", "cite_spans": [], "ref_spans": [{"start": 188, "end": 195, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Reading Comprehension"}, {"text": "On the other hand, the goal of reading comprehension is to understand a given (short)", "cite_spans": [], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "text. Thus a machine uses unstructured knowledge texts only. The texts are typically short and carefully written so that they require more reading comprehension skills. For example, 8 multiple given passages might share some information. Such shared information is called world knowledge, and some machines might be able to answer a question correctly without reading the given passage but using the world knowledge written in other passages. Hence, this issue makes it difficult to tell if the machine has reading comprehension skills. To avoid this issue, early work in this field mostly focused on fictional stories [13] because each fictional story has different characters and stories and then unlikely shares information.", "cite_spans": [{"start": 619, "end": 623, "text": "[13]", "ref_id": "BIBREF26"}], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "Early study [14] describes this difference by using terms; micro-reading/macro-reading.", "cite_spans": [{"start": 12, "end": 16, "text": "[14]", "ref_id": "BIBREF27"}], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "Macro-reading is a task where the input is a large text collection, and the output is a large collection of facts expressed by the text collection, without requiring that every fact be extracted. Micro-reading is a task where a single text document is input, and the desired output is the full information content of that document.", "cite_spans": [], "ref_spans": [], "section": "Reading Comprehension"}, {"text": "Reading comprehension question answering is not new, and we can find early work from the 1970s. In this section, we review the history of three paradigms: development of the theory, rule-based systems, and deep learning systems.", "cite_spans": [], "ref_spans": [], "section": "History"}, {"text": "Very early systems operate in very limited domains in the 1970s. For example, SHRDLU [15] is a computer program where a user can move some objects in a 3D computer graphic by using English. LUNAR [16] is another computer program that answers questions about lunar geology and chemistry, and Baseball [17] is for questions about baseball.", "cite_spans": [{"start": 85, "end": 89, "text": "[15]", "ref_id": "BIBREF28"}, {"start": 196, "end": 200, "text": "[16]", "ref_id": "BIBREF29"}, {"start": 300, "end": 304, "text": "[17]", "ref_id": "BIBREF30"}], "ref_spans": [], "section": "History"}, {"text": "One of the most notable early work in the 1970s might be the QUALM system [13] . The work proposed a conceptual theory to understand the nature of question answering. Here the work analyzed how humans classify questions, and the algorithm classified questions in a similar way that humans do.", "cite_spans": [{"start": 74, "end": 78, "text": "[13]", "ref_id": "BIBREF26"}], "ref_spans": [], "section": "History"}, {"text": "In the 1980s to 1990s, various rule-based systems were proposed for each domain. Here we describe a notable shared task and dataset. The dataset was proposed by Hirschman et al. [18] and consists of 60 stories for development and 60 stories for testing of 3rd to 6th grade material, and each story is followed by short-answer questions, i.e., who, what, when,", "cite_spans": [{"start": 178, "end": 182, "text": "[18]", "ref_id": "BIBREF31"}], "ref_spans": [], "section": "9"}, {"text": "where and why questions. In the task, a machine takes each story and question then finds a sentence in the story that most likely contains the answer key. Multiple rule-based systems were developed for this task. Deep Read [18] takes a bag-of-words approach with shallow linguistic processing, including stemming, name identification, semantic class identification, and pronoun resolution. QUARC [19] uses lexical and semantic correspondence, and then", "cite_spans": [{"start": 223, "end": 227, "text": "[18]", "ref_id": "BIBREF31"}, {"start": 396, "end": 400, "text": "[19]", "ref_id": "BIBREF32"}], "ref_spans": [], "section": "9"}, {"text": "Charniak et al. [20] combines them. As the results, these systems achieved 30-40% accuracy,", "cite_spans": [{"start": 16, "end": 20, "text": "[20]", "ref_id": null}], "ref_spans": [], "section": "9"}, {"text": "i.e., these systems correctly predict a sentence containing the answer for 30-40% of questions.", "cite_spans": [], "ref_spans": [], "section": "9"}, {"text": "From the 2010s, supervised learning models significantly improved their performance in various tasks, including reading comprehension tasks. Even some supervised learning models overcame human performance in some tasks [2] . These improvements were made by deep neural networks and large-scale datasets.", "cite_spans": [{"start": 219, "end": 222, "text": "[2]", "ref_id": null}], "ref_spans": [], "section": "9"}, {"text": "A deep neural network is a scalable machine learning model. A deep neural network is typically composed of \"units\". Each unit takes an input vector x and returns an output vector y by using a linear and non-linear transformation as the following.", "cite_spans": [], "ref_spans": [], "section": "9"}, {"text": "where W is a matrix, b is a bias vector, and f is a non-linear function. The deep neural network is trained by a stochastic gradient descent algorithm where a loss is computed on a subset of training instances, and then the gradient of the loss is computed with respecting the parameters of the deep neural network. Hence, the parameters are updated to the direction of the gradient.", "cite_spans": [], "ref_spans": [], "section": "9"}, {"text": "where L is the loss function to be minimized, X is a subset of training instances called mini-batch, and \u03b8 is the parameters. The stochastic gradient algorithm takes linear time against the size of the training data, and the memory requirement is linear to the size of the mini-batch. Thus neural network models can learn any large-scale training data in linear time by the stochastic gradient algorithm.", "cite_spans": [], "ref_spans": [], "section": "9"}, {"text": "Larger training data provides more instances to learn, hence scaling up training data is believed to be a promising approach in machine learning. Here, we note the contribution of the World Wide Web (WWW) to the large-scale training data. The WWW is an information system over the Internet where a document or web resource is identified by a Uniform", "cite_spans": [], "ref_spans": [], "section": "9"}, {"text": "Resource Locators (URL). People uploads various kinds of texts on the WWW, including news articles, blog articles, and encyclopedia articles. The amount of these texts on the WWW was estimated as at least 320 million pages in 1998 [21] , and it is estimated as at least billions in 2016 [22] . Naturally, these texts are computer-readable, unlike texts on books, and some of them are copy-right free. Hence we find the text on the WWW as a large accessible text resource. Recently, the WWW is a major resource of multiple standard reading comprehension datasets including, SQuAD, Wikihop, HotpotQA [23, 5, 24] .", "cite_spans": [{"start": 231, "end": 235, "text": "[21]", "ref_id": "BIBREF35"}, {"start": 287, "end": 291, "text": "[22]", "ref_id": "BIBREF36"}, {"start": 598, "end": 602, "text": "[23,", "ref_id": "BIBREF37"}, {"start": 603, "end": 605, "text": "5,", "ref_id": null}, {"start": 606, "end": 609, "text": "24]", "ref_id": "BIBREF38"}], "ref_spans": [], "section": "9"}, {"text": "Thanks to the large-scale dataset supported by the WWW and the scalable training algorithm, deep neural network models can learn significantly large information on the dataset.", "cite_spans": [], "ref_spans": [], "section": "9"}, {"text": "As a result, these models perform better and better, and then their performances are achieving the human performance in some tasks [2] .", "cite_spans": [{"start": 131, "end": 134, "text": "[2]", "ref_id": null}], "ref_spans": [], "section": "9"}, {"text": "The significant success of deep learning raises two questions.", "cite_spans": [], "ref_spans": [], "section": "9"}, {"text": "\u2022 \"What is a good question in reading comprehension tasks?\"", "cite_spans": [], "ref_spans": [], "section": "9"}, {"text": "\u2022 \"How do these machines understand texts?\"", "cite_spans": [], "ref_spans": [], "section": "9"}, {"text": "Questions in reading comprehension tasks are designed for testing reading comprehension skills, and each question requires these skills to solve. Today, as the deep neural network models perform better and better, we are more and more interested in more complicated reading comprehension skills that are beyond NER, coreference resolution, and dependency parsing. Additionally, we need to feed millions of such questions to train the deep neural network models, and it is not realistic for us to write each question manually. To address the problem and provide millions of such comprehension questions, we take a sampling approach in Chapter 2.", "cite_spans": [], "ref_spans": [], "section": "9"}, {"text": "Early systems were rule-based, and the mechanism of their text-understanding is relatively explainable. For example, if a machine reads a given text by operating rules designed by a researcher, then the process can be explained by a sequence of rules that the machine used. This sequence explains how the machine understands the given texts. On the other hand, deep neural network models operate multiple vector transformations, and each transformation does not explicitly correlate with any grammatical/semantic rules. Thus, unlike rule-based systems, the sequence of these operations does not explain enough how the machine understands the given text. We claim that entities and their relations can be a key to explainability in Section 1.2. Then, we empirically analyze how neural network models understand texts by using entities and their relations in Chapter 3, and apply it to our novel neural reader in Chapter 4. In Chapter 5, we extracted these entities and relations and visualize them for materials science.", "cite_spans": [], "ref_spans": [], "section": "9"}, {"text": "We are interested in entities and their relations in the context of reading comprehension.", "cite_spans": [], "ref_spans": [], "section": "Entity and Relation"}, {"text": "In the following, we overview entities and their relations in the context of knowledge bases.", "cite_spans": [], "ref_spans": [], "section": "Entity and Relation"}, {"text": "Then, we describe reading comprehension datasets focusing on entities and relations, and also relation extraction from the point of view of reading comprehension.", "cite_spans": [], "ref_spans": [], "section": "Entity and Relation"}, {"text": "Entities and their relations are well studied in the context of knowledge bases. A knowledge base such as WordNet [25] or Wikidata [26] is a structured database that typically represents its information by using entities and their relations as Fig.1 .6 shows the relations around \"John McCormick\". Here, entities and their relation are defined for the information desired to be represented. Quine [27] stated that \"To be assumed as an entity is [...] to be reckoned as the value of a variable\" or \"to be is to be the value of a variable \". Hobbs [28] , inspired by Quine, limited entity types to \"physical object, numbers, sets, times, possible worlds, propositions, events\". Naturally, their relations are also designed for the target information.", "cite_spans": [{"start": 114, "end": 118, "text": "[25]", "ref_id": "BIBREF39"}, {"start": 131, "end": 135, "text": "[26]", "ref_id": "BIBREF40"}, {"start": 397, "end": 401, "text": "[27]", "ref_id": "BIBREF41"}, {"start": 546, "end": 550, "text": "[28]", "ref_id": "BIBREF42"}], "ref_spans": [{"start": 244, "end": 249, "text": "Fig.1", "ref_id": "FIGREF3"}], "section": "Entity and Relation"}, {"text": "Entities and their relations are critical to solve questions in some reading comprehension question answering tasks. For example, each answer of CNN/Daily Mail dataset [1] is an entity that satisfies the condition given by the question sentence. The dataset is Clozestyle, where each question is a sentence whose key entity is blanked out. Here the question asks to find the blanked entity from the given passage. In other cases, each question of Wikireading [9] and Wikihop [5] consists of an entity and/or relation. In Wikihop, each question is a pair of a subject entity and relation, and the answer is an object entity that has the relation with the subject entity. In Wikireading, each question is a relation and the passage describes a subject entity and the answer is an object entity that has the relation with the subject entity described in the passage. We also consider question answering tasks whose answers are relations. These tasks are studied in the context of relation extraction in knowledge base population described in Section 1.2.1. Relation extraction is a task for finding a relation between two given entities described in a text resource. It is worth noting that the task is different from relation classification. Relation classification is a task for finding a relation between two given entities described in a given text resource (typically a sentence) where the positions of these entities are given. On the other hand, the positions are not given in relation extraction, and the text resource is typically longer than a single sentence. Thus, the task can be viewed as another reading comprehension task focusing entities and relations in the text.", "cite_spans": [{"start": 168, "end": 171, "text": "[1]", "ref_id": "BIBREF12"}, {"start": 459, "end": 462, "text": "[9]", "ref_id": null}, {"start": 475, "end": 478, "text": "[5]", "ref_id": null}], "ref_spans": [], "section": "Entity and Relation"}, {"text": "Entities and relations are critical for these tasks; however, we believe that such entities and their relations are critical, not only for these datasets but also for other datasets that implicitly require a machine to understand entities and their relations.", "cite_spans": [], "ref_spans": [], "section": "Entity and Relation"}, {"text": "In this section, we briefly overview how knowledge bases help various tasks, including question answering and information retrieval, and the motivation of knowledge base population, a task to fill a knowledge base from texts.", "cite_spans": [], "ref_spans": [], "section": "Knowledge base population"}, {"text": "A knowledge base is often a critical component of an expert system. An expert system 14 is typically composed of inference rules written by hand and a knowledge base and emulates the decision-making ability of a human expert. As it is sometimes difficult for the human expert to explain his/her decision, it is difficult to design complicated inference rules, but it might be easier to add more knowledge to the knowledge base. The performance of each system heavily depends on the coverage of its knowledge base.", "cite_spans": [], "ref_spans": [], "section": "Knowledge base population"}, {"text": "Today, some large-scale knowledge bases are available, e.g., Freebase and Wikidata. Freebase started as a collaborative knowledge base whose data was accumulated by its community members. Freebase consists of 125M tuples of a subject entity, object entity, and their relation, whose topics spread over 4K types, including people, media, and locations [29, 30] .", "cite_spans": [{"start": 351, "end": 355, "text": "[29,", "ref_id": "BIBREF43"}, {"start": 356, "end": 359, "text": "30]", "ref_id": "BIBREF44"}], "ref_spans": [], "section": "Knowledge base population"}, {"text": "Wikidata is also a collaborative knowledge base consisting of 87M entities 1 and most of these entities are linked to entities in sister projects such as Wikipedia; thus, it can provide extra information about these entities. Such large-scale knowledge bases help various tasks, including information retrieval and question answering, but still, the coverage of the knowledge base is critical for the performance.", "cite_spans": [], "ref_spans": [], "section": "Knowledge base population"}, {"text": "Despite the efforts of the community members who are maintaining these knowledge bases, their sizes are far from sufficient because new knowledge is emerging rapidly. On the other hand, we are more likely able to access textual information describing the new knowledge. Thus, we study knowledge base population to feed the knowledge base from texts.", "cite_spans": [], "ref_spans": [], "section": "Knowledge base population"}, {"text": "Entity-centered reading comprehension dataset Researchers distinguish the problem of general knowledge question answering from that of reading comprehension [1, 31] as descibed in Section 1.1.2. Reading comprehension is more difficult than knowledge-based or Information Retrieval (IR)-based question answering in two ways. First, reading comprehension systems must infer answers from a given unstructured passage rather than structured knowledge sources such as Freebase [29] or the Google Knowledge Graph [32] . Second, reading comprehension systems cannot exploit the large level of redundancy present on the web to find statements that provide a strong syntactic match to the question [33] . In contrast, a reading comprehension system must use the single phrasing in the given passage, which may be a poor syntactic match to the question.", "cite_spans": [{"start": 157, "end": 160, "text": "[1,", "ref_id": "BIBREF12"}, {"start": 161, "end": 164, "text": "31]", "ref_id": "BIBREF45"}, {"start": 472, "end": 476, "text": "[29]", "ref_id": "BIBREF43"}, {"start": 507, "end": 511, "text": "[32]", "ref_id": "BIBREF46"}, {"start": 689, "end": 693, "text": "[33]", "ref_id": "BIBREF47"}], "ref_spans": [], "section": "Chapter Two"}, {"text": "In this chapter, we describe the construction of a new reading comprehension dataset that we refer to as Who-did-What (WDW) [7] . Two typical examples are shown in Question: Sources close to the presidential palace said that Fujimori declined at the last moment to leave the country and instead he will send a high level delegation to the ceremony, at which Chilean President Eduardo Frei will pass the mandate to ***. the first sentence of the question article. An information retrieval system is then used to select a passage with high overlap with the first sentence of the question article, and an answer choice list is generated from the person named entities in the passage.", "cite_spans": [{"start": 124, "end": 127, "text": "[7]", "ref_id": null}], "ref_spans": [], "section": "Chapter Two"}, {"text": "Our dataset differs from the CNN/Daily Mail dataset [1] in that it forms questions from two distinct articles rather than summary points. This allows problems to be derived from document collections that do not contain manually-written summaries. This also reduces the syntactic similarity between the question and the relevant sentences in the passage, increasing the need for deeper semantic analysis.", "cite_spans": [{"start": 52, "end": 55, "text": "[1]", "ref_id": "BIBREF12"}], "ref_spans": [], "section": "Chapter Two"}, {"text": "To make the dataset more challenging we selectively remove problems so as to suppress four simple baselines -selecting the most mentioned person, the first mentioned person, and two language model baselines. This is also intended to produce problems requiring deeper semantic analysis.", "cite_spans": [], "ref_spans": [], "section": "Chapter Two"}, {"text": "The resulting dataset yields a larger gap between human and machine performance than existing ones. Humans can answer questions in our dataset with an 84% success rate compared to the estimates of 75% for CNN [2] and 82% for the CBT named entities task [31] . In spite of this higher level of human performance, various existing readers perform significantly worse on our dataset than they do on the CNN dataset. For example, the Attentive Reader [1] achieves 63% on CNN but only 55% on WDW and the Attention Sum Reader [3] achieves 70% on CNN but only 59% on WDW.", "cite_spans": [{"start": 209, "end": 212, "text": "[2]", "ref_id": null}, {"start": 253, "end": 257, "text": "[31]", "ref_id": "BIBREF45"}, {"start": 447, "end": 450, "text": "[1]", "ref_id": "BIBREF12"}, {"start": 520, "end": 523, "text": "[3]", "ref_id": null}], "ref_spans": [], "section": "Chapter Two"}, {"text": "In summary, we believe that our WDW is more challenging, and requires deeper semantic analysis.", "cite_spans": [], "ref_spans": [], "section": "Chapter Two"}, {"text": "Our WDW is related to several datasets for machine comprehension. In this section, we review notable reading comprehension datasets since the 1990s including dataset developed after our WDW.", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "The Deep Read dataset [18] is an outstanding early work on reading comprehension dataset. The dataset consists of 60 development and 60 test simulated news stories of 3rd to 6th grade material. Each story is followed by short-answer 5W questions; who, what, when,", "cite_spans": [{"start": 22, "end": 26, "text": "[18]", "ref_id": "BIBREF31"}], "ref_spans": [], "section": "Related work"}, {"text": "where, and why questions, as a sample on Table 2 Question2: What is the name of our national library?", "cite_spans": [], "ref_spans": [{"start": 41, "end": 48, "text": "Table 2", "ref_id": "TABREF7"}], "section": "Related work"}, {"text": "Question3: When did this library burn down?", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "Question4: Where can this library be found?", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "Question5: Why were some early people called \"men of the written tablets\"? Question1: What is the name of the trouble making turtle?", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "Candidate answers: a)Fries, b)Pudding, c)James, d)Jane", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "Question2: What did James pull off of the shelves in the grocery store?", "cite_spans": [], "ref_spans": [], "section": "Answer1: (c)James"}, {"text": "Candidate answers: a)pudding, b)fries, c)food, d)splinters", "cite_spans": [], "ref_spans": [], "section": "Answer1: (c)James"}, {"text": "Answer2: (a)pudding by seven year old children. These fictional stories and questions were written by Amazon", "cite_spans": [], "ref_spans": [], "section": "Answer1: (c)James"}, {"text": "Mechanical Turk cloud workers. Although they claim that their cloud sourcing approach is scalable, this dataset is too small to train models for the general problem of reading comprehension.", "cite_spans": [], "ref_spans": [], "section": "Answer1: (c)James"}, {"text": "The bAbI synthetic question answering dataset [8] contains passages describing a series of actions in a simulation followed by a question. For this synthetic data a logical algorithm can be written to solve the problems exactly (and, in fact, is used to generate ground truth 20 answers whose answer is a span of text in the given document. A sample question is given in Table   2 . 6 . Questions and answer spans are written by cloud workers. In the dataset construction, a cloud worker writes five questions, and their answer spans for each passage that is a paragraph of a Wikipedia article whose length is shorter than 500 characters. In addition 22", "cite_spans": [{"start": 46, "end": 49, "text": "[8]", "ref_id": null}, {"start": 383, "end": 384, "text": "6", "ref_id": null}], "ref_spans": [{"start": 371, "end": 380, "text": "Table   2", "ref_id": "TABREF7"}], "section": "Answer1: (c)James"}, {"text": "Passage: ... a small aircraft carrying @entity5 , @entity6 and @entity7 the @entity12 @entity3 crashed a few miles from @entity9 , near @entity10 , @entity11 ...", "cite_spans": [], "ref_spans": [], "section": "Answer1: (c)James"}, {"text": "Candidate answers: 1)entity1, 2)entity2, 3)entity3, ... . Short, intense periods of rain in scattered locations are called \"showers\"...", "cite_spans": [], "ref_spans": [], "section": "Question: pilot error and snow were reasons stated for @placeholder plane crash"}, {"text": "Question2: What is another main form of precipitation besides drizzle, rain, snow, sleet and hail?", "cite_spans": [], "ref_spans": [], "section": "Question1: What causes precipitation to fall?"}, {"text": "Question3: Where do water droplets collide with ice crystals to form precipitation? to the answer span, two other cloud workers are given the passage and question only and predict the answer span. Thus, each question has at most three gold answer spans. The evaluation metric is EM and F1. Here F1 is computed between a bag of tokens in a gold answer span and a bag of tokens in the predicted span.", "cite_spans": [], "ref_spans": [], "section": "Question1: What causes precipitation to fall?"}, {"text": "dataset with the aspect of macro-reading. The dataset consists of 100K questions sampled from user queries issued to a search engine. Each question comes with a passage, which is a set of approximately ten web-pages that are retrieved by an information retrieval system. These 23 questions and passages make the task more like a general question answering task rather than a reading comprehension task. Firstly, the passage is longer than that in other datasets whose passage is a paragraph or a news article. Secondly, it is unclear if answering these questions based on web-queries require the reading comprehension skills, e.g., we generally make a web-query by using keywords rather than a question sentence to help keyword matching.", "cite_spans": [], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "These aspects make these questions more likely to be solved by syntactic matching.", "cite_spans": [], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "TriviaQA [36] is another reading comprehension dataset with the aspect of Macro-reading.", "cite_spans": [{"start": 9, "end": 13, "text": "[36]", "ref_id": "BIBREF50"}], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "The dataset consists of 96K questions and 663K evidence documents. These questions and their answers are from 14 trivial and quiz-league websites. The answer type is free-form answer, and the evaluation metrics are EM and F1 as following SQuAD. The evidence document is a passage in our context and collected from web-pages and Wikipedia articles by using a Web search engine. Hence, it is worth noting that each question has multiple evidence documents to read, unlike SQuAD where each question has one passage. Thus the passage is relatively long for each question, and then the dataset has the aspect of Macro-reading.", "cite_spans": [], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "NarrativeQA [37] is a medium-scale reading comprehension dataset consisting of 1.5K passages and 47K questions. These questions are from books or movie scripts, and questions are written by cloud workers. In the dataset construction, the cloud workers write the pairs of a question and answer based solely on a given summary of the corresponding passage.", "cite_spans": [{"start": 12, "end": 16, "text": "[37]", "ref_id": "BIBREF51"}], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "The answer type is free-form answer, and then the evaluation metric is BLEU, Meteor and ROUGE, and the mean reciprocal rank (MRR). Here MRR is ave r 1 r where r is the rank of the correct answer among candidate answers.", "cite_spans": [], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "HotpotQA [24] is a reading comprehension dataset requiring the reasoning. Here the reasoning is a task to provide a set of sentences explaining why the answer is selected. The dataset consists of 113K questions and passages. Each passage is a set of paragraphs from 24 Wikipedia articles, and the question is written by a cloud worker. Additionally, the cloud worker picks support facts, sentences in the passage that determine the answer for each question. The dataset employed Joint F1 for the evaluation metric in addition to EM and F1. Joint F1 is computed as follows:", "cite_spans": [{"start": 9, "end": 13, "text": "[24]", "ref_id": "BIBREF38"}], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "where P (ans) and P (sup) are the precisions of the answer span and the support facts for each, and R (ans) and R (sup) are the recalls of the answer span and the support facts for each.", "cite_spans": [], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "This evaluation metric forces machines to find not only the correct answer span but also the correct support facts.", "cite_spans": [], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "Wikireading [9] is the largest reading comprehension dataset in the datasets in this section that consists of 19M pairs of a question and answer. The dataset is constructed from Wikipedia and Wikidata. Wikipedia is a free online encyclopedia hosted by the Wikimedia Foundation that consists of more than 6 million articles 2 . Wikidata is a collaboratively edited knowledge base hosted by the Wikimedia Foundation that consists of sets of tuples,", "cite_spans": [{"start": 12, "end": 15, "text": "[9]", "ref_id": null}], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "i.e., (subject entity, relation type, argument entity). There are more than 7,000 relation types, including \"instance_of\" and \"location\", and most entities in Wikidata and entries in", "cite_spans": [], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "Wikipedia are linked for each other. In the dataset, each question is a pair of the subject entity and relation type in a tuple, and then the answer is the argument entity in the tuple.", "cite_spans": [], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "The corresponding passage for the question is a Wikipedia article whose title is the subject entity. The answer type is free-form answer, and a machine is expected to predict the name of the argument entity. Again, the evaluation metrics are EM and F1 as following SQuAD.", "cite_spans": [], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "The dataset is pretty biased, and the top 20 relation types cover 75% of the dataset so that ", "cite_spans": [], "ref_spans": [], "section": "MS Machine Reading Comprehension (MS MARCO) [35] is a reading comprehension"}, {"text": "We now describe the construction of our WDW in more detail. To generate a question, we first generate the question by selecting a random article -the \"question article\" -from the Gigaword corpus and taking the first sentence of that article -the \"question sentence\"as the source of the cloze question. The hope is that the first sentence of an article contains prominent people and events which are likely to be discussed in other independent articles.", "cite_spans": [], "ref_spans": [], "section": "Dataset construction"}, {"text": "To convert the question sentence to a cloze question, we first extract named entities using the Stanford NER system [38] and parse the sentence using the Stanford PCFG parser [39] .", "cite_spans": [{"start": 116, "end": 120, "text": "[38]", "ref_id": "BIBREF52"}, {"start": 175, "end": 179, "text": "[39]", "ref_id": "BIBREF53"}], "ref_spans": [], "section": "Dataset construction"}, {"text": "The person named entities are candidates for deletion to create a cloze problem. For each person named entity, we then identify a noun phrase in the automatic parse that is headed with Apple Founder Steve Jobs\" we identify the two person noun phrases \"President Obama\"", "cite_spans": [], "ref_spans": [], "section": "Dataset construction"}, {"text": "and \"Apple Founder Steve Jobs\". When a person named entity is selected for deletion, the entire noun phrase is deleted. For example, when deleting the second named entity, we get \"President Obama met yesterday with ***\" rather than \"President Obama met yesterday with Apple founder ***\". This increases the difficulty of the problems because systems cannot rely on descriptors and other local contextual cues. About 700,000 question sentences are generated from Gigaword articles (8% of the total number of articles).", "cite_spans": [], "ref_spans": [], "section": "Dataset construction"}, {"text": "Once a cloze question has been formed, we select an appropriate article as a passage.", "cite_spans": [], "ref_spans": [], "section": "Dataset construction"}, {"text": "The article should be independent of the question article but should discuss the people and events mentioned in the question sentence. To find a passage, we search the Gigaword dataset using the Apache Lucene information retrieval system [40] , using the question sentence as the query. The named entity to be deleted is included in the query and required to be included in the returned article. We also restrict the search to articles published within two weeks of the date of the question article. Articles containing sentences too similar to the question in word overlap and phrase matching near the blanked phrase are removed. We select the best matching article satisfying our constraints. If no such article can be found, we abort the process and move on to a new question.", "cite_spans": [{"start": 238, "end": 242, "text": "[40]", "ref_id": "BIBREF54"}], "ref_spans": [], "section": "Dataset construction"}, {"text": "Given a question and a passage, we next form the list of choices. We collect all person named entities in the passage except unblanked person named entities in the question. Person named entities that are subsets of another longer named entity are eliminated from the choice list. For example, the choice \"Obama\" would be eliminated if the list also contains \"Barack", "cite_spans": [], "ref_spans": [], "section": "Dataset construction"}, {"text": "Obama\". We also discard ambiguous cases where a part of a blanked NE appears in multiple choices in the list, e.g., if a passage has \"Bill Clinton\" and \"Hillary Clinton\" and the blanked phrase is \"Clinton\" then we discard it. We found this simple coreference rule to work well in 28 practice since news articles usually employ full names for initial mentions of persons. If the resulting choice list contains fewer than two or more than five choices, the process is aborted and we move on to a new question. 3 After forming an initial set of problems, we then remove \"duplicated\" problems. Duplication arises because Gigaword contains many copies of the same article or articles where one is clearly an edited version of another. Our duplication-removal process ensures that no two problems have very similar questions. Here, similarity is defined as the ratio of the size of the bag of words intersection to the size of the smaller bag.", "cite_spans": [{"start": 508, "end": 509, "text": "3", "ref_id": null}], "ref_spans": [], "section": "Dataset construction"}, {"text": "Then we remove some problems in order to focus our dataset on the most interesting problems. We decided to remove questions that can be solved by a syntactic matching algorithm, counting algorithm, or simple heuristic algorithm because we found machine learning systems easily learned these techniques from these questions; thus, they were not appropriate to teach and test deeper reading comprehension skills of these machine learning systems. We used the following two syntactic matching algorithms, a counting algorithm, and a heuristic algorithm as baselines to find such questions. We remove these questions to suppress their performance.", "cite_spans": [], "ref_spans": [], "section": "Dataset construction"}, {"text": "\u2022 First person in passage: Select the person that appears first in the passage.", "cite_spans": [], "ref_spans": [], "section": "Dataset construction"}, {"text": "\u2022 Most frequent person: Select the most frequent person in the passage.", "cite_spans": [], "ref_spans": [], "section": "Dataset construction"}, {"text": "\u2022 n-gram: Select the most likely answer to fill the blank under a 5-gram language model trained on Gigaword minus articles which are too similar to one of the questions in word overlap and phrase matching.", "cite_spans": [], "ref_spans": [], "section": "Dataset construction"}, {"text": "\u2022 Unigram: Select the most frequent last name using the unigram counts from the 5-gram model. 3 The maximum of five helps to avoid sports articles containing structured lists of results.", "cite_spans": [{"start": 94, "end": 95, "text": "3", "ref_id": null}], "ref_spans": [], "section": "Dataset construction"}, {"text": "To minimize the number of questions removed we solve an optimization problem defined by limiting the performance of each baseline to a specified target value while removing as few problems as possible, i.e.,", "cite_spans": [], "ref_spans": [], "section": "29"}, {"text": "where T (C) is the subset of the questions solved by the subset C of the suppressed baselines, \u03b1(C) is a keeping rate for question set T (C), C i = 1 indicates the i-th baseline is in the subset, |b| is the number of baselines, N is a total number of questions, and k is the upper bound for the baselines after suppression. We choose k to yield random performance for the baselines. The performance of the baselines before and after suppression is shown in ", "cite_spans": [], "ref_spans": [], "section": "29"}, {"text": "We report the performance of following several systems to characterize our dataset: \u2022 Word overlap: Select the choice c inserted to the question q which is the most similar to any sentence s in the passage, i.e., CosSim(bag(c + q), bag(s)).", "cite_spans": [], "ref_spans": [], "section": "Performance Benchmarks"}, {"text": "\u2022 Sliding window and Distance baselines (and their combination) from Richardson et al. [34] .", "cite_spans": [{"start": 87, "end": 91, "text": "[34]", "ref_id": "BIBREF48"}], "ref_spans": [], "section": "Performance Benchmarks"}, {"text": "\u2022 Semantic features: NLP feature based system from Wang et al. [41] .", "cite_spans": [{"start": 63, "end": 67, "text": "[41]", "ref_id": "BIBREF55"}], "ref_spans": [], "section": "Performance Benchmarks"}, {"text": "\u2022 Attentive Reader: LSTM with attention mechanism [1] .", "cite_spans": [{"start": 50, "end": 53, "text": "[1]", "ref_id": "BIBREF12"}], "ref_spans": [], "section": "Performance Benchmarks"}, {"text": "\u2022 Stanford Reader: An attentive reader modified with a bilinear term [2] .", "cite_spans": [{"start": 69, "end": 72, "text": "[2]", "ref_id": null}], "ref_spans": [], "section": "31"}, {"text": "\u2022 Attention Sum Reader: GRU with a point-attention mechanism [3] .", "cite_spans": [{"start": 61, "end": 64, "text": "[3]", "ref_id": null}], "ref_spans": [], "section": "31"}, {"text": "\u2022 Gated-Attention Reader: Attention Sum Reader with gated layers [4] . leverage the frequency of the answer in the passage, a heuristic which appears beneficial for the CNN/Daily Mail tasks. It seems that our suppression of the most-frequent-person baseline more strongly affects the performance of these latter systems.", "cite_spans": [{"start": 65, "end": 68, "text": "[4]", "ref_id": null}], "ref_spans": [], "section": "31"}, {"text": "We presented a large-scale person-centered cloze dataset. The dataset is not anonymized, and each passage is a raw text which is not only natural but also easier to be pre-processed by syntactic and semantic parsers. In the dataset construction, we used baseline suppression,", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "where we selected undesired questions by multiple baseline systems and randomly removed some of them. This approach can flexibly design the difficulty and quality of a dataset by replacing baseline systems that select undesired questions. [3] , and result marked IV is from Dhingra et al. [4] .", "cite_spans": [{"start": 239, "end": 242, "text": "[3]", "ref_id": null}, {"start": 289, "end": 292, "text": "[4]", "ref_id": null}], "ref_spans": [], "section": "Conclusion"}, {"text": "Analysis of a neural structure in entity-centered reading comprehension", "cite_spans": [], "ref_spans": [], "section": "Chapter Three"}, {"text": "As we discussed in Section 2.1, several large scale cloze-style reading comprehension datasets [1, 31, 7] have been introduced, and the large sizes of them enable the application of deep learning. Despite the significant performance of the deep learning models, the prediction structure of these models is poorly understood.", "cite_spans": [{"start": 95, "end": 98, "text": "[1,", "ref_id": "BIBREF12"}, {"start": 99, "end": 102, "text": "31,", "ref_id": "BIBREF45"}, {"start": 103, "end": 105, "text": "7]", "ref_id": null}], "ref_spans": [], "section": "Chapter Three"}, {"text": "In this chapter, we present empirical evidence for the emergence of predication structure in a certain class of deep learning models for reading comprehension (neural readers);", "cite_spans": [], "ref_spans": [], "section": "Chapter Three"}, {"text": "\"Aggregation\" and \"Explicit reference\" readers. Both readers work on the CNN/Daily Mail dataset, a dataset with anonymized entities. This work was published as the best paper in 2nd Workshop on Representation Learning for NLP [42] .", "cite_spans": [{"start": 226, "end": 230, "text": "[42]", "ref_id": "BIBREF56"}], "ref_spans": [], "section": "Chapter Three"}, {"text": "Before we explain the neural readers, we review the CNN/Daily Mail dataset where entities are anonymized. This dataset consists of anonymized passages and questions where named entities are replaced by anonymous entity identifiers such as \"entity37\". For example, the passage might contain \"entity52 gave entity24 a rousing applause\", and the question might be \"X received a rounding applause from entity52\", then the answer is the most 35 appropriate entity identifier in the passage to fill X. The same entity identifiers are used over all the problems, and a different identifier is assigned to an entity every time the passage and question are read. Thus, the entity identifiers are presumably just pointers to semanticsfree tokens and do not have any semantic meaning. We will write entity identifiers as logical constant symbols such as c rather than strings such as \"entity37\".", "cite_spans": [], "ref_spans": [], "section": "Chapter Three"}, {"text": "\"Aggregation\" readers, including Memory Networks [8, 43] , the Attentive Reader [1] , and the Stanford Reader [2] , use bidirectional LSTMs or GRUs to construct a contextual embedding h t of each position t in the passage and also an embedding h q of the question q.", "cite_spans": [{"start": 49, "end": 52, "text": "[8,", "ref_id": null}, {"start": 53, "end": 56, "text": "43]", "ref_id": "BIBREF57"}, {"start": 80, "end": 83, "text": "[1]", "ref_id": "BIBREF12"}, {"start": 110, "end": 113, "text": "[2]", "ref_id": null}], "ref_spans": [], "section": "Chapter Three"}, {"text": "They then select an answer c using a criterion similar to", "cite_spans": [], "ref_spans": [], "section": "Chapter Three"}, {"text": "where e(c) is the vector embedding of the constant symbol (entity identifier) c. In practice the inner-product h t , h q is normalized over t using a softmax to yield attention weights \u03b1 t Here t \u03b1 t h t can be viewed as a vector representation of the passage.", "cite_spans": [], "ref_spans": [], "section": "Chapter Three"}, {"text": "We argue that for aggregation readers, roughly defined by Equation ( 3)", "cite_spans": [], "ref_spans": [], "section": "Chapter Three"}, {"text": "The first inner product in Equation (3.3) is interpreted as measuring the extent to which", "cite_spans": [], "ref_spans": [], "section": "Chapter Three"}, {"text": "for any x. The second inner product is interpreted as restricting t to positions talking about the constant symbol c. Note that the posited decomposition of h t is not explicit in Equation (3.2) but instead must emerge during training. We present empirical evidence that this structure does emerge. The empirical evidence is somewhat tricky as the direct sum structure that divides h t into its two parts need not be axis aligned and therefore need not literally correspond to vector concatenation.", "cite_spans": [], "ref_spans": [], "section": "Chapter Three"}, {"text": "\"Explicit reference readers\", including the Attention Sum Reader [3] , the Gated-Attention", "cite_spans": [{"start": 65, "end": 68, "text": "[3]", "ref_id": null}], "ref_spans": [], "section": "Chapter Three"}, {"text": "Reader [4] , and the Attention-over-Attention Reader [44] , avoid Equation In this research, we have only considered anonymized datasets that require the handling of semantics-free constant symbols. However, even for non-anonymized datasets such as WDW, it is helpful to add features which indicate which positions in the passage are referring to which candidate answers. This indicates, not surprisingly, that reference is important in question answering. The fact that explicit reference features are needed in aggregation readers on non-anonymized data indicates that reference is not being solved by the aggrega-tion readers. However, as reference seems to be important for cloze-style question answering, these problems may ultimately provide training data from which reference resolution can be learned.", "cite_spans": [{"start": 7, "end": 10, "text": "[4]", "ref_id": null}, {"start": 53, "end": 57, "text": "[44]", "ref_id": "BIBREF58"}], "ref_spans": [], "section": "Chapter Three"}, {"text": "Here we classify readers into aggregation readers and explicit reference readers. Aggregation readers appeared first in the literature, including Memory Networks [8, 43] , the Attentive Reader [1] , and the Stanford Reader [2] . Then, Explicit reference readers, including the Attention Sum Reader [3] , the Gated-Attention Reader [4] , and the Attention-over-Attention", "cite_spans": [{"start": 162, "end": 165, "text": "[8,", "ref_id": null}, {"start": 166, "end": 169, "text": "43]", "ref_id": "BIBREF57"}, {"start": 193, "end": 196, "text": "[1]", "ref_id": "BIBREF12"}, {"start": 223, "end": 226, "text": "[2]", "ref_id": null}, {"start": 298, "end": 301, "text": "[3]", "ref_id": null}, {"start": 331, "end": 334, "text": "[4]", "ref_id": null}], "ref_spans": [], "section": "Related work"}, {"text": "Reader [44] , were proposed. In the following sections, we define aggregation readers more specifically by Equations (3.7) and (3.9) and then explicit reference readers by Equation ", "cite_spans": [{"start": 7, "end": 11, "text": "[44]", "ref_id": "BIBREF58"}], "ref_spans": [], "section": "Related work"}, {"text": "Finally, they compute a probability distribution P over the answers: is the output of a multi layer perceptron given input x. Also, the answer distribution in the Attentive Reader is defined over the full vocabulary rather than just the candidate answer set A: Here we think of R(a, p) as the set of references to a in the passage p. It is important to note that Equation (3.12) is an equality and that P (a|p, q, A) is not normalized to the members of R(a, p). When training with the log-loss objective this drives the attention \u03b1 t to be normalized -to have support only on the positions t with t \u2208 R(a, p) for some a.", "cite_spans": [{"start": 372, "end": 378, "text": "(3.12)", "ref_id": null}], "ref_spans": [], "section": "Related work"}, {"text": "Gated-Attention Reader. The Gated-Attention Reader [4] involves a K-layer biGRU architecture defined by the following equations.", "cite_spans": [{"start": 51, "end": 54, "text": "[4]", "ref_id": null}], "ref_spans": [], "section": "Related work"}, {"text": "h q = [fGRU(e(q)) |q| , bGRU(e(q)) 1 ], 1 \u2264 \u2264 K.", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "(3.14) Attention-over-Attention Reader. The Attention-over-Attention Reader [44] uses a more elaborate method to compute the attention \u03b1 t . We will use t to range over positions in the passage and j to range over positions in the question. The model is then defined by the following equations. h = biGRU(e(p)), h q = biGRU(e(q)).", "cite_spans": [{"start": 76, "end": 80, "text": "[44]", "ref_id": "BIBREF58"}], "ref_spans": [], "section": "Related work"}, {"text": "Note that the final equation defining \u03b1 t can be interpreted as applying the attention \u03b2 j to the attentions \u03b1 t,j . This reader uses Equations (3.12) and (3.13).", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "In this section, we claim an emergent predication structure in the hidden vectors h t that explains the high performance of aggregation readers. Intuitively we think of the hidden Formally, the decomposition of h t into this predication structure is not necessarily axis aligned. Rather than posit an axis-aligned concatenation, we posit that the hidden vector space H is a possibly non-aligned direct sum", "cite_spans": [], "ref_spans": [], "section": "Emergent Predication Structure"}, {"text": "where S is a subspace of \"statement vectors\" and E is an orthogonal subspace of \"entity pointers\". Each hidden state vector h \u2208 H then has a unique decomposition as h = \u03a8 + e 42 for \u03a8 \u2208 S and e \u2208 E. This is equivalent to saying that the hidden vector space H is some rotation of a concatenation of the vector spaces S and E. In this non-axis aligned model, we assume emergent embeddings s(\u03a6) and s(a) with s(\u03a6) \u2208 S and s(a) \u2208 E. We also assume that the latent spaces are learned in such a way that explicit entity output embeddings satisfy e o (a) \u2208 E.", "cite_spans": [], "ref_spans": [], "section": "Emergent Predication Structure"}, {"text": "This predication structure explains that a question asks for a value of x such that a statement \u03a8[x] is implied by the passage. For a question \u03a8 we might even suggest the following vectorial interpretation of entailment.", "cite_spans": [], "ref_spans": [], "section": "Emergent Predication Structure"}, {"text": "This interpretation is exactly correct if some of the dimensions of the vector space correspond to predicates, \u03a8 is a 0-1 vector representing a conjunction predicates, and \u03a6 is also 0-1 on these dimensions indicating whether a predicate is implied by the context.", "cite_spans": [], "ref_spans": [], "section": "Emergent Predication Structure"}, {"text": "We now present empirical evidence for this emergent structure. The empirical evidence supports two corollaries that are derived from the structure. Thus, the aggregation readers and the explicit reference readers are using essentially the same answer selection criterion.", "cite_spans": [], "ref_spans": [], "section": "Emergent Predication Structure"}, {"text": "The first three rows of ", "cite_spans": [], "ref_spans": [], "section": "Emergent Predication Structure"}, {"text": "In this section, we propose a novel approach, one-hot pointer annotation, to locate entities approach, we use a non-anonymized dataset (WDW), and add a one-hot indicator to each input (word embedding) that indicates occurrences of candidate answers in a passage. This approach simply provides the reference information R(a, p) without losing any information in the passage, unlike anonymized entity identifiers that remove original tokens in the passage.", "cite_spans": [], "ref_spans": [], "section": "Pointer Annotation Readers"}, {"text": "Additionally, we hope that the one-hot indicator helps aggregation readers that are apparently benefited by the anonymization. The performance of aggregation and explicit reference readers on WDW is in Table ( ", "cite_spans": [], "ref_spans": [{"start": 202, "end": 209, "text": "Table (", "ref_id": null}], "section": "Pointer Annotation Readers"}, {"text": "Our experiments indicate that both explicit reference and aggregation readers benefit greatly from this externally provided reference information. Especially, explicit reference readers rely on reference resolution-a specification of which phrases in the given passage refer to candidate answers. Aggregation readers also seem to demonstrate a stronger learning ability in that they essentially learn to mimic explicit reference readers by identifying reference annotation and using it appropriately. This is done most clearly in the pointer reader architectures. Furthermore, we have argued for, and given experimental evidence for, an interpretation of aggregation readers as learning emergent predication structure-a factoring of neural representations into a direct sum of a statement (predicate) representation and an entity (argument) representation. Human Performance -84 Table 3 .2 Accuracy on Who-did-What dataset. Each result is based on a single model. Results for neural readers other than NSE are based on replications of those systems. All models were trained on the relaxed training set which uniformly yields better performance than the restricted training set. The first group of models are explicit reference models and the second group are aggregation models. + indicates anonymization with better reference identifier.", "cite_spans": [], "ref_spans": [{"start": 879, "end": 886, "text": "Table 3", "ref_id": "TABREF1"}], "section": "Discussion"}, {"text": "There is great interest in learning representations for natural language understanding.", "cite_spans": [], "ref_spans": [], "section": "49"}, {"text": "These neural reading comprehension is such that systems still benefit from externally provided linguistic features, including externally annotated reference resolution. It would be interesting to develop fully automated neural readers that perform as well as readers using externally provided annotations.", "cite_spans": [], "ref_spans": [], "section": "49"}, {"text": "In this work, we claimed and empirically showed that the success of aggregation readers and explicit readers could be explained by Equation Finally, we proposed one-hop pointer annotation to helps aggregation readers whose performance indicates that these neural networks are benefited from externally provided linguistic features, including externally annotated reference information.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "Relation and entity centered reading comprehension In this work, we apply the externally provided reference information that improved the performance of neural readers in Chapter 3 to another reading comprehension task focusing on not only entities but also their relations, and propose a novel neural model and training algorithm that memory-efficiently trains the model. We propose a Transformer based model with an explicit reference structure that efficiently captures the global contexts. Although the self-attention layer in Transformer consumes a memory that quadratically scales to the length of the input sequence, we propose a training algorithm whose memory requirement is constant to the length of the sequence. We employed Wikihop to show the performance of the model and the training algorithm. The dataset is a reading comprehension dataset focusing on not only entities but also their relations. We presented studies to find an entity from a passage for a given textual query, i.e., cloze-style reading comprehension, in Chapter 2 and Chapter 3. On the other hand, Wikihop is a reading comprehension task whose query consists of a relation and entity and asks another entity that has the relation to the entity.", "cite_spans": [], "ref_spans": [], "section": "Chapter Four"}, {"text": "Our model, trained by the algorithm, achieved the state-of-the-art in Wikihop. 51 ", "cite_spans": [{"start": 79, "end": 81, "text": "51", "ref_id": "BIBREF66"}], "ref_spans": [], "section": "Chapter Four"}, {"text": "Wikihop consists of a passage, question, candidate answers, and an answer. Here a question is a tuple of a query entity and relation, and then the answer is another entity that has the relation to the query entity. The task is closely related to the relation extraction tasks, and, unlike cloze-style reading comprehension, the task requires not only finding an entity but also understanding relations in the passage. In addition to that, the dataset also provides anonymized passages that help the reference resolution.", "cite_spans": [], "ref_spans": [], "section": "Wikihop dataset"}, {"text": "Wikihop is designed for multi-hop reading comprehension with relatively long passages.", "cite_spans": [], "ref_spans": [], "section": "Wikihop dataset"}, {"text": "In Wikihop, each passage has multiple paragraphs, as shown in Fig. 4 [5] .", "cite_spans": [{"start": 69, "end": 72, "text": "[5]", "ref_id": null}], "ref_spans": [{"start": 62, "end": 68, "text": "Fig. 4", "ref_id": "FIGREF29"}], "section": "Wikihop dataset"}, {"text": "that is designed as a set of tuples, and each tuple consists of a subject entity, object entity, and their relation. There are more than 7,000 relation types, including \"instance_of\" and ", "cite_spans": [], "ref_spans": [], "section": "Wikihop dataset"}, {"text": "In this section, we review related work for Wikihop by using three approaches. In the first approach, models have various self-attention structures. A limitation of the naive selfattention layer is the maximum length of a sequence that it can take. These models modified the self-attention structure to overcome the limitation; however, their training time (including pre-training and fine-tuning for a downstream task) is longer than those of other models.", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "In the second approach, models consist of a pre-trained encoder and additional network structure, so that they are solely fine-tuned for a downstream task. We also take the pretraining and fine-tuning approach, but we propose a simpler model on the top of an encoder.", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "In the third approach, models are full scratch models whose parameters are all randomly initialized and optimized only on the dataset of the downstream task. These models have no access to the additional linguistic resources used in pre-training and do not perform as well as pre-trained models.", "cite_spans": [], "ref_spans": [], "section": "54"}, {"text": "Models modifying self-attention structure:", "cite_spans": [], "ref_spans": [], "section": "54"}, {"text": "In recent years, pre-trained Transformers are surpassing the performance of other neural structures like recurrent neural networks, and convolutional neural networks in reading comprehension tasks. Transformer is a neural structure that processes a sequence by stacked self-attention layers [46] . Each self-attention layer computes an attention from a token to other tokens as follows:", "cite_spans": [{"start": 291, "end": 295, "text": "[46]", "ref_id": "BIBREF60"}], "ref_spans": [], "section": "54"}, {"text": "where Q, K and V are query, key and value vectors for each token. The network structure is completely geometry free, i.e., there is no structure to reserve the order of tokens in the sequence like recurrent networks, but Transformer takes a position embedding along with a word embedding for each token. This self-attention mechanism gives a rich expressive power to Transformer.", "cite_spans": [], "ref_spans": [], "section": "54"}, {"text": "However, the structure requires an amount of memory that is quadratic in the sequence length in training. The self-attention structure is trained by a stochastic algorithm. The algorithm has two steps to update parameters in the structure. The first step is the forwarding process, where the structure computes the loss through the query, key, and value embeddings. The second step is the backpropagation, where we compute the gradient for each parameter using the query, key, and value embeddings. Thus, the query, key, and value embeddings must be kept until the backpropagation. As Equation Here, we review approaches that modify the structure, self-attention layer to address the issues. Dynamic self-attention [47] is a self-attention layer whose attention is over top-K tokens selected by a convolutional layer [48] . Transformer-XL and XLNet [49, 50] have a self-attention layer that uses relative position embeddings rather than absolute positions. A relative position provides the distance between two tokens; a token that we compute the attention from, and another token that we compute the attention to. Thus they are not Although these approaches potentially solve the fundamental limitation of the Transformer encoder, these models need to be pre-trained from scratch. Typically, these Transformer encoders are pre-trained on a large training data that is much larger than the training data of downstream tasks. As the result, the pre-training is the most time-consuming part of its parameter optimization. Thus, other approaches that are reviewed in the following section add additional structure on the top of pre-trained encoders so that they can avoid the pre-training.", "cite_spans": [{"start": 715, "end": 719, "text": "[47]", "ref_id": "BIBREF61"}, {"start": 817, "end": 821, "text": "[48]", "ref_id": "BIBREF62"}, {"start": 849, "end": 853, "text": "[49,", "ref_id": "BIBREF63"}, {"start": 854, "end": 857, "text": "50]", "ref_id": "BIBREF65"}], "ref_spans": [], "section": "54"}, {"text": "Another approach is fine-tuning based on pre-trained encoders. In this approach, a model consists of an encoder whose parameters are pre-trained and an additional neural structure whose parameters are randomly initialized. The pre-trained encoder provides contextual word embeddings for each input text. The encoder is pre-trained on a large scale language resource so that it is believed that the encoder obtained some general linguistic knowledge and its contextual word embeddings help downstream tasks. The additional structure is a task-specific neural structure that can efficiently leverage these contextual word embeddings for the downstream task. Thus, the parameters of the structures are fine-tuned for the task during the model is trained on the downstream dataset. For example, Graph Convolutional", "cite_spans": [], "ref_spans": [], "section": "Fine-tuning models:"}, {"text": "Networks is used on the top of Embeddings from Language Model (ELMo) encoder [53, 54] .", "cite_spans": [{"start": 77, "end": 81, "text": "[53,", "ref_id": "BIBREF68"}, {"start": 82, "end": 85, "text": "54]", "ref_id": "BIBREF69"}], "ref_spans": [], "section": "Fine-tuning models:"}, {"text": "Chen et al. [55] proposed a two-stage approach. In the first stage, a pointer network [56] selects a part of the passage that is likely essential for solving the question. In the second stage, a Transformer model takes the part of the passage and finds the answer.", "cite_spans": [{"start": 12, "end": 16, "text": "[55]", "ref_id": "BIBREF70"}, {"start": 86, "end": 90, "text": "[56]", "ref_id": "BIBREF71"}], "ref_spans": [], "section": "Fine-tuning models:"}, {"text": "It is worth mentioning that, in some studies, models are trained from scratch. These models consist of a relatively simple encoder and a relatively complicated additional neural structure.", "cite_spans": [], "ref_spans": [], "section": "Other network structures trained from scratch:"}, {"text": "For example, Zhong et al. [57] proposed a Coarse-grain Fine-grain Coattention Network consisting of attention over candidate entities mentioned in each paragraph and another attention over the paragraphs on the top of a bidirectional Gated Recurrent Unit (GRU) encoder [58] . Tu et al. [59] proposed a Heterogeneous Document-Entity (HDE) graph whose node is each entity-mention and paragraph encoded by GRU. Dhingra et al. [60] proposed a GRU with additional connections between tokens if these tokens are referring to the same entity (coreference).", "cite_spans": [{"start": 26, "end": 30, "text": "[57]", "ref_id": "BIBREF72"}, {"start": 269, "end": 273, "text": "[58]", "ref_id": "BIBREF73"}, {"start": 286, "end": 290, "text": "[59]", "ref_id": null}, {"start": 423, "end": 427, "text": "[60]", "ref_id": "BIBREF76"}], "ref_spans": [], "section": "Other network structures trained from scratch:"}, {"text": "We propose a simpler and efficient structure that adds a sum layer on the top of a", "cite_spans": [], "ref_spans": [], "section": "Other network structures trained from scratch:"}, {"text": "Transformer encoder. Our model works without the time-consuming pre-training, and also our experiments indicate our simple structure efficiently leverages the context embeddings given by the pre-trained Transformer encoder.", "cite_spans": [], "ref_spans": [], "section": "Other network structures trained from scratch:"}, {"text": "We propose a Transformer-based model with the explicit reference structure and a training algorithm for it. Here, the Transformer encoder is a function that takes a sequence of tokens and returns a contextual embedding for each token in the sequence. As we explained in Section 3, the explicit reference structure is a neural network structure that explicitly takes the contextual embedding of a token referring to a candidate answer to score the candidate, and these models explicitly leverage these embeddings. In this model, the Transformer encoder encodes each paragraph and computes the contextual embeddings of tokens for each paragraph independently, so that its memory usage is linear to the number of the paragraphs 58 and does not quadratically scale with the length of the passage, as we see in Section 4.2.", "cite_spans": [], "ref_spans": [], "section": "Explicit reference transformer"}, {"text": "Then the model accumulates these embeddings over paragraphs and scores the candidate answers. The overview of this model is shown in Figure 4 .3. We also propose a training algorithm for it, which reduces the memory usage during the training to the constant to the number of paragraphs.", "cite_spans": [], "ref_spans": [{"start": 133, "end": 141, "text": "Figure 4", "ref_id": "FIGREF29"}], "section": "Explicit reference transformer"}, {"text": "Remembering that the passage is a set of paragraphs, the Transformer encoder encodes the paragraphs independently. We denote the k-th paragraph by para k , the question by q, and then the encoder parameters by \u03a6. Then letting the contextual embeddings of the k-th", "cite_spans": [], "ref_spans": [], "section": "Explicit reference transformer"}, {"text": "Here the Transformer encoder takes a concatenation of the question and paragraph. The contextual embeddings of a token referring to each candidate answer are accumulated over all paragraphs.", "cite_spans": [], "ref_spans": [], "section": "Explicit reference transformer"}, {"text": "Remembering that each question consists of a relation and entity q e , we also similarly accumulate a query entity embedding, then the candidate answer embeddings are concatenated to the query entity embeddings. Letting the score of the i-th candidate answer be", "cite_spans": [], "ref_spans": [], "section": "Explicit reference transformer"}, {"text": "where H k \u03a6 [t] is the t-th contextual representation vector for the given paragraph, f is a fully connected layer, and R(para k , c) is the set of positions t where the entity c occurs in the paragraph. To find these positions, we matched entities and noun phrases in the passage whose most words match each entity when entities are not anonymized. ", "cite_spans": [], "ref_spans": [], "section": "Explicit reference transformer"}, {"text": "We also propose a stochastic gradient algorithm to train this model, whose memory usage is constant to the number of paragraphs as Algorithm 1. In this model, the Transformer encoder takes each paragraph instead of the entire passage, so the memory usage of the naive stochastic gradient algorithm is quadratic to the length of paragraphs and linear to the number of the paragraphs, which is still too large to fit a GPU memory when the passage has many paragraphs. During the training, the memory is consumed by a computational graph. The computational graph can be viewed as a representation of an objective function and requires memory for each neural output of parameterized functions in the objective function during the training. For example, parameters of a parameterized function f (x; \u03b8) is updated by the following during the training,", "cite_spans": [], "ref_spans": [], "section": "Training algorithm"}, {"text": "\u2202f", "cite_spans": [], "ref_spans": [], "section": "4)"}, {"text": "Here the computational graph keeps the output value of the neural f 2 in the forwarding propagation until the backpropagation. Our training algorithm computes the forwarding propagation twice for each backpropagation. In the first forwarding propagation, we compute the loss without keeping all neural outputs, and then in the second forward propagation, we compute the same loss with keeping a subset of the neural outputs whose parameters are updated on the upcoming backpropagation.", "cite_spans": [], "ref_spans": [], "section": "4)"}, {"text": "In the first forwarding propagation, we compute the contextual embedding for each paragraph independently without keeping neural outputs. We denote the embeddings by H k \u03a6 which is computed as", "cite_spans": [], "ref_spans": [], "section": "4)"}, {"text": "Here we keep the contextual embedding only and remove the left of neural output values.", "cite_spans": [], "ref_spans": [], "section": "4)"}, {"text": "In the second forwarding propagation, we recompute the contextual embedding for a single paragraph then compute the total loss with keeping neural outputs for the following backpropagation. We denote the contextual embedding of the target paragraph by H k \u03a6 , and", "cite_spans": [], "ref_spans": [], "section": "4)"}, {"text": "Now we sum the contextual embedding of the target paragraph and that of other paragraphs.", "cite_spans": [], "ref_spans": [], "section": "4)"}, {"text": "Then, the total loss for the passage is", "cite_spans": [], "ref_spans": [], "section": "4)"}, {"text": "where a is the correct answer and only neural outputs under H k \u03a6 are stored in the computational graph. And then the gradient is computed with respect to \u03a6 thus \u03a6 is updated in Algorithm 1 Update steps for each question in the training algorithm that performs the forward propagation twice for the backpropagation.", "cite_spans": [], "ref_spans": [], "section": "4)"}, {"text": "Input: query q, paragraphs p 0 , p 1 , ..., candidate answers c 0 , c 1 , ..., and answer a \u2208 {c 0 , c 1 , ...} 1: for para k \u2208 para 0 , para 1 , ... The total loss is computed for each paragraph so that all parameters are updated.", "cite_spans": [], "ref_spans": [], "section": "4)"}, {"text": "Our model is mostly initialized by BERT pre-trained model and fine-tuned on anonymized", "cite_spans": [], "ref_spans": [], "section": "Experiments"}, {"text": "Wikihop. We use the anonymized version and avoid solving the coreference resolution and identifying mentions of each candidate answer by ourselves so that we use the exact same reference information that other systems used. The encoder of our model is BERT [61] , whose parameters are initialized by BERT-base with twelve self-attention layers and 512 position embeddings. BERT-base is a medium-size Transformer encoder whose scale is similar to each anonymized entity in passages. Other parameters are randomly initialized. Our model is fine-tuned on Wikihop for five epochs. During the fine-tuning, we permutated candidate answers in each passage to avoid over-fitting. We used 10% dropout [62] , warmup [63] over the first 8% of the training data, and Adam optimizer [64] for the parameter optimization.", "cite_spans": [{"start": 257, "end": 261, "text": "[61]", "ref_id": "BIBREF77"}, {"start": 692, "end": 696, "text": "[62]", "ref_id": "BIBREF78"}, {"start": 706, "end": 710, "text": "[63]", "ref_id": "BIBREF79"}, {"start": 770, "end": 774, "text": "[64]", "ref_id": "BIBREF80"}], "ref_spans": [], "section": "Experiments"}, {"text": "The learning rate is searched from 2 \u00d7 10 \u22126 upto 2 \u00d7 10 \u22124 . Table 4 .2 shows the performance of each system on the development data and test data.", "cite_spans": [], "ref_spans": [{"start": 62, "end": 69, "text": "Table 4", "ref_id": "TABREF2"}], "section": "Experiments"}, {"text": "The first four models are trained from scratch, and the following models are pre-trained on large-scale data and then fine-tuned on the Wikihop training data. The table shows that the performance of our system is significantly higher than those of the other systems on the development data. Our system shows more than 2% higher accuracy than Longformer-base on the development data. Longformer-base and Longformer-large have 12 and 24 layers for each, and our model uses BERT-base with 12 layers; hence its parameter size is similar to that of Longformer-base. In the test data, Longformer-large achieved the highest accuracy; however, our model achieved the best accuracy in the models with its parameter size scale.", "cite_spans": [], "ref_spans": [], "section": "Main result"}, {"text": "Additionally, Longformers are trained on non-anonymized data and they can potentially leverage the information of candidate answer names. On the other hand, our model is trained on anonymized data where candidate answers are replaced by entity IDs; thus, it is impossible to leverage the information of candidate answer names. It is also worth noting that models trained on the anonymized training data perform as good as or better on the non-anonymized test data than the anonymized test data because we can always convert the non-anonymized data into the anonymized data. ", "cite_spans": [], "ref_spans": [], "section": "Main result"}, {"text": "In order to better understand the contribution of the explicit reference structure to the performance, we show two upper bound accuracies; a model that reads each paragraph independently, and an oracle model that solely reads paragraphs mentioning the answer.", "cite_spans": [], "ref_spans": [], "section": "Ablation studies"}, {"text": "The first model scores each candidate answer for each paragraph independently during the training so that the model does not take account of the contexts beyond each paragraph. Then, each candidate answer is scored for each paragraph independently unlike the explicit reference reader as follows:", "cite_spans": [], "ref_spans": [], "section": "Ablation studies"}, {"text": ".", "cite_spans": [], "ref_spans": [], "section": "Ablation studies"}, {"text": "(4.11)", "cite_spans": [], "ref_spans": [], "section": "Ablation studies"}, {"text": "The model predicts the answer by the maximum score over the paragraphs, i.e.,", "cite_spans": [], "ref_spans": [], "section": "Ablation studies"}, {"text": "The first row of Table 4 .3 shows the accuracy of the model. The accuracy dropped by 8% from our full explicit reference Transformer. This gap indicates that the simple embedding sum significantly contributes to capturing the contexts beyond each paragraph.", "cite_spans": [], "ref_spans": [{"start": 17, "end": 24, "text": "Table 4", "ref_id": "TABREF2"}], "section": "Ablation studies"}, {"text": "The second model is an oracle model that takes solely paragraphs containing the correct answer so that it gives an identical maximum performance of the explicit reference Transformer in each paragraph. The model is trained and tested solely on paragraphs containing the correct answer. It is worth noting that the oracle is strong and removes most of the candidate answers.", "cite_spans": [], "ref_spans": [], "section": "Ablation studies"}, {"text": "The second row of Table 4 .3 shows the accuracy of the oracle model. Naturally, the performance is better than those of non-oracle models, and the strong accuracy indicates the potential of the explicit reference Transformer.", "cite_spans": [], "ref_spans": [{"start": 18, "end": 25, "text": "Table 4", "ref_id": "TABREF2"}], "section": "Ablation studies"}, {"text": "We proposed the explicit reference Transformer that has a simple sum layer on the top of a pre-trained Transformer encoder. The sum layer, called explicit reference structure, performs over contextual token embeddings referring to each candidate answer. Our model is simple and efficiently fine-tuned over Wikihop, and its performance is significantly better than that of models with the similar parameter size.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "Dev accuracy (%) Independent paragraphs 69.4", "cite_spans": [], "ref_spans": [], "section": "System"}, {"text": "Oracle paragraphs 96.9", "cite_spans": [], "ref_spans": [], "section": "System"}, {"text": "Our model 77.4 Table 4 .3 The model of independent paragraph reads each paragraph independently, and the model of oracle paragraphs takes solely paragraphs mentioning the correct answer.", "cite_spans": [], "ref_spans": [{"start": 15, "end": 22, "text": "Table 4", "ref_id": "TABREF2"}], "section": "System"}, {"text": "We also proposed a novel stochastic gradient descent training algorithm. The algorithm performs the forward computation twice; one for computing contextual embeddings and another for storing all neural outputs for the backpropagation. The algorithm requires a constant size of the memory-usage to the length of the input text; thus, it memory-efficiently trains the Transformer encoder.", "cite_spans": [], "ref_spans": [], "section": "System"}, {"text": "For future work, we would like to apply this model to other datasets to show the robustness of this approach. The Transformer encoder encodes geometric information by solely position embeddings, unlike recurrent networks and convolutional networks that encode geometric information by their network structures. However, the Transformer encoder, we believe, strongly associated with the geometry of the input sequence, and the contextual token embedding on the top of the t-th token is mostly representing the token. Hence, using the explicit embeddings of a task-specific token seems a promising approach.", "cite_spans": [], "ref_spans": [], "section": "System"}, {"text": "Relation extraction with weakly supervised learning for materials science", "cite_spans": [], "ref_spans": [], "section": "Chapter Five"}, {"text": "In this chapter, we present our work in relation extraction for materials science [65] . As we described in Section 1.2, relation extraction is studied in the context of knowledge base population, however; it can be view as a reading comprehension desiring a relation between two given entities. Thus, in this study, we find a relation between two given entities from a text resource, and also we build a graph using the relations that visualize the knowledge described in the text resource. Additionally, this work is collaborative work with materials science, and our target knowledge to be visualized is information that helps material development.", "cite_spans": [{"start": 82, "end": 86, "text": "[65]", "ref_id": "BIBREF81"}], "ref_spans": [], "section": "Chapter Five"}, {"text": "A key strategy to build the structured knowledge in materials science is Processing-Structure-Property-Performance (PSPP) reciprocity [66] . The PSPP reciprocity is a framework to understand material development, a field of study to find a manufacturing process that gives a material with specific properties. The PSPP reciprocity explains how the manufacturing process gives a property of a material on three steps: process, structure, and property. The first step is a set of processings where each processing is a (typically) chemical or physical input to the material. The second step is a set of structures where each structure of the material is a pattern of molecules in the material. The third step is a set of properties.", "cite_spans": [{"start": 134, "end": 138, "text": "[66]", "ref_id": "BIBREF82"}], "ref_spans": [], "section": "Chapter Five"}, {"text": "Each property is a character of the material that we find valuable. The PSPP reciprocity explains that the first step -processings -builds structures in the material, and the second step -structure -gives some properties of the material, then the third step -propertygives the performance of the material.", "cite_spans": [], "ref_spans": [], "section": "Chapter Five"}, {"text": "The PSPP reciprocity derives a knowledge graph, and PSPP chart defined as follows. In the knowledge graph, each node represents a specific process, structure, or property, a node of processing has an edge to a node of a structure if the processing builds the structure, and the node of the structure has an edge to the node of a property if the structure affects the property. Then no node of processing and no node of a structure are connected because, according to the PSPP reciprocity, all properties are given by processings through structures.", "cite_spans": [], "ref_spans": [], "section": "Chapter Five"}, {"text": "A subset of the knowledge graph is called a PSPP chart, e.g., Fig.5 .1 [67] . These edges in the PSPP chart indirectly visualize processings that impact on specific desired properties and help material development.", "cite_spans": [{"start": 71, "end": 75, "text": "[67]", "ref_id": "BIBREF83"}], "ref_spans": [{"start": 62, "end": 67, "text": "Fig.5", "ref_id": "FIGREF47"}], "section": "Chapter Five"}, {"text": "Even though PSPP charts are practically helpful in material development, there are a huge number of nodes in the knowledge graph, and it is expensive to find all edges by hand.", "cite_spans": [], "ref_spans": [], "section": "Chapter Five"}, {"text": "Hundreds of processings, structures, and properties are known in materials science. Thus the number of all possible edge is exponentially large, and finding such a number of edges by hand is practically impossible. In practice, expert researchers draw a PSPP chart, subgraph of the knowledge graph around target properties.", "cite_spans": [], "ref_spans": [], "section": "Chapter Five"}, {"text": "In this research, we developed a computer-aided material design system that automatically finds a PSPP chart from given scientific articles. The system is based on weak supervision that is well studied in the context of knowledge base completion, such as TAC 1 .", "cite_spans": [], "ref_spans": [], "section": "Chapter Five"}, {"text": "Here, the system is trained on about 100 relationships and thousands of non-annotated The process-structure-property-performance reciprocity scientific articles from Elsevier's API 2 , and then completes all relations among processing/structure/property nodes. The system does not rely on any specific dataset such as AtomWork [68] , but it relies on scientific articles that likely cover the knowledge needed to fill the PSPP chart. Then, the system visualizes processings that likely impact on given target properties.", "cite_spans": [{"start": 327, "end": 331, "text": "[68]", "ref_id": "BIBREF84"}], "ref_spans": [], "section": "Chapter Five"}, {"text": "This study is closely related to knowledge base population, a task to find relations between entities in a knowledge base. A knowledge base is a well-structured database consisting of relationships among entities, i.e., tuples of an entity-pair and relation. For the knowledge base, it is difficult to complete all relationships in the knowledge base by hand, and automatic approaches to complete the knowledge graph from texts are studied in the field of NLP.", "cite_spans": [], "ref_spans": [], "section": "Related work"}, {"text": "In these approaches, we used distant supervision [69] . In distant supervision, we preprocess the training data; a subgraph of the knowledge base (tuples of a pair of entities and their relation) and corpus (raw text), and then generate weakly labeled sentences. Each weakly labeled sentence is a sentence mentioning multiple entities whose relation is in the subgraph, and labeled by the relation. In other words, the weakly labeled sentence is distantly labeled by the relation on the knowledge base. Then these weakly labeled sentences are used to train machine learning models. The labels of these sentences seem noisier than manual labeling for each sentence, and the noise reduction of these labels is a key to this approach.", "cite_spans": [{"start": 49, "end": 53, "text": "[69]", "ref_id": "BIBREF85"}], "ref_spans": [], "section": "Related work"}, {"text": "Feature-based machine learning models and convolutional neural network (CNN) models are studied in the distant supervised approach. In recent years, CNN models have surpassed feature-based models [70, 71, 72, 73] . Residual learning is used to help the deep CNN network [74] . Zeng et al. [75] split a sentence into three parts, then applied max pooling to each part of the sentence over a CNN layer. Sentence level attention is introduced for selecting a key sentence. In this approach, a network takes a set of sentences for a relation between two entities. Each sentence contains both entities. An attention mechanism over a CNN allows the network to automatically select a key sentence which is likely describing the desired relation. It seems helpful to overcome the noise of distant labels [76, 77, 78] . 70 ", "cite_spans": [{"start": 196, "end": 200, "text": "[70,", "ref_id": "BIBREF86"}, {"start": 201, "end": 204, "text": "71,", "ref_id": null}, {"start": 205, "end": 208, "text": "72,", "ref_id": null}, {"start": 209, "end": 212, "text": "73]", "ref_id": "BIBREF93"}, {"start": 270, "end": 274, "text": "[74]", "ref_id": "BIBREF94"}, {"start": 289, "end": 293, "text": "[75]", "ref_id": "BIBREF95"}, {"start": 796, "end": 800, "text": "[76,", "ref_id": "BIBREF97"}, {"start": 801, "end": 804, "text": "77,", "ref_id": "BIBREF98"}, {"start": 805, "end": 808, "text": "78]", "ref_id": "BIBREF99"}, {"start": 811, "end": 813, "text": "70", "ref_id": "BIBREF86"}], "ref_spans": [], "section": "Related work"}, {"text": "Our task is to complete a PSPP knowledge graph from scientific articles and extract a subgraph of the PSPP knowledge graph. Let E be the entities of the knowledge graph, and ", "cite_spans": [], "ref_spans": [], "section": "Preliminary"}, {"text": "Our system completes the PSPP knowledge graph by two steps; entity collection and relation identification, and then produces a PSPP chart for given properties from the knowledge graph.", "cite_spans": [], "ref_spans": [], "section": "System description"}, {"text": "In the first step, our system collects entities E in the PSPP knowledge graph, and then these entities were classified into three material development steps; processing, structure, and property. For example, 'tempering' and 'hot working' are classified into processing, 'grain refining' and 'austenite dispersion' are classified into structure, and then 'strength'", "cite_spans": [], "ref_spans": [], "section": "System description"}, {"text": "and 'cost' are classified into property.", "cite_spans": [], "ref_spans": [], "section": "System description"}, {"text": "In the second step, our system identifies relations among entities r e i ,e j from scientific articles. Here a machine learning model is trained on weakly labeled sentences, i.e.,", "cite_spans": [], "ref_spans": [], "section": "System description"}, {"text": "{(S e i ,e j , r e i ,e j )|e i , e j \u2208 E train \u2282 E},", "cite_spans": [], "ref_spans": [], "section": "System description"}, {"text": "where E train is a set of entities in PSPP charts for training. The trained model fills other 71 relations to complete the PSPP knowledge graph.", "cite_spans": [], "ref_spans": [], "section": "System description"}, {"text": "Then additionally, our system finds and visualizes processes that likely impact on given", "cite_spans": [], "ref_spans": [], "section": "System description"}, {"text": "properties. Here, we assume a scenario where a researcher is developing a new material with certain desired properties and looking for processes related to the properties in a PSPP chart. In this scenario, the PSPP chart is with certain processes and structures around the desired properties.", "cite_spans": [], "ref_spans": [], "section": "System description"}, {"text": "In this section, we describe how we collected entities in the knowledge graph. not long enough to cover structural entities from a materials science standpoint.", "cite_spans": [], "ref_spans": [], "section": "Entity collection"}, {"text": "All keywords and the n most frequent noun phrases are collected, and each word/phrase is assigned a node in the PSPP knowledge graph. The total numbers of entities were 500, 500, and 1000 for process, property, and structural entities respectively. ", "cite_spans": [], "ref_spans": [], "section": "Entity collection"}, {"text": "In this section, we describe our CNN model for identifying the relation between entities.", "cite_spans": [], "ref_spans": [], "section": "Relation identification"}, {"text": "We use a stacked CNN with residual connections [74] . The CNN model consists of convolutional units with a deep residual learning framework that embeds the sentence into a vector representation. Then, the vector representation produces the probability distribution of the binary relation with a sigmoid layer. We show the overview of the model in Fig. 5. 3.", "cite_spans": [{"start": 47, "end": 51, "text": "[74]", "ref_id": "BIBREF94"}], "ref_spans": [{"start": 347, "end": 354, "text": "Fig. 5.", "ref_id": "FIGREF47"}], "section": "Relation identification"}, {"text": "The CNN model takes each weakly labeled sentence. Let the sentence be s = {t 0 , ..., t i , ...},", "cite_spans": [], "ref_spans": [], "section": "Relation identification"}, {"text": "where t i is the i-th token in the sentence, and W (t i ) \u2208 R dw be a word embedding of the token t i . We define a relative distance from a token to an entity in the sentence as k \u2212 i 74 where k is the position of the entity and i is the position of the token. Let the relative position embedding of the token be P (k \u2212 i) \u2208 R dp . We define the token embedding as", "cite_spans": [], "ref_spans": [], "section": "Relation identification"}, {"text": "where k 1 and k 2 are the first and second entity in the sentence. Note that each sentence s is padded to a fixed length L, and any relative distance greater than D max , is treated as D max .", "cite_spans": [], "ref_spans": [], "section": "Relation identification"}, {"text": "We put the token embeddings into the first convolutional layer. The convolutional unit of the first layer takes token embeddings around the position i, and computes c i \u2208 R dc as follows:", "cite_spans": [], "ref_spans": [], "section": "Relation identification"}, {"text": "...; x i+h\u22121 ], w \u2208 R dc\u00d7h(dw+2dp) and b \u2208 R dc is a bias. g is an elementwise non-linear function, ReLU.", "cite_spans": [], "ref_spans": [], "section": "Relation identification"}, {"text": "Following the first convolutional layer, the other convolutional layers are stacked with residual learning connections that directly transmit a signal from a lower to a higher layer while skipping the middle layers. We define these two adjacent convolutional layers called a residual CNN block as follows:", "cite_spans": [], "ref_spans": [], "section": "Relation identification"}, {"text": "wherec 0 = c. Here, the first convolutional layer\u0109 k i takes a signal from the immediately lower layerc k\u22121 i:i+h and another signal from the lower blockc k\u22122 i:i+h . We put the output of the last convolutional layer into a max pooling layer. Denoting the last output asc K \u2208 R L\u2212h+1\u00d7dc ,", "cite_spans": [], "ref_spans": [], "section": "Relation identification"}, {"text": "Then, we put z into two fully connected layers and a sigmoid function that gives the probability distribution of the desired relation given the sentence P (r|s):", "cite_spans": [], "ref_spans": [], "section": "Relation identification"}, {"text": "where r is the binary relation between the entities, w g \u2208 R dc\u00d7dc and b g \u2208 R dc .", "cite_spans": [], "ref_spans": [], "section": "Relation identification"}, {"text": "The desired probability P (r = True|e i , e j ) is the maximum of the probabilities over sentences. This is where the parameters \u03a6 = {W, P 1 , P 2 , w, b}.", "cite_spans": [], "ref_spans": [], "section": "Relation identification"}, {"text": "Additionally, we generate a PSPP chart from the knowledge graph for given desired properties. Here the PSPP chart is a subgraph of the knowledge graph that indicates processings that are likely impact on the desired properties. We find the PSPP chart by considering a max-flow problem where the flow occurs from the given properties to the processings. The inlets are all processings and the outlets are given properties. The capacity of each edge is the score of the relation, i.e., P (r = True|e i , e j ). We maximize the amount of flow with a limited number of nodes in the graph.", "cite_spans": [], "ref_spans": [], "section": "Branching"}, {"text": "We compute the capacity of each entity in the graph, which is the amount of flow that it can accept. Recalling that nodes of structure are connected to property and processing, and no processing and property are connected, all flows pass through nodes of structure. We define the capacity of a node of structure e as C(e) = min e \u2208PRC P (r = True|e, e ), e\u2208PRP P (r = True|e , e) , (5.12) where PRC is a set of all nodes of processing and PRP' is a set of the desired properties.", "cite_spans": [{"start": 382, "end": 388, "text": "(5.12)", "ref_id": null}], "ref_spans": [], "section": "Branching"}, {"text": "Similarly, we define the capacity of processing as", "cite_spans": [], "ref_spans": [], "section": "Branching"}, {"text": "where STR' is a set of all nodes of structure.", "cite_spans": [], "ref_spans": [], "section": "Branching"}, {"text": "The produced PSPP chart is composed of n processings, m structures, and the desired properties where n and m are the given hyper-parameters. The entities of the processing/structure are the n and m most capable nodes. For efficiency, the nodes are greedily searched so that optimality is not guaranteed. The PSPP chart shows the processings/structures related to the desired properties.", "cite_spans": [], "ref_spans": [], "section": "Branching"}, {"text": "The CNN model in Section 5.3 was trained and evaluated on a set of PSPP charts and scientific articles. The model was trained on weakly labeled sentences mentioning entities in PSPP charts for training, and then it took weakly labeled sentences mentioning entities in held-out PSPP charts for testing and predicted relations between entities in these held-out PSPP charts. In both of training and testing, the weakly labeled sentences are found in the scientific articles in Section 5.4. Structure \u2194 Property 10 31", "cite_spans": [], "ref_spans": [], "section": "Experiment for relation identification"}, {"text": "We used four PSPP charts [66] for training and testing. These four charts have 104 entity pairs in total as shown in Table 5.2 and Table 5 .3. We used three arbitrary charts for training and the fourth chart for testing. Thus, we trained and tested our model on four pairs of training and test charts. We used the likelihoods of relationships in these four test charts for computing precision and recall curves in Section 5.4 to obtain a smooth curve.", "cite_spans": [{"start": 25, "end": 29, "text": "[66]", "ref_id": "BIBREF82"}], "ref_spans": [{"start": 117, "end": 138, "text": "Table 5.2 and Table 5", "ref_id": "TABREF7"}], "section": "PSPP charts"}, {"text": "We used publicly accessible scientific articles on ScienceDirect 4 for training and testing.", "cite_spans": [], "ref_spans": [], "section": "Scientific articles"}, {"text": "ScienceDirect is an Elsevier platform providing access to articles in journals in a variety of fields, such as social sciences and engineering. Approximately 3,400 articles were collected using the keyword ('material' and 'microstructure') on ScienceDirect, i.e., each article is related to both 'material' and 'microstructure'. About 5,000 weakly labeled sentences were founded in these scientific articles by using the four PSPP charts, i.e., roughly 50 sentences for each entity pair on average.", "cite_spans": [], "ref_spans": [], "section": "Scientific articles"}, {"text": "We trained our CNN model described in Section 5.3 on weakly labeled sentences. Each weakly labeled sentence is labeled as follows. Let a set of sentences mentioning entities e i and e j be S e i ,e j . Here each entity is mapped to a span in a sentence by max-span string matching, i.e., an entity is mapped to the span if the span is the entity name, and no other entity names overlap it. For instance,", "cite_spans": [], "ref_spans": [], "section": "Training detail"}, {"text": "\u2022 Within each phase, the properties are ...", "cite_spans": [], "ref_spans": [], "section": "Training detail"}, {"text": "\u2022 When a substance undergoes a phase transition ...", "cite_spans": [], "ref_spans": [], "section": "Training detail"}, {"text": "The phase in the first sentence is mapped to entity 'phase', but phrase transition is mapped to 'phase_transition' instead of 'phase' in the second sentence. Thus a sentence mentions an entity if and only if it is mapped on a span in the sentence.", "cite_spans": [], "ref_spans": [], "section": "Training detail"}, {"text": "The model parameters are optimized by stochastic gradient descent and dropout. Dropout randomly drops some signals in the network that are thought to help the generalization capabilities of the network. We employed an Adam optimizer with a learning rate of 0.00005, and randomly dropped signals from max pooling during training with a probability of 20%.", "cite_spans": [], "ref_spans": [], "section": "Training detail"}, {"text": "The word embeddings were initialized with GloVe vectors [79] . Other hyper-parameters are listed in Table 5 .4.", "cite_spans": [{"start": 56, "end": 60, "text": "[79]", "ref_id": "BIBREF100"}], "ref_spans": [{"start": 100, "end": 107, "text": "Table 5", "ref_id": "TABREF29"}], "section": "Training detail"}, {"text": "We compared the performance of our CNN model to the performance of legacy machine learning models; Logistic regression and Support Vector Machine (SVM). The baseline mod- ", "cite_spans": [], "ref_spans": [], "section": "Baseline models"}, {"text": "The evaluation metrics are precision and recall, which are standard metrics for information extraction tasks. Precision is the ratio of correctly predicted positive entity pairs to all predicted positive entity pairs, and gives the accuracy of the prediction. Recall is the ratio of correct predictions to all positive entity pairs in the test data, and gives the coverage of the prediction. A positive entity pair is a pair whose relation is True. We obtain high precision In this evaluation, the hyper-parameter was an integer t, the number of positive entity pairs in the prediction. For a given t and a set of entity pairs in the test relationship data, the system predicts a binary relation, for each pair. It predicts the t most likely positive pairs, and the other pairs are predicted as negative.", "cite_spans": [], "ref_spans": [], "section": "Evaluation metric"}, {"text": "The entity pairs in the test relationship data were scored by a machine learning model trained on the corresponding training relationship data, where the score was P (r = True|e i , e j ). Then, a precision and recall pair for a given hyper-parameter t was computed as follows:", "cite_spans": [], "ref_spans": [], "section": "Evaluation metric"}, {"text": "where R test is the set of entity pairs with positive relations in all test relationship data, and R t represents the t most likely positive entity pairs. The likelihood was a score given by the model. ", "cite_spans": [], "ref_spans": [], "section": "Evaluation metric"}, {"text": "We developed a web-based end-to-end demo system to demonstrate our system in Fig. 5 .7.", "cite_spans": [], "ref_spans": [{"start": 77, "end": 83, "text": "Fig. 5", "ref_id": "FIGREF47"}], "section": "End-to-end system"}, {"text": "The demo system worked in a typical scenario of material development, where a scientist was looking for factors related to certain desired properties. The demo system provides an PSPP design chart for the properties that the scientist provided. The end-to-end system works on Apache Tomcat 5 .", "cite_spans": [], "ref_spans": [], "section": "End-to-end system"}, {"text": "The system input consisted of the desired properties along with a base material. The desired properties were selected from a list of properties collected as in Section 5.3. The base material was the target material, such as aluminum or titanium. It was important to obtain the desired knowledge. For example, the relationship between 'strength' and 'matrix'", "cite_spans": [], "ref_spans": [], "section": "End-to-end system"}, {"text": "in titanium alloys might have been different from this relationship in aluminum alloys. Then, the system predicts PSPP relations from the scientific articles about the base material. Firstly, the system selects a set of scientific articles for the base material. As in Section 5.4, the articles were collected by keyword search in ScientceDirect. Thus the system predicted all relations among the entities collected in Section 5.3, and scored them as in Section 5.3. Then the system generated a PSPP chart for the given properties as Section 5.3.", "cite_spans": [], "ref_spans": [], "section": "End-to-end system"}, {"text": "The system output was a PSPP design chart suggesting the required structures and processes. The chart formed by three columns -process, structure, and property-suggested relations from the processes to the desired properties. Moreover, for each relation, the system provided a representative sentence to justify the relation and aid the researcher's understanding.", "cite_spans": [], "ref_spans": [], "section": "End-to-end system"}, {"text": "In this study, we developed and tested our knowledge extraction and representation system intended to support material design, by representing knowledge as relationships. Knowledge was represented as relationships in PSPP design charts. We leveraged weakly supervised learning for relation extraction. The end-to-end system proved our concept, and its relation extraction performance was superior to that of other baseline models.", "cite_spans": [], "ref_spans": [], "section": "Conclusions and contribution"}, {"text": "Our contribution in this study is twofold. Firstly, we proposed a novel knowledge graph based on PSPP charts, and developed a system to build the knowledge graph from text using NLP technologies. Secondly, we experimentally verified that such technical knowledge can be extracted from text by using machine learning models. Our target knowledge is relations in PSPP design charts. These relations appear rather technical and significantly different from typical relations in NLP such as 'has_a' and 'is_a'. Extraction of these relations from text is nontrivial, and might need other knowledge resource such as equations and properties The end-to-end demo system. a) Desired properties and a base material were selected. b) A sample of the generated PSPP design chart. The desired properties were toughness and creep strength, and 'steel' was selected as base material. c) A sentence describing the relation between toughness and carbon content.", "cite_spans": [], "ref_spans": [], "section": "Conclusions and contribution"}, {"text": "of materials; however, we experimentally verified that a state-of-the-art machine learning model can find these relations from texts.", "cite_spans": [], "ref_spans": [], "section": "Conclusions and contribution"}, {"text": "Knowledge graphs in the scientific domain are recently highly demanded, and numerous works have been published. We overview the related work after our work.", "cite_spans": [], "ref_spans": [], "section": "Follow-up work"}, {"text": "As we employed the PSPP reciprocity, various types of knowledge graphs are studied for each target information. In the general scientific domain, Auer et al. [80] proposed the vision and infrastructure of a knowledge base for the general scientific domain. In biology, Jiang et al. [81] pointed out some relational scientific facts are true under specific conditions in biology. For example, given the following sentence: \"We observed that ... alkaline pH increases the activity of TRV5/V6 channels in Jurkat T Cells.\" [82] We can find a relational fact, {\"alkaline pH\", increase, \"TRV5/V6 channels\"}, which is true if {\"TRV5/V6 channels\", locate, \"Jurkat T Cells\" }. Another knowledge base for biology combines multiple structured databases and scientific papers [83] . In materials science, Mrdjenovich et al. [84] manually Luan et al. [88] proposed SciERC as extending these datasets. These annotations provide cleaner training labels and make training efficient.", "cite_spans": [{"start": 158, "end": 162, "text": "[80]", "ref_id": null}, {"start": 282, "end": 286, "text": "[81]", "ref_id": "BIBREF103"}, {"start": 519, "end": 523, "text": "[82]", "ref_id": null}, {"start": 764, "end": 768, "text": "[83]", "ref_id": "BIBREF106"}, {"start": 812, "end": 816, "text": "[84]", "ref_id": "BIBREF107"}, {"start": 838, "end": 842, "text": "[88]", "ref_id": "BIBREF111"}], "ref_spans": [], "section": "Follow-up work"}, {"text": "Information extraction for materials science (material informatics) is also highly demanded and actively studied. For example, another desired information to be extracted for materials science might be synthesis procedures. A synthesis procedure is a sequence of operations to synthesis a compound. Mention-level annotated datasets are provided for this task [89, 90, 91] , and Mysore et al. [92] apply the generative model of Kiddon et al. [93] to induce the procedures. Furthermore, several essential NLP technologies are studied for material informatics, such as entity recognition for materials science [94, 95] , and word2vec [96] on materials science publications [97] . Table 5 .5 Sample representative sentences scored by the CNN model. Label P indicates that the entities are positively related in the test relationship data and label N indicates a negative relation. Entities in each sentence are underlined. The score is the v r z 2 of each sentence.", "cite_spans": [{"start": 359, "end": 363, "text": "[89,", "ref_id": "BIBREF113"}, {"start": 364, "end": 367, "text": "90,", "ref_id": "BIBREF115"}, {"start": 368, "end": 371, "text": "91]", "ref_id": "BIBREF116"}, {"start": 392, "end": 396, "text": "[92]", "ref_id": "BIBREF117"}, {"start": 441, "end": 445, "text": "[93]", "ref_id": "BIBREF118"}, {"start": 607, "end": 611, "text": "[94,", "ref_id": "BIBREF119"}, {"start": 612, "end": 615, "text": "95]", "ref_id": "BIBREF120"}, {"start": 631, "end": 635, "text": "[96]", "ref_id": "BIBREF121"}, {"start": 670, "end": 674, "text": "[97]", "ref_id": "BIBREF122"}], "ref_spans": [{"start": 677, "end": 684, "text": "Table 5", "ref_id": "TABREF29"}], "section": "Follow-up work"}, {"text": "Score/Label Sentence 1 36.5/P ... the following matrix form : [11] k \u223c u = \u03bbu ... ", "cite_spans": [{"start": 62, "end": 66, "text": "[11]", "ref_id": null}], "ref_spans": [], "section": "Follow-up work"}, {"text": "In this thesis, we discussed reading comprehension, focusing on entities and their relations.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "We started with an overview of reading comprehension tasks and the role of entities and their relations in these tasks. In early work, these tasks provide a small hand-written dataset for rule-based systems. Later, the datasets are getting bigger and bigger for machine learning models, especially for deep neural network models that are capable of being trained on such large scale training data. Then we claim that the goal of these tasks is to test the reading comprehension skills of machines, and it differentiates the reading comprehension from other question answering tasks. Additionally, we are interested in not only testing these skills but also how the machine understands texts, and then claim that entities and their relation can be a key to explain it.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "In Chapter 2, we constructed a reading comprehension dataset, WDW, that is designed to validate the reading comprehension skills, especially the skill to understand entities in given texts. Here we used baseline systems and a sampling approach to control the difficulty of the dataset so that each question requires appropriate reading comprehension skills to solve it. The dataset gives a larger gap between human performance and machine performance, which shows that our dataset requires deeper text understanding.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "In Chapter 3, we investigated the skill to understand entities and experimentally identified a neural network module that associates with each entity in neural readers. We explored neural readers and classified them into aggregation readers and explicit readers by their neural structures on top of contextual token embeddings. We experimentally found contextual token embeddings that strongly correlate with each entity, and then showed the attention layer of the aggregation reader mimics the explicit reference of the explicit reader.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "In Chapter 4, we feedbacked the findings to another entity and relation centric reading comprehension dataset, Wikihop, and improved the performance of the neural network model. Here we leverage the neural structure associating with each entity for scor-", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "ing each candidate answer. Additionally, we proposed a training algorithm that can train self-attention layers without quadratically consuming the memory.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "In Chapter 5, we developed a visualization system that summarizes given texts into a graph consisting of entities and their relations. This system extracts entities and their relations from a bunch of scientific articles. These entities and relations produce a graph that visualizes a summary of the given scientific articles. This work is collaborative work with materials science, and our target information to be visualized is PSPP relations. We showed that such highly scientific relations could be extracted by the novel neural network trained on about 100 labeled relations and scientific articles.", "cite_spans": [], "ref_spans": [], "section": "Conclusion"}, {"text": "We presented our contribution to reading comprehension focusing on entities and their relations. Here, we discuss straightforwardly more work to do to understand the reading comprehension skills of deep neural networks better.", "cite_spans": [], "ref_spans": [], "section": "Future work"}, {"text": "Thanks to the deep neural networks and large scale datasets, the performance of machines 92 in reading comprehension tasks is significantly improved. On the other hand, it becomes more and more difficult to explain each semantic role of vector representation as the network structure becomes more and more complicated.", "cite_spans": [], "ref_spans": [], "section": "Future work"}, {"text": "We presented an empirical analysis of neural readers in Chapter 3, and identified contextual token embeddings that strongly correlate with each entity embedding in an entity-centric dataset. A follow-up question might be the following.", "cite_spans": [], "ref_spans": [], "section": "Future work"}, {"text": "\"How are entities treated in other reading comprehension styles and other neural models ?\"", "cite_spans": [], "ref_spans": [], "section": "Future work"}, {"text": "Recently, other reading comprehension styles, such as the span prediction and free-form answer, is more popular, and other neural models are proposed, such as Transformer. However, they are still based on linear transformations; thus, we can capture a correlation between arbitrary two vector representations by computing inner-product just as Chapter 3. Then, we can apply the same approach to these reading comprehension styles and capture neural module that correlates with each entity.", "cite_spans": [], "ref_spans": [], "section": "Future work"}, {"text": "We are also interested in a practical issue of the machine learning we faced in Chapter 5, a lacking of training data for a specific domain. In many practical cases, it is untrivial to collect enough amount of manually labeled training data for neural network models, and a domain-specific dataset tends to be smaller than a general-domain dataset, like [98] . Thus, the size of the dataset tends to be a bottleneck of the performance. In this thesis, we took three approaches to address this issue. Firstly, we build a dataset by heuristically matching news articles and sampling them in Chapter 2. Secondly, we initialize our model with a pretrained neural network and then fine-tuned in Chapter 4. Thirdly, we combined relational information and texts by the idea of distant supervision in Chapter 5. There are other interesting approaches, including zero-shot learning [99] , one-shot learning [100, 101] fewshot learning [102] . We believe it is critical to choose a suitable learning scheme to develop 93 a domain-specific machine learning system. 94 ", "cite_spans": [{"start": 354, "end": 358, "text": "[98]", "ref_id": "BIBREF123"}, {"start": 873, "end": 877, "text": "[99]", "ref_id": "BIBREF124"}, {"start": 898, "end": 903, "text": "[100,", "ref_id": "BIBREF125"}, {"start": 904, "end": 908, "text": "101]", "ref_id": "BIBREF126"}, {"start": 926, "end": 931, "text": "[102]", "ref_id": "BIBREF127"}, {"start": 1054, "end": 1056, "text": "94", "ref_id": "BIBREF119"}], "ref_spans": [], "section": "Future work"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Investigating variability of fatigue indicator parameters of two-phase nickel-based superalloy microstructures", "authors": [{"first": "Bin", "middle": [], "last": "Wen", "suffix": ""}, {"first": "Nicholas", "middle": [], "last": "Zabaras", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.commatsci.2011.07.055"]}}, "BIBREF1": {"ref_id": "b1", "title": "A study on the microstructures and mechanical properties of Ti-B20-0.1B alloys of direct rolling in the \u03b1+\u03b2 phase region", "authors": [{"first": "Liguo", "middle": [], "last": "Huang", "suffix": ""}, {"first": "Yuyong", "middle": [], "last": "Chen", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.jallcom.2015.05.244"]}}, "BIBREF2": {"ref_id": "b2", "title": "Preparation and properties of an Al2O3/Ti(C,N) micro-nano-composite ceramic tool material by microwave sintering", "authors": [{"first": "Zengbin", "middle": [], "last": "Yin", "suffix": ""}, {"first": "Juntang", "middle": [], "last": "Yuan", "suffix": ""}, {"first": "Zhenhua", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Hanpeng", "middle": [], "last": "Hu", "suffix": ""}, {"first": "Yu", "middle": [], "last": "Cheng", "suffix": ""}, {"first": "Xiaoqiu", "middle": [], "last": "Hu", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.ceramint.2015.11.082"]}}, "BIBREF3": {"ref_id": "b3", "title": "Microstructure evolution and mechanical properties of drop-tube processed, rapidly solidified grey cast iron", "authors": [{"first": "Olamilekan", "middle": [], "last": "Oloyede", "suffix": ""}, {"first": "Timothy", "middle": ["D"], "last": "Bigg", "suffix": ""}, {"first": "Robert", "middle": ["F"], "last": "Cochrane", "suffix": ""}, {"first": "Andrew", "middle": ["M"], "last": "Mullis", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.msea.2015.12.020"]}}, "BIBREF4": {"ref_id": "b4", "title": "Effects of Sc addition on the microstructure and mechanical properties of cast Al-3Li-1.5Cu-0.15Zr alloy", "authors": [{"first": "Chunchang", "middle": [], "last": "Shi", "suffix": ""}, {"first": "Liang", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Guohua", "middle": [], "last": "Wu", "suffix": ""}, {"first": "Xiaolong", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Antao", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Jiashen", "middle": [], "last": "Tao", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.msea.2016.10.063"]}}, "BIBREF5": {"ref_id": "b5", "title": "Microstructure analysis and yield strength simulation in high Co-Ni secondary hardening steel", "authors": [{"first": "Chenchong", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Chi", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Zhigang", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Jie", "middle": [], "last": "Su", "suffix": ""}, {"first": "Yuqing", "middle": [], "last": "Weng", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.msea.2016.05.069"]}}, "BIBREF6": {"ref_id": "b6", "title": "Study on the microstructure and mechanical properties of Aermet 100 steel at the tempering temperature around 482 \u2022 C", "authors": [{"first": "Xiaohui", "middle": [], "last": "Shi", "suffix": ""}, {"first": "Weidong", "middle": [], "last": "Zeng", "suffix": ""}, {"first": "Qinyang", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "Wenwen", "middle": [], "last": "Peng", "suffix": ""}, {"first": "Chao", "middle": [], "last": "Kang", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.jallcom.2016.04.087"]}}, "BIBREF7": {"ref_id": "b7", "title": "Effect of thermo-mechanical cycling on the microstructure and toughness in the weld CGHAZ of a novel high strength low carbon steel", "authors": [{"first": "H", "middle": [], "last": "Xie", "suffix": ""}, {"first": "L.-X", "middle": [], "last": "Du", "suffix": ""}, {"first": "J", "middle": [], "last": "Hu", "suffix": ""}, {"first": "G.-S", "middle": [], "last": "Sun", "suffix": ""}, {"first": "H.-Y", "middle": [], "last": "Wu", "suffix": ""}, {"first": "R", "middle": ["D K"], "last": "Misra", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.msea.2015.05.033"]}}, "BIBREF8": {"ref_id": "b8", "title": "Effect of ingot grain refinement on the tensile properties of 2024 Al alloy sheets", "authors": [{"first": "Wei", "middle": [], "last": "Haigen", "suffix": ""}, {"first": "Xia", "middle": [], "last": "Fuzhong", "suffix": ""}, {"first": "Wang", "middle": [], "last": "Mingpu", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.msea.2016.11.016"]}}, "BIBREF9": {"ref_id": "b9", "title": "Effect of microstructure on cleavage resistance of high-strength quenched and tempered steels", "authors": [{"first": "A", "middle": [], "last": "", "suffix": ""}, {"first": "Di", "middle": [], "last": "Schino", "suffix": ""}, {"first": "C", "middle": [], "last": "Guarnaschelli", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.matlet.2009.06.032"]}}, "BIBREF10": {"ref_id": "b10", "title": "Effects of solution treatment on microstructure and mechanical properties of thixoformed Mg2Sip/AM60B composite", "authors": [{"first": "F", "middle": ["L"], "last": "Cheng", "suffix": ""}, {"first": "T", "middle": ["J"], "last": "Chen", "suffix": ""}, {"first": "Y", "middle": ["S"], "last": "Qi", "suffix": ""}, {"first": "S", "middle": ["Q"], "last": "Zhang", "suffix": ""}, {"first": "P", "middle": [], "last": "Yao", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.jallcom.2015.02.147"]}}, "BIBREF11": {"ref_id": "b11", "title": "Microstructural evolution and mechanical properties of linear friction welded Ti2AlNb joint during solution and aging treatment", "authors": [{"first": "X", "middle": [], "last": "Chen", "suffix": ""}, {"first": "F", "middle": ["Q"], "last": "Xie", "suffix": ""}, {"first": "T", "middle": ["J"], "last": "Ma", "suffix": ""}, {"first": "W", "middle": ["Y"], "last": "Li", "suffix": ""}, {"first": "X", "middle": ["Q"], "last": "Wu", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.msea.2016.05.030"]}}, "BIBREF12": {"ref_id": "b12", "title": "Teaching machines to read and comprehend", "authors": [{"first": "Karl", "middle": [], "last": "Moritz Hermann", "suffix": ""}, {"first": "Tomas", "middle": [], "last": "Kocisky", "suffix": ""}, {"first": "Edward", "middle": [], "last": "Grefenstette", "suffix": ""}, {"first": "Lasse", "middle": [], "last": "Espeholt", "suffix": ""}, {"first": "Will", "middle": [], "last": "Kay", "suffix": ""}, {"first": "Mustafa", "middle": [], "last": "Suleyman", "suffix": ""}, {"first": "Phil", "middle": [], "last": "Blunsom", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Proceedings of Advances in Neural Information Processing Systems", "authors": [], "year": 2015, "venue": "", "volume": "28", "issn": "", "pages": "", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "A thorough examination of the CNN/daily mail reading comprehension task", "authors": [{"first": "Danqi", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Jason", "middle": [], "last": "Bolton", "suffix": ""}, {"first": "Christopher", "middle": ["D"], "last": "Manning", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Text understanding with the attention sum reader network", "authors": [{"first": "Rudolf", "middle": [], "last": "Kadlec", "suffix": ""}, {"first": "Martin", "middle": [], "last": "Schmid", "suffix": ""}, {"first": "Ondrej", "middle": [], "last": "Bajgar", "suffix": ""}, {"first": "Jan", "middle": [], "last": "Kleindienst", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF16": {"ref_id": "b16", "title": "Gated-attention readers for text comprehension", "authors": [{"first": "Bhuwan", "middle": [], "last": "Dhingra", "suffix": ""}, {"first": "Hanxiao", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Zhilin", "middle": [], "last": "Yang", "suffix": ""}, {"first": "William", "middle": [], "last": "Cohen", "suffix": ""}, {"first": "Ruslan", "middle": [], "last": "Salakhutdinov", "suffix": ""}], "year": null, "venue": "Proceedings of the 55th", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF17": {"ref_id": "b17", "title": "Annual Meeting of the Association for Computational Linguistics", "authors": [], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF18": {"ref_id": "b18", "title": "Constructing datasets for multi-hop reading comprehension across documents", "authors": [{"first": "Johannes", "middle": [], "last": "Welbl", "suffix": ""}, {"first": "Pontus", "middle": [], "last": "Stenetorp", "suffix": ""}, {"first": "Sebastian", "middle": [], "last": "Riedel", "suffix": ""}], "year": 2018, "venue": "Transactions of the Association for Computational Linguistics", "volume": "6", "issn": "", "pages": "287--302", "other_ids": {"DOI": ["10.1162/tacl_a_00021"]}}, "BIBREF20": {"ref_id": "b20", "title": "Who did what: A large-scale person-centered cloze dataset", "authors": [{"first": "Takeshi", "middle": [], "last": "Onishi", "suffix": ""}, {"first": "Hai", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Mohit", "middle": [], "last": "Bansal", "suffix": ""}, {"first": "Kevin", "middle": [], "last": "Gimpel", "suffix": ""}, {"first": "David", "middle": [], "last": "Mcallester", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF21": {"ref_id": "b21", "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "authors": [{"first": "Jason", "middle": [], "last": "Weston", "suffix": ""}, {"first": "Antoine", "middle": [], "last": "Bordes", "suffix": ""}, {"first": "Sumit", "middle": [], "last": "Chopra", "suffix": ""}, {"first": "Alexander", "middle": ["M"], "last": "Rush", "suffix": ""}, {"first": "Armand", "middle": [], "last": "Bart Van Merri\u00ebnboer", "suffix": ""}, {"first": "Tomas", "middle": [], "last": "Joulin", "suffix": ""}, {"first": "", "middle": [], "last": "Mikolov", "suffix": ""}], "year": 2016, "venue": "Proceedings of 4th International Conference on Learning Representations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF22": {"ref_id": "b22", "title": "WikiReading: A novel large-scale language understanding task over Wikipedia", "authors": [{"first": "Daniel", "middle": [], "last": "Hewlett", "suffix": ""}, {"first": "Alexandre", "middle": [], "last": "Lacoste", "suffix": ""}, {"first": "Llion", "middle": [], "last": "Jones", "suffix": ""}, {"first": "Illia", "middle": [], "last": "Polosukhin", "suffix": ""}, {"first": "Andrew", "middle": [], "last": "Fandrianto", "suffix": ""}, {"first": "Jay", "middle": [], "last": "Han", "suffix": ""}, {"first": "Matthew", "middle": [], "last": "Kelcey", "suffix": ""}, {"first": "David", "middle": [], "last": "Berthelot", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF23": {"ref_id": "b23", "title": "Bleu: a method for automatic evaluation of machine translation", "authors": [{"first": "Kishore", "middle": [], "last": "Papineni", "suffix": ""}, {"first": "Salim", "middle": [], "last": "Roukos", "suffix": ""}, {"first": "Todd", "middle": [], "last": "Ward", "suffix": ""}, {"first": "Wei-Jing", "middle": [], "last": "Zhu", "suffix": ""}], "year": 2002, "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF24": {"ref_id": "b24", "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "authors": [{"first": "Satanjeev", "middle": [], "last": "Banerjee", "suffix": ""}, {"first": "Alon", "middle": [], "last": "Lavie", "suffix": ""}], "year": 2005, "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF25": {"ref_id": "b25", "title": "ROUGE: A package for automatic evaluation of summaries", "authors": [{"first": "Chin-Yew", "middle": [], "last": "Lin", "suffix": ""}], "year": 2004, "venue": "Proceedings of Text Summarization Branches Out", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF26": {"ref_id": "b26", "title": "A conceptual theory of question answering", "authors": [{"first": "Wendy", "middle": ["G"], "last": "Lehnert", "suffix": ""}], "year": 1977, "venue": "Proceedings of the 5th International Joint Conference on Artificial Intelligence, IJCAI'77", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF27": {"ref_id": "b27", "title": "Populating the semantic web by macro-reading internet text", "authors": [{"first": "Matt", "middle": [], "last": "Tom", "suffix": ""}, {"first": "Justin", "middle": [], "last": "Betteridge", "suffix": ""}, {"first": "Andrew", "middle": [], "last": "Carlson", "suffix": ""}, {"first": "Estevam", "middle": [], "last": "Hruschka", "suffix": ""}, {"first": "Richard", "middle": [], "last": "Wang", "suffix": ""}], "year": 2009, "venue": "Proceedings of the 8th International Semantic Web Conference", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF28": {"ref_id": "b28", "title": "Procedures as a representation for data in a computer program for understanding natural language", "authors": [{"first": "Terry", "middle": [], "last": "Winograd", "suffix": ""}], "year": 1971, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF29": {"ref_id": "b29", "title": "The lunar sciences natural language system: final report", "authors": [{"first": "A", "middle": [], "last": "William", "suffix": ""}, {"first": "", "middle": [], "last": "Woods", "suffix": ""}, {"first": "M", "middle": [], "last": "Ronald", "suffix": ""}, {"first": "Bonnie", "middle": [], "last": "Kaplan", "suffix": ""}, {"first": "", "middle": [], "last": "Nash-Webber", "suffix": ""}], "year": 1972, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF30": {"ref_id": "b30", "title": "Baseball: an automatic question-answerer", "authors": [{"first": "Alice", "middle": ["K"], "last": "Bert F Green", "suffix": ""}, {"first": "Carol", "middle": [], "last": "Wolf", "suffix": ""}, {"first": "Kenneth", "middle": [], "last": "Chomsky", "suffix": ""}, {"first": "", "middle": [], "last": "Laughery", "suffix": ""}], "year": 1961, "venue": "Proceedings of Western joint IRE-AIEE-ACM computer conference", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF31": {"ref_id": "b31", "title": "Deep read: A reading comprehension system", "authors": [{"first": "Lynette", "middle": [], "last": "Hirschman", "suffix": ""}, {"first": "Marc", "middle": [], "last": "Light", "suffix": ""}, {"first": "Eric", "middle": [], "last": "Breck", "suffix": ""}, {"first": "John", "middle": ["D"], "last": "Burger", "suffix": ""}], "year": 1999, "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF32": {"ref_id": "b32", "title": "A rule-based question answering system for reading comprehension tests", "authors": [{"first": "Ellen", "middle": [], "last": "Riloff", "suffix": ""}, {"first": "Michael", "middle": [], "last": "Thelen", "suffix": ""}], "year": 2000, "venue": "Proceedings of the 2000 ANLP/NAACL Workshop on Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Sytems, ANLP/NAACL-ReadingComp '00", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF34": {"ref_id": "b34", "title": "Reading comprehension programs in a statisticallanguage-processing class", "authors": [{"first": "Shawn", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Lisa", "middle": [], "last": "Zeller", "suffix": ""}, {"first": "", "middle": [], "last": "Zorn", "suffix": ""}], "year": 2000, "venue": "Proceedings of the 2000 ANLP/NAACL Workshop on Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems, ANLP/NAACL-ReadingComp '00", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF35": {"ref_id": "b35", "title": "Searching the world wide web", "authors": [{"first": "Steve", "middle": [], "last": "Lawrence", "suffix": ""}, {"first": "C. Lee", "middle": [], "last": "Giles", "suffix": ""}], "year": 1998, "venue": "Science", "volume": "280", "issn": "5360", "pages": "98--100", "other_ids": {"DOI": ["10.1126/science.280.5360.98"]}}, "BIBREF36": {"ref_id": "b36", "title": "Estimating search engine index size variability: a 9-year longitudinal study", "authors": [{"first": "Antal", "middle": [], "last": "Van Den", "suffix": ""}, {"first": "Toine", "middle": [], "last": "Bosch", "suffix": ""}, {"first": "Maurice", "middle": [], "last": "Bogers", "suffix": ""}, {"first": "", "middle": [], "last": "De Kunder", "suffix": ""}], "year": 2016, "venue": "Scientometrics", "volume": "107", "issn": "", "pages": "839--856", "other_ids": {"DOI": ["10.1007/s11192-016-1863-z"]}}, "BIBREF37": {"ref_id": "b37", "title": "SQuAD: 100,000+ questions for machine comprehension of text", "authors": [{"first": "Pranav", "middle": [], "last": "Rajpurkar", "suffix": ""}, {"first": "Jian", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Konstantin", "middle": [], "last": "Lopyrev", "suffix": ""}, {"first": "Percy", "middle": [], "last": "Liang", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF38": {"ref_id": "b38", "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering", "authors": [{"first": "Zhilin", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Peng", "middle": [], "last": "Qi", "suffix": ""}, {"first": "Saizheng", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Yoshua", "middle": [], "last": "Bengio", "suffix": ""}, {"first": "William", "middle": [], "last": "Cohen", "suffix": ""}, {"first": "Ruslan", "middle": [], "last": "Salakhutdinov", "suffix": ""}, {"first": "Christopher", "middle": ["D"], "last": "Manning", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF39": {"ref_id": "b39", "title": "WordNet: An Electronic Lexical Database. Language, Speech, and Communication", "authors": [], "year": 1998, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF40": {"ref_id": "b40", "title": "Wikidata: A Free Collaborative Knowledgebase", "authors": [{"first": "Denny", "middle": [], "last": "Vrande\u010di\u0107", "suffix": ""}, {"first": "Markus", "middle": [], "last": "Kr\u00f6tzsch", "suffix": ""}], "year": 2014, "venue": "Communications of the ACM", "volume": "57", "issn": "10", "pages": "78--85", "other_ids": {"DOI": ["10.1145/2629489"]}}, "BIBREF41": {"ref_id": "b41", "title": "On what there is. The review of metaphysics", "authors": [{"first": "", "middle": [], "last": "Willard V Quine", "suffix": ""}], "year": 1948, "venue": "", "volume": "", "issn": "", "pages": "21--38", "other_ids": {}}, "BIBREF42": {"ref_id": "b42", "title": "Ontological promiscuity", "authors": [{"first": "Jerry", "middle": ["R"], "last": "Hobbs", "suffix": ""}], "year": 1985, "venue": "Proceedings of 23rd Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF43": {"ref_id": "b43", "title": "Freebase: A collaboratively created graph database for structuring human knowledge", "authors": [{"first": "Kurt", "middle": [], "last": "Bollacker", "suffix": ""}, {"first": "Colin", "middle": [], "last": "Evans", "suffix": ""}, {"first": "Praveen", "middle": [], "last": "Paritosh", "suffix": ""}, {"first": "Tim", "middle": [], "last": "Sturge", "suffix": ""}, {"first": "Jamie", "middle": [], "last": "Taylor", "suffix": ""}], "year": 2008, "venue": "Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD '08", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF44": {"ref_id": "b44", "title": "Freebase data dumps", "authors": [{"first": "", "middle": [], "last": "Google", "suffix": ""}], "year": 2013, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF45": {"ref_id": "b45", "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations", "authors": [{"first": "Felix", "middle": [], "last": "Hill", "suffix": ""}, {"first": "Antoine", "middle": [], "last": "Bordes", "suffix": ""}, {"first": "Sumit", "middle": [], "last": "Chopra", "suffix": ""}, {"first": "Jason", "middle": [], "last": "Weston", "suffix": ""}], "year": 2016, "venue": "Proceedings of 4th International Conference on Learning Representations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF46": {"ref_id": "b46", "title": "Introducing the knowledge graph: things, not strings. Official Google blog", "authors": [{"first": "Amit", "middle": [], "last": "Singhal", "suffix": ""}], "year": 2012, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF47": {"ref_id": "b47", "title": "WikiQA: A challenge dataset for open-domain question answering", "authors": [{"first": "Yi", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Wen-Tau", "middle": [], "last": "Yih", "suffix": ""}, {"first": "Christopher", "middle": [], "last": "Meek", "suffix": ""}], "year": 2015, "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF48": {"ref_id": "b48", "title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "authors": [{"first": "Matthew", "middle": [], "last": "Richardson", "suffix": ""}, {"first": "J", "middle": ["C"], "last": "Christopher", "suffix": ""}, {"first": "Erin", "middle": [], "last": "Burges", "suffix": ""}, {"first": "", "middle": [], "last": "Renshaw", "suffix": ""}], "year": 2013, "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF49": {"ref_id": "b49", "title": "Ms marco: A human generated machine reading comprehension dataset", "authors": [{"first": "Tri", "middle": [], "last": "Nguyen", "suffix": ""}, {"first": "Mir", "middle": [], "last": "Rosenberg", "suffix": ""}, {"first": "Xia", "middle": [], "last": "Song", "suffix": ""}, {"first": "Jianfeng", "middle": [], "last": "Gao", "suffix": ""}, {"first": "Saurabh", "middle": [], "last": "Tiwary", "suffix": ""}, {"first": "Rangan", "middle": [], "last": "Majumder", "suffix": ""}, {"first": "Li", "middle": [], "last": "Deng", "suffix": ""}], "year": 2016, "venue": "The Computing Research Repository (CoRR)", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF50": {"ref_id": "b50", "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension", "authors": [{"first": "Mandar", "middle": [], "last": "Joshi", "suffix": ""}, {"first": "Eunsol", "middle": [], "last": "Choi", "suffix": ""}, {"first": "Daniel", "middle": ["S"], "last": "Weld", "suffix": ""}, {"first": "Luke", "middle": [], "last": "Zettlemoyer", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF51": {"ref_id": "b51", "title": "The narrativeqa reading comprehension challenge", "authors": [{"first": "Tom\u00e1\u0161", "middle": [], "last": "Ko\u010disk\u00fd", "suffix": ""}, {"first": "Jonathan", "middle": [], "last": "Schwarz", "suffix": ""}, {"first": "Phil", "middle": [], "last": "Blunsom", "suffix": ""}, {"first": "Chris", "middle": [], "last": "Dyer", "suffix": ""}, {"first": "Karl", "middle": [], "last": "Hermann", "suffix": ""}, {"first": "G\u00e1bor", "middle": [], "last": "Melis", "suffix": ""}, {"first": "Edward", "middle": [], "last": "Grefenstette", "suffix": ""}], "year": 2017, "venue": "Transactions of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1162/tacl_a_00023"]}}, "BIBREF52": {"ref_id": "b52", "title": "The Stanford CoreNLP Natural Language Processing Toolkit", "authors": [{"first": "Christopher", "middle": ["D"], "last": "Manning", "suffix": ""}, {"first": "Mihai", "middle": [], "last": "Surdeanu", "suffix": ""}, {"first": "John", "middle": [], "last": "Bauer", "suffix": ""}, {"first": "Jenny", "middle": [], "last": "Finkel", "suffix": ""}, {"first": "J", "middle": [], "last": "Steven", "suffix": ""}, {"first": "David", "middle": [], "last": "Bethard", "suffix": ""}, {"first": "", "middle": [], "last": "Mcclosky", "suffix": ""}], "year": 2014, "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF53": {"ref_id": "b53", "title": "Accurate unlexicalized parsing", "authors": [{"first": "Dan", "middle": [], "last": "Klein", "suffix": ""}, {"first": "Christopher", "middle": ["D"], "last": "Manning", "suffix": ""}], "year": 2003, "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF54": {"ref_id": "b54", "title": "Lucene in Action, Second Edition", "authors": [{"first": "Michael", "middle": [], "last": "Mccandless", "suffix": ""}, {"first": "Erik", "middle": [], "last": "Hatcher", "suffix": ""}, {"first": "Otis", "middle": [], "last": "Gospodnetic", "suffix": ""}], "year": 2010, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF55": {"ref_id": "b55", "title": "Machine comprehension with syntax, frames, and semantics", "authors": [{"first": "Hai", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Mohit", "middle": [], "last": "Bansal", "suffix": ""}, {"first": "Kevin", "middle": [], "last": "Gimpel", "suffix": ""}, {"first": "David", "middle": [], "last": "Mcallester", "suffix": ""}], "year": 2015, "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF56": {"ref_id": "b56", "title": "Emergent predication structure in hidden state vectors of neural readers", "authors": [{"first": "Takeshi", "middle": [], "last": "Onishi", "suffix": ""}, {"first": "Hai", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Kevin", "middle": [], "last": "Gimpel", "suffix": ""}, {"first": "David", "middle": [], "last": "Mcallester", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 2nd Workshop on Representation Learning for NLP", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF57": {"ref_id": "b57", "title": "End-to-end memory networks", "authors": [{"first": "Sainbayar", "middle": [], "last": "Sukhbaatar", "suffix": ""}, {"first": "Jason", "middle": [], "last": "Weston", "suffix": ""}, {"first": "Rob", "middle": [], "last": "Fergus", "suffix": ""}], "year": 2015, "venue": "Proceedings of Advances in Neural Information Processing Systems", "volume": "28", "issn": "", "pages": "", "other_ids": {}}, "BIBREF58": {"ref_id": "b58", "title": "Attention-over-attention neural networks for reading comprehension", "authors": [{"first": "Yiming", "middle": [], "last": "Cui", "suffix": ""}, {"first": "Zhipeng", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Si", "middle": [], "last": "Wei", "suffix": ""}, {"first": "Shijin", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Ting", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Guoping", "middle": [], "last": "Hu", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF59": {"ref_id": "b59", "title": "Long short-term memory", "authors": [{"first": "Sepp", "middle": [], "last": "Hochreiter", "suffix": ""}, {"first": "J\u00fcrgen", "middle": [], "last": "Schmidhuber", "suffix": ""}], "year": null, "venue": "Neural computation", "volume": "9", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1162/neco.1997.9.8.1735"]}}, "BIBREF60": {"ref_id": "b60", "title": "Attention is all you need", "authors": [{"first": "Ashish", "middle": [], "last": "Vaswani", "suffix": ""}, {"first": "Noam", "middle": [], "last": "Shazeer", "suffix": ""}, {"first": "Niki", "middle": [], "last": "Parmar", "suffix": ""}, {"first": "Jakob", "middle": [], "last": "Uszkoreit", "suffix": ""}, {"first": "Llion", "middle": [], "last": "Jones", "suffix": ""}, {"first": "Aidan", "middle": ["N"], "last": "Gomez", "suffix": ""}, {"first": "Illia", "middle": [], "last": "Kaiser", "suffix": ""}, {"first": "", "middle": [], "last": "Polosukhin", "suffix": ""}], "year": 2017, "venue": "Proceedings of Advances in Neural Information Processing Systems", "volume": "30", "issn": "", "pages": "", "other_ids": {}}, "BIBREF61": {"ref_id": "b61", "title": "Token-level dynamic self-attention network for multi-passage reading comprehension", "authors": [{"first": "Yimeng", "middle": [], "last": "Zhuang", "suffix": ""}, {"first": "Huadong", "middle": [], "last": "Wang", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF62": {"ref_id": "b62", "title": "Xception: Deep learning with depthwise separable convolutions", "authors": [{"first": "Fran\u00e7ois", "middle": [], "last": "Chollet", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF63": {"ref_id": "b63", "title": "Generalized autoregressive pretraining for language understanding", "authors": [{"first": "Zhilin", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Zihang", "middle": [], "last": "Dai", "suffix": ""}, {"first": "Yiming", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Jaime", "middle": [], "last": "Carbonell", "suffix": ""}, {"first": "R", "middle": [], "last": "Russ", "suffix": ""}, {"first": "Quoc V", "middle": [], "last": "Salakhutdinov", "suffix": ""}, {"first": "", "middle": [], "last": "Le", "suffix": ""}, {"first": "", "middle": [], "last": "Xlnet", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF64": {"ref_id": "b64", "title": "Proceedings of Advances in Neural Information Processing Systems", "authors": [], "year": 2019, "venue": "", "volume": "32", "issn": "", "pages": "", "other_ids": {}}, "BIBREF65": {"ref_id": "b65", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "authors": [{"first": "Zihang", "middle": [], "last": "Dai", "suffix": ""}, {"first": "Zhilin", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Yiming", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Jaime", "middle": [], "last": "Carbonell", "suffix": ""}, {"first": "Quoc", "middle": [], "last": "Le", "suffix": ""}, {"first": "Ruslan", "middle": [], "last": "Salakhutdinov", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF66": {"ref_id": "b66", "title": "Reformer: The efficient transformer", "authors": [{"first": "Nikita", "middle": [], "last": "Kitaev", "suffix": ""}, {"first": "Lukasz", "middle": [], "last": "Kaiser", "suffix": ""}, {"first": "Anselm", "middle": [], "last": "Levskaya", "suffix": ""}], "year": 2020, "venue": "Proceedings of 8th International Conference on Learning Representations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF67": {"ref_id": "b67", "title": "Longformer: The long-document transformer", "authors": [{"first": "Iz", "middle": [], "last": "Beltagy", "suffix": ""}, {"first": "Matthew", "middle": ["E"], "last": "Peters", "suffix": ""}, {"first": "Arman", "middle": [], "last": "Cohan", "suffix": ""}], "year": 2020, "venue": "The Computing Research Repository (CoRR)", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF68": {"ref_id": "b68", "title": "Question answering by reasoning across documents with graph convolutional networks", "authors": [{"first": "Nicola", "middle": [], "last": "De Cao", "suffix": ""}, {"first": "Wilker", "middle": [], "last": "Aziz", "suffix": ""}, {"first": "Ivan", "middle": [], "last": "Titov", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF69": {"ref_id": "b69", "title": "Deep contextualized word representations", "authors": [{"first": "Matthew", "middle": [], "last": "Peters", "suffix": ""}, {"first": "Mark", "middle": [], "last": "Neumann", "suffix": ""}, {"first": "Mohit", "middle": [], "last": "Iyyer", "suffix": ""}, {"first": "Matt", "middle": [], "last": "Gardner", "suffix": ""}, {"first": "Christopher", "middle": [], "last": "Clark", "suffix": ""}, {"first": "Kenton", "middle": [], "last": "Lee", "suffix": ""}, {"first": "Luke", "middle": [], "last": "Zettlemoyer", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF70": {"ref_id": "b70", "title": "Multi-hop question answering via reasoning chains", "authors": [{"first": "Jifan", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Greg", "middle": [], "last": "Shih Ting Lin", "suffix": ""}, {"first": "", "middle": [], "last": "Durrett", "suffix": ""}], "year": 2019, "venue": "The Computing Research Repository (CoRR)", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF71": {"ref_id": "b71", "title": "Pointer networks", "authors": [{"first": "Oriol", "middle": [], "last": "Vinyals", "suffix": ""}, {"first": "Meire", "middle": [], "last": "Fortunato", "suffix": ""}, {"first": "Navdeep", "middle": [], "last": "Jaitly", "suffix": ""}], "year": 2015, "venue": "Proceedings of Advances in Neural Information Processing Systems", "volume": "28", "issn": "", "pages": "", "other_ids": {}}, "BIBREF72": {"ref_id": "b72", "title": "Coarse-grain finegrain coattention network for multi-evidence question answering", "authors": [{"first": "Victor", "middle": [], "last": "Zhong", "suffix": ""}, {"first": "Caiming", "middle": [], "last": "Xiong", "suffix": ""}, {"first": "Nitish", "middle": [], "last": "Keskar", "suffix": ""}, {"first": "Richard", "middle": [], "last": "Socher", "suffix": ""}], "year": 2019, "venue": "Proceedings of 7th International Conference on Learning Representations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF73": {"ref_id": "b73", "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "authors": [{"first": "Kyunghyun", "middle": [], "last": "Cho", "suffix": ""}, {"first": "Caglar", "middle": [], "last": "Bart Van Merri\u00ebnboer", "suffix": ""}, {"first": "Dzmitry", "middle": [], "last": "Gulcehre", "suffix": ""}, {"first": "Fethi", "middle": [], "last": "Bahdanau", "suffix": ""}, {"first": "Holger", "middle": [], "last": "Bougares", "suffix": ""}, {"first": "Yoshua", "middle": [], "last": "Schwenk", "suffix": ""}, {"first": "", "middle": [], "last": "Bengio", "suffix": ""}], "year": 2014, "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF75": {"ref_id": "b75", "title": "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs", "authors": [], "year": 2019, "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF76": {"ref_id": "b76", "title": "Neural models for reasoning over multiple mentions using coreference", "authors": [{"first": "Bhuwan", "middle": [], "last": "Dhingra", "suffix": ""}, {"first": "Qiao", "middle": [], "last": "Jin", "suffix": ""}, {"first": "Zhilin", "middle": [], "last": "Yang", "suffix": ""}, {"first": "William", "middle": [], "last": "Cohen", "suffix": ""}, {"first": "Ruslan", "middle": [], "last": "Salakhutdinov", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF77": {"ref_id": "b77", "title": "BERT: Pretraining of deep bidirectional transformers for language understanding", "authors": [{"first": "Jacob", "middle": [], "last": "Devlin", "suffix": ""}, {"first": "Ming-Wei", "middle": [], "last": "Chang", "suffix": ""}, {"first": "Kenton", "middle": [], "last": "Lee", "suffix": ""}, {"first": "Kristina", "middle": [], "last": "Toutanova", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF78": {"ref_id": "b78", "title": "Dropout: A simple way to prevent neural networks from overfitting", "authors": [{"first": "Nitish", "middle": [], "last": "Srivastava", "suffix": ""}, {"first": "Geoffrey", "middle": [], "last": "Hinton", "suffix": ""}, {"first": "Alex", "middle": [], "last": "Krizhevsky", "suffix": ""}, {"first": "Ilya", "middle": [], "last": "Sutskever", "suffix": ""}, {"first": "Ruslan", "middle": [], "last": "Salakhutdinov", "suffix": ""}], "year": 2014, "venue": "Journal of Machine Learning Research", "volume": "15", "issn": "56", "pages": "1929--1958", "other_ids": {}}, "BIBREF79": {"ref_id": "b79", "title": "Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. The Computing Research Repository (CoRR)", "authors": [{"first": "Priya", "middle": [], "last": "Goyal", "suffix": ""}, {"first": "Piotr", "middle": [], "last": "Doll\u00e1r", "suffix": ""}, {"first": "Ross", "middle": ["B"], "last": "Girshick", "suffix": ""}, {"first": "Pieter", "middle": [], "last": "Noordhuis", "suffix": ""}, {"first": "Lukasz", "middle": [], "last": "Wesolowski", "suffix": ""}, {"first": "Aapo", "middle": [], "last": "Kyrola", "suffix": ""}, {"first": "Andrew", "middle": [], "last": "Tulloch", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF80": {"ref_id": "b80", "title": "Adam: A method for stochastic optimization", "authors": [{"first": "P", "middle": [], "last": "Diederik", "suffix": ""}, {"first": "Jimmy", "middle": [], "last": "Kingma", "suffix": ""}, {"first": "", "middle": [], "last": "Ba", "suffix": ""}], "year": 2015, "venue": "Proceedings of 3rd International Conference on Learning Representations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF81": {"ref_id": "b81", "title": "Relation extraction with weakly supervised learning based on process-structure-property-performance reciprocity", "authors": [{"first": "Takeshi", "middle": [], "last": "Onishi", "suffix": ""}, {"first": "Takuya", "middle": [], "last": "Kadohira", "suffix": ""}, {"first": "Ikumu", "middle": [], "last": "Watanabe", "suffix": ""}], "year": 2018, "venue": "Science and Technology of Advanced Materials", "volume": "19", "issn": "1", "pages": "649--659", "other_ids": {"DOI": ["10.1080/14686996.2018.1500852"], "PMID": ["30245757"]}}, "BIBREF82": {"ref_id": "b82", "title": "Cybermaterials: Materials by design and accelerated insertion of materials", "authors": [{"first": "Wei", "middle": [], "last": "Xiong", "suffix": ""}, {"first": "Gregory", "middle": ["B"], "last": "Olson", "suffix": ""}], "year": 2016, "venue": "npj Computational Materials", "volume": "2", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1038/npjcompumats.2015.9"]}}, "BIBREF83": {"ref_id": "b83", "title": "Designing a new material world", "authors": [{"first": "Gregory", "middle": ["B"], "last": "Olson", "suffix": ""}], "year": 2000, "venue": "Science", "volume": "288", "issn": "5468", "pages": "993--998", "other_ids": {"DOI": ["10.1126/science.288.5468.993"]}}, "BIBREF84": {"ref_id": "b84", "title": "Inorganic materials database for exploring the nature of material", "authors": [{"first": "Yibin", "middle": [], "last": "Xu", "suffix": ""}, {"first": "Masayoshi", "middle": [], "last": "Yamazaki", "suffix": ""}, {"first": "Pierre", "middle": [], "last": "Villars", "suffix": ""}], "year": 2011, "venue": "Japanese Journal of Applied Physics", "volume": "50", "issn": "11S", "pages": "11--13", "other_ids": {}}, "BIBREF85": {"ref_id": "b85", "title": "Distant supervision for relation extraction without labeled data", "authors": [{"first": "Mike", "middle": [], "last": "Mintz", "suffix": ""}, {"first": "Steven", "middle": [], "last": "Bills", "suffix": ""}, {"first": "Rion", "middle": [], "last": "Snow", "suffix": ""}, {"first": "Dan", "middle": [], "last": "Jurafsky", "suffix": ""}], "year": 2009, "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF86": {"ref_id": "b86", "title": "Knowledge-based weak supervision for information extraction of overlapping relations", "authors": [{"first": "Raphael", "middle": [], "last": "Hoffmann", "suffix": ""}, {"first": "Congle", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Xiao", "middle": [], "last": "Ling", "suffix": ""}, {"first": "Luke", "middle": [], "last": "Zettlemoyer", "suffix": ""}, {"first": "Daniel", "middle": ["S"], "last": "Weld", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF87": {"ref_id": "b87", "title": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "authors": [], "year": 2011, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF89": {"ref_id": "b89", "title": "Multi-instance multi-label learning for relation extraction", "authors": [], "year": null, "venue": "Proceedings of the 2012", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF90": {"ref_id": "b90", "title": "Natural Language Processing and Computational Natural Language Learning", "authors": [], "year": 2012, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF92": {"ref_id": "b92", "title": "Distant supervision for relation extraction with matrix completion", "authors": [{"first": "Chang", "middle": [], "last": "", "suffix": ""}], "year": 2014, "venue": "Pro-Baltimore (MD)", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF93": {"ref_id": "b93", "title": "Modeling Relations and Their Mentions without Labeled Text", "authors": [{"first": "Sebastian", "middle": [], "last": "Riedel", "suffix": ""}, {"first": "Limin", "middle": [], "last": "Yao", "suffix": ""}, {"first": "Andrew", "middle": [], "last": "Mccallum", "suffix": ""}], "year": 2010, "venue": "Proceedings of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF94": {"ref_id": "b94", "title": "Deep residual learning for weakly-supervised relation extraction", "authors": [{"first": "Yi Yao", "middle": [], "last": "Huang", "suffix": ""}, {"first": "William", "middle": ["Yang"], "last": "Wang", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF95": {"ref_id": "b95", "title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "authors": [{"first": "Daojian", "middle": [], "last": "Zeng", "suffix": ""}, {"first": "Kang", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Yubo", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Jun", "middle": [], "last": "Zhao", "suffix": ""}], "year": 2015, "venue": "Proceedings of the", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF96": {"ref_id": "b96", "title": "Conference on Empirical Methods in Natural Language Processing", "authors": [], "year": 2015, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF97": {"ref_id": "b97", "title": "Neural relation extraction with selective attention over instances", "authors": [{"first": "Yankai", "middle": [], "last": "Lin", "suffix": ""}, {"first": "Shiqi", "middle": [], "last": "Shen", "suffix": ""}, {"first": "Zhiyuan", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Huanbo", "middle": [], "last": "Luan", "suffix": ""}, {"first": "Maosong", "middle": [], "last": "Sun", "suffix": ""}], "year": 2016, "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF98": {"ref_id": "b98", "title": "Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions", "authors": [{"first": "Guoliang", "middle": [], "last": "Ji", "suffix": ""}, {"first": "Kang", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Shizhu", "middle": [], "last": "He", "suffix": ""}, {"first": "Jun", "middle": [], "last": "Zhao", "suffix": ""}], "year": 2017, "venue": "Proceedings of Association for the Advancement of Artificial Intelligence", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF99": {"ref_id": "b99", "title": "A soft-label method for noise-tolerant distantly supervised relation extraction", "authors": [{"first": "Tianyu", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Kexiang", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Baobao", "middle": [], "last": "Chang", "suffix": ""}, {"first": "Zhifang", "middle": [], "last": "Sui", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF100": {"ref_id": "b100", "title": "GloVe: Global vectors for word representation", "authors": [{"first": "Jeffrey", "middle": [], "last": "Pennington", "suffix": ""}, {"first": "Richard", "middle": [], "last": "Socher", "suffix": ""}, {"first": "Christopher", "middle": [], "last": "Manning", "suffix": ""}], "year": 2014, "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF102": {"ref_id": "b102", "title": "Towards a knowledge graph for science", "authors": [{"first": "Maria", "middle": ["Esther"], "last": "Vidal", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics, WIMS '18", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF103": {"ref_id": "b103", "title": "The role of \"condition\": A novel scientific knowledge graph representation and construction model", "authors": [{"first": "Tianwen", "middle": [], "last": "Jiang", "suffix": ""}, {"first": "Tong", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "Bing", "middle": [], "last": "Qin", "suffix": ""}, {"first": "Ting", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Nitesh", "middle": ["V"], "last": "Chawla", "suffix": ""}, {"first": "Meng", "middle": [], "last": "Jiang", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD '19", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF105": {"ref_id": "b105", "title": "Trpv5/v6 channels mediate ca(2+) influx in jurkat t cells under the control of extracellular ph", "authors": [], "year": 2016, "venue": "Journal of cellular biochemistry", "volume": "117", "issn": "1", "pages": "197--206", "other_ids": {"DOI": ["10.1002/jcb.25264"]}}, "BIBREF106": {"ref_id": "b106", "title": "An information extraction and knowledge graph platform for accelerating biochemical discoveries", "authors": [{"first": "Matteo", "middle": [], "last": "Manica", "suffix": ""}, {"first": "Christoph", "middle": [], "last": "Auer", "suffix": ""}, {"first": "Federico", "middle": [], "last": "Weber", "suffix": ""}, {"first": "Michele", "middle": [], "last": "Zipoli", "suffix": ""}, {"first": "Peter", "middle": [], "last": "Dolfi", "suffix": ""}, {"first": "Teodoro", "middle": [], "last": "Staar", "suffix": ""}, {"first": "Costas", "middle": [], "last": "Laino", "suffix": ""}, {"first": "Akihiro", "middle": [], "last": "Bekas", "suffix": ""}, {"first": "Hiroki", "middle": [], "last": "Fujita", "suffix": ""}, {"first": "Shuichi", "middle": [], "last": "Toda", "suffix": ""}, {"first": "Yasumitsu", "middle": [], "last": "Hirose", "suffix": ""}, {"first": "", "middle": [], "last": "Orii", "suffix": ""}], "year": 2019, "venue": "The Computing Research Repository (CoRR)", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF107": {"ref_id": "b107", "title": "propnet: A knowledge graph for materials science. Matter, 2", "authors": [{"first": "David", "middle": [], "last": "Mrdjenovich", "suffix": ""}, {"first": "Matthew", "middle": [], "last": "Horton", "suffix": ""}, {"first": "Joseph", "middle": [], "last": "Montoya", "suffix": ""}, {"first": "Christian", "middle": [], "last": "Legaspi", "suffix": ""}, {"first": "Shyam", "middle": [], "last": "Dwaraknath", "suffix": ""}, {"first": "Vahe", "middle": [], "last": "Tshitoyan", "suffix": ""}, {"first": "Anubhav", "middle": [], "last": "Jain", "suffix": ""}, {"first": "Kristin", "middle": [], "last": "Persson", "suffix": ""}], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1016/j.matt.2019.11.013"]}}, "BIBREF108": {"ref_id": "b108", "title": "Towards the bosch materials science knowledge base", "authors": [{"first": "Jannik", "middle": [], "last": "Str\u00f6tgen", "suffix": ""}, {"first": "Trung-Kien", "middle": [], "last": "Tran", "suffix": ""}, {"first": "Annemarie", "middle": [], "last": "Friedrich", "suffix": ""}, {"first": "Dragan", "middle": [], "last": "Milchevski", "suffix": ""}, {"first": "Federico", "middle": [], "last": "Tomazic", "suffix": ""}, {"first": "Anika", "middle": [], "last": "Marusczyk", "suffix": ""}, {"first": "Heike", "middle": [], "last": "Adel", "suffix": ""}, {"first": "Daria", "middle": [], "last": "Stepanova", "suffix": ""}, {"first": "Felix", "middle": [], "last": "Hildebrand", "suffix": ""}, {"first": "Evgeny", "middle": [], "last": "Kharlamov", "suffix": ""}], "year": 2019, "venue": "Proceedings of ISWC Satellites", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF109": {"ref_id": "b109", "title": "SemEval 2017 task 10: ScienceIE -extracting keyphrases and relations from scientific publications", "authors": [{"first": "Isabelle", "middle": [], "last": "Augenstein", "suffix": ""}, {"first": "Mrinal", "middle": [], "last": "Das", "suffix": ""}, {"first": "Sebastian", "middle": [], "last": "Riedel", "suffix": ""}, {"first": "Lakshmi", "middle": [], "last": "Vikraman", "suffix": ""}, {"first": "Andrew", "middle": [], "last": "Mccallum", "suffix": ""}], "year": 2017, "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF110": {"ref_id": "b110", "title": "SemEval-2018 task 7: Semantic relation extraction and classification in scientific papers", "authors": [{"first": "Kata", "middle": [], "last": "G\u00e1bor", "suffix": ""}, {"first": "Davide", "middle": [], "last": "Buscaldi", "suffix": ""}, {"first": "Anne-Kathrin", "middle": [], "last": "Schumann", "suffix": ""}, {"first": "Behrang", "middle": [], "last": "Qasemizadeh", "suffix": ""}, {"first": "Ha\u00effa", "middle": [], "last": "Zargayouna", "suffix": ""}, {"first": "Thierry", "middle": [], "last": "Charnois", "suffix": ""}], "year": 2018, "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF111": {"ref_id": "b111", "title": "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction", "authors": [{"first": "Yi", "middle": [], "last": "Luan", "suffix": ""}, {"first": "Luheng", "middle": [], "last": "He", "suffix": ""}, {"first": "Mari", "middle": [], "last": "Ostendorf", "suffix": ""}, {"first": "Hannaneh", "middle": [], "last": "Hajishirzi", "suffix": ""}], "year": null, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF112": {"ref_id": "b112", "title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing", "authors": [], "year": 2018, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF113": {"ref_id": "b113", "title": "The SOFC-exp corpus and neural approaches to information extraction in the materials science domain", "authors": [{"first": "Annemarie", "middle": [], "last": "Friedrich", "suffix": ""}, {"first": "Heike", "middle": [], "last": "Adel", "suffix": ""}, {"first": "Federico", "middle": [], "last": "Tomazic", "suffix": ""}, {"first": "Johannes", "middle": [], "last": "Hingerl", "suffix": ""}, {"first": "Renou", "middle": [], "last": "Benteau", "suffix": ""}, {"first": "Anika", "middle": [], "last": "Marusczyk", "suffix": ""}, {"first": "Lukas", "middle": [], "last": "Lange", "suffix": ""}], "year": null, "venue": "Proceedings of the 58th", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF114": {"ref_id": "b114", "title": "Annual Meeting of the Association for Computational Linguistics", "authors": [], "year": 2020, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF115": {"ref_id": "b115", "title": "The materials science procedural text corpus: Annotating materials synthesis procedures with shallow semantic structures", "authors": [{"first": "Zachary", "middle": [], "last": "Sheshera Mysore", "suffix": ""}, {"first": "Edward", "middle": [], "last": "Jensen", "suffix": ""}, {"first": "Kevin", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Haw-Shiuan", "middle": [], "last": "Huang", "suffix": ""}, {"first": "Emma", "middle": [], "last": "Chang", "suffix": ""}, {"first": "Jeffrey", "middle": [], "last": "Strubell", "suffix": ""}, {"first": "Andrew", "middle": [], "last": "Flanigan", "suffix": ""}, {"first": "Elsa", "middle": [], "last": "Mccallum", "suffix": ""}, {"first": "", "middle": [], "last": "Olivetti", "suffix": ""}], "year": 2019, "venue": "Proceedings of the 13th Linguistic Annotation Workshop", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF116": {"ref_id": "b116", "title": "Text-mined dataset of inorganic materials synthesis recipes. Scientific Data", "authors": [{"first": "Olga", "middle": [], "last": "Kononova", "suffix": ""}, {"first": "Haoyan", "middle": [], "last": "Huo", "suffix": ""}, {"first": "Tanjin", "middle": [], "last": "He", "suffix": ""}, {"first": "Ziqin", "middle": [], "last": "Rong", "suffix": ""}, {"first": "Tiago", "middle": [], "last": "Botari", "suffix": ""}, {"first": "Wenhao", "middle": [], "last": "Sun", "suffix": ""}, {"first": "Vahe", "middle": [], "last": "Tshitoyan", "suffix": ""}, {"first": "Gerbrand", "middle": [], "last": "Ceder", "suffix": ""}], "year": 2019, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1038/s41597-019-0224-1"]}}, "BIBREF117": {"ref_id": "b117", "title": "Automatically extracting action graphs from materials science synthesis procedures", "authors": [{"first": "Sheshera", "middle": [], "last": "Mysore", "suffix": ""}, {"first": "Edward", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Emma", "middle": [], "last": "Strubell", "suffix": ""}, {"first": "Ao", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Haw-Shiuan", "middle": [], "last": "Chang", "suffix": ""}, {"first": "Srikrishna", "middle": [], "last": "Kompella", "suffix": ""}, {"first": "Kevin", "middle": [], "last": "Huang", "suffix": ""}, {"first": "Andrew", "middle": [], "last": "Mccallum", "suffix": ""}, {"first": "Elsa", "middle": [], "last": "Olivetti", "suffix": ""}], "year": 2017, "venue": "The Computing Research Repository (CoRR)", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF118": {"ref_id": "b118", "title": "Mise en place: Unsupervised interpretation of instructional recipes", "authors": [{"first": "Chlo\u00e9", "middle": [], "last": "Kiddon", "suffix": ""}, {"first": "Thandavam", "middle": [], "last": "Ganesa", "suffix": ""}, {"first": "Luke", "middle": [], "last": "Ponnuraj", "suffix": ""}, {"first": "Yejin", "middle": [], "last": "Zettlemoyer", "suffix": ""}, {"first": "", "middle": [], "last": "Choi", "suffix": ""}], "year": 2015, "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF119": {"ref_id": "b119", "title": "Named entity recognition and normalization applied to largescale information extraction from the materials science literature", "authors": [{"first": "L", "middle": [], "last": "Weston", "suffix": ""}, {"first": "V", "middle": [], "last": "Tshitoyan", "suffix": ""}, {"first": "J", "middle": [], "last": "Dagdelen", "suffix": ""}, {"first": "O", "middle": [], "last": "Kononova", "suffix": ""}, {"first": "A", "middle": [], "last": "Trewartha", "suffix": ""}, {"first": "K", "middle": ["A"], "last": "Persson", "suffix": ""}, {"first": "G", "middle": [], "last": "Ceder", "suffix": ""}, {"first": "A", "middle": [], "last": "Jain", "suffix": ""}], "year": 2019, "venue": "Journal of Chemical Information and Modeling", "volume": "59", "issn": "9", "pages": "3692--3702", "other_ids": {"DOI": ["10.1021/acs.jcim.9b00470"], "PMID": ["31361962"]}}, "BIBREF120": {"ref_id": "b120", "title": "Machine-learned and codified synthesis parameters of oxide materials. Scientific Data", "authors": [{"first": "Edward", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Kevin", "middle": [], "last": "Huang", "suffix": ""}, {"first": "Alex", "middle": [], "last": "Tomala", "suffix": ""}, {"first": "Sara", "middle": [], "last": "Matthews", "suffix": ""}, {"first": "Emma", "middle": [], "last": "Strubell", "suffix": ""}, {"first": "Adam", "middle": [], "last": "Saunders", "suffix": ""}, {"first": "Andrew", "middle": [], "last": "Mccallum", "suffix": ""}, {"first": "Elsa", "middle": [], "last": "Olivetti", "suffix": ""}], "year": 2017, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1038/sdata.2017.127"]}}, "BIBREF121": {"ref_id": "b121", "title": "Efficient estimation of word representations in vector space", "authors": [{"first": "Tomas", "middle": [], "last": "Mikolov", "suffix": ""}, {"first": "Kai", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Greg", "middle": [], "last": "Corrado", "suffix": ""}, {"first": "Jeffrey", "middle": [], "last": "Dean", "suffix": ""}], "year": 2013, "venue": "Proceedings of 1st International Conference on Learning Representations", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF122": {"ref_id": "b122", "title": "Unsupervised word embeddings capture latent knowledge from materials science literature", "authors": [{"first": "Vahe", "middle": [], "last": "Tshitoyan", "suffix": ""}, {"first": "John", "middle": [], "last": "Dagdelen", "suffix": ""}, {"first": "Leigh", "middle": [], "last": "Weston", "suffix": ""}, {"first": "Alexander", "middle": [], "last": "Dunn", "suffix": ""}, {"first": "Ziqin", "middle": [], "last": "Rong", "suffix": ""}, {"first": "Olga", "middle": [], "last": "Kononova", "suffix": ""}, {"first": "Kristin", "middle": [], "last": "Persson", "suffix": ""}, {"first": "Gerbrand", "middle": [], "last": "Ceder", "suffix": ""}, {"first": "Anubhav", "middle": [], "last": "Jain", "suffix": ""}], "year": null, "venue": "Nature", "volume": "571", "issn": "", "pages": "", "other_ids": {"DOI": ["10.1038/s41586-019-1335-8"]}}, "BIBREF123": {"ref_id": "b123", "title": "Imagenet: A large-scale hierarchical image database", "authors": [{"first": "Jia", "middle": [], "last": "Deng", "suffix": ""}, {"first": "Wei", "middle": [], "last": "Dong", "suffix": ""}, {"first": "Richard", "middle": [], "last": "Socher", "suffix": ""}, {"first": "Li-Jia", "middle": [], "last": "Li", "suffix": ""}, {"first": "Kai", "middle": [], "last": "Li", "suffix": ""}, {"first": "Li", "middle": [], "last": "Fei-Fei", "suffix": ""}], "year": 2009, "venue": "Proceedings of the 2009 IEEE conference on computer vision and pattern recognition", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF124": {"ref_id": "b124", "title": "Learning to detect unseen object classes by between-class attribute transfer", "authors": [{"first": "C", "middle": ["H"], "last": "Lampert", "suffix": ""}, {"first": "H", "middle": [], "last": "Nickisch", "suffix": ""}, {"first": "S", "middle": [], "last": "Harmeling", "suffix": ""}], "year": 2009, "venue": "Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF125": {"ref_id": "b125", "title": "One-shot learning of object categories", "authors": [{"first": "Li", "middle": [], "last": "Fei-Fei", "suffix": ""}, {"first": "R", "middle": [], "last": "Fergus", "suffix": ""}, {"first": "P", "middle": [], "last": "Perona", "suffix": ""}], "year": 2006, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "volume": "28", "issn": "4", "pages": "594--611", "other_ids": {}}, "BIBREF126": {"ref_id": "b126", "title": "Object classification from a single example utilizing class relevance metrics", "authors": [{"first": "Michael", "middle": [], "last": "Fink", "suffix": ""}], "year": 2004, "venue": "Proceedings of Advances in Neural Information Processing Systems", "volume": "17", "issn": "", "pages": "", "other_ids": {}}, "BIBREF127": {"ref_id": "b127", "title": "Diverse few-shot text classification with 110 multiple metrics", "authors": [{"first": "Mo", "middle": [], "last": "Yu", "suffix": ""}, {"first": "Xiaoxiao", "middle": [], "last": "Guo", "suffix": ""}, {"first": "Jinfeng", "middle": [], "last": "Yi", "suffix": ""}, {"first": "Shiyu", "middle": [], "last": "Chang", "suffix": ""}, {"first": "Saloni", "middle": [], "last": "Potdar", "suffix": ""}, {"first": "Yu", "middle": [], "last": "Cheng", "suffix": ""}, {"first": "Gerald", "middle": [], "last": "Tesauro", "suffix": ""}, {"first": "Haoyu", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Bowen", "middle": [], "last": "Zhou", "suffix": ""}], "year": 2018, "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "volume": "", "issn": "", "pages": "", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "of two given entities by using text resources. This work focuses on relations occurring in materials science. We target factors for material development and visualizes these factors and their relations in a graph formalism. The particular factors and their relations are extracted from thousands of materials science articles. Finally, Chapter 6 concludes the thesis. v ACKNOWLEDGMENT It has been an honor for me to be advised by Professor David McAllester, who has been giving me his outstanding mentorship, his patience and unconditional support from the first day to the last day of my graduate program. Through him, I also got to understand more about how to think and act as a true academic scientist. I would never have come this far without his guidance and encouragement. I would also like to thank Professor Kevin Gimpel for his research collaboration which was a new chapter in my Ph.D. A big thanks to Professor Yutaka Sasaki for the insightful discussions and educational supports that we had and his precise and constructive comments on my thesis. I would also like to thank Professor Makoto Miwa for the insightful discussions that helps me in the early days of my Ph.D. research, and his advice for my Ph.D. thesis. I would like to thank Professor Ikumu Watanabe for the great research collaboration and his insightful advice.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "My deepest gratitude goes to my family for their endless love and support during all these years. I'd like to thank my parents for supporting my decisions throughout all these years. vi 1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.1 Reading Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1.1 Problem Formulations . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.1.2 Reading comprehension task and other question answering tasks . 7 1.1.3 History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.2 Entity and Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.2.1 Knowledge base population . . . . . . . . . . . . . . . . . . . . . . 14 2 Entity-centered reading comprehension dataset . . . . . . . . . . . . . . 16 2.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2 Dataset construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.3 Performance Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3 Analysis of a neural structure in entity-centered reading comprehension 35 3.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 vii 3.2 Emergent Predication Structure . . . . . . . . . . . . . . . . . . . . . . . 42 3.3 Pointer Annotation Readers . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 3.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 4 Relation and entity centered reading comprehension . . . . . . . . . . 51 4.1 Wikihop dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 4.3 Explicit reference transformer . . . . . . . . . . . . . . . . . . . . . . . . 58 4.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 4.4.1 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.4.2 Ablation studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 5 Relation extraction with weakly supervised learning for materials science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 5.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 5.2 Preliminary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 5.3 System description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 5.4 Experiment for relation identification . . . . . . . . . . . . . . . . . . . . 77 5.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 5.6 End-to-end system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 5.7 Conclusions and contribution . . . . . . . . . . . . . . . . . . . . . . . . . 85 5.8 Follow-up work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 6.1 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 viii LIST OF TABLES", "latex": null, "type": "figure"}, "FIGREF2": {"text": "An example of part-of-speech tagging. Each tag indicates a part-ofspeech of each token; DT (determiner), CD (cardinal number), HYPH (hyphen), JJ (adjective), NNP (proper noun, singular), VBD (verb, past tense), and NNS (noun, plural). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . An example of syntactic parsing. Each tag indicates a type of phrase; VP (verb phrase), NP (noun phrase), ADJP (adjective phrase.), S (sentence) An example of dependency parsing. . . . . . . . . . . . . . . . . . . . An example of named entity recognition. Here, person names, place names, and organization names are recognized. . . . . . . . . . . . . . . . . . An example of coreference resolution. Here, two person entities; Robbie Keane and Dimitar Berbatov, are recognized. . . . . . . . . . . . . . . . . . Entities and their relations around \"John McCormick\" in Wikidata. . Plot of e o (a i ) e o (a j ) from the Stanford Reader trained on the CNN dataset, where rows range over i values and columns range over j values. Offdiagonal values have mean 25.6 and variance 17.2 while diagonal values have mean 169 and variance 17.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . The length of each paragraph in Wikihop. . . . . . . . . . . . . . . . 54 xii Figure 4.2 The number of paragraphs for each passage in Wikihop. . . . . . . . . Explicit reference on the Transformer encoder. . . . . . . . . . . . . . The process-structure-property-performance reciprocity . . . . . . . . Sentences containing noun phrases. . . . . . . . . . . . . . . . . . . . Structure of the CNN model. The convolutional layers embed a sentence, and the max pooling and two fully connected layers give a binary probability distribution with a sigmoid function. . . . . . . . . . . . . . . . . . . Precision-recall curve of the logistic regression model. The features are 'bag of words', 'bag of words + stop word removal' and 'bag of unigram + bigram + trigram' . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Precision-recall curve of the SVM model. The features are 'bag of words', 'bag of words + stop word removal' and 'bag of unigram + bigram + trigram' . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Precision-recall curve over the relationship data of the CNN model . . The end-to-end demo system. a) Desired properties and a base material were selected. b) A sample of the generated PSPP design chart. The desired properties were toughness and creep strength, and 'steel' was selected as base material. c) A sentence describing the relation between toughness and carbon content. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 xiii Chapter One", "latex": null, "type": "figure"}, "FIGREF3": {"text": "3 shows an example of dependency parsing.", "latex": null, "type": "figure"}, "FIGREF4": {"text": "4 shows an example of named entity recognition.", "latex": null, "type": "figure"}, "FIGREF5": {"text": "An example of part-of-speech tagging. Each tag indicates a part-ofspeech of each token; DT (determiner), CD (cardinal number), HYPH (hyphen), JJ (adjective), NNP (proper noun, singular), VBD (verb, past tense), and NNS (noun, plural).", "latex": null, "type": "figure"}, "FIGREF6": {"text": "An example of syntactic parsing. Each tag indicates a type of phrase; VP (verb phrase), NP (noun phrase), ADJP (adjective phrase.), S (sentence) 1.1.1 Problem Formulations Multiple reading comprehension tasks with different styles have been studied (see examples in Section 2.1). In these reading comprehension tasks, a machine takes a passage and question", "latex": null, "type": "figure"}, "FIGREF7": {"text": "An example of dependency parsing.", "latex": null, "type": "figure"}, "FIGREF8": {"text": "An example of named entity recognition. Here, person names, place names, and organization names are recognized.", "latex": null, "type": "figure"}, "FIGREF9": {"text": "An example of coreference resolution. Here, two person entities; Robbie Keane and Dimitar Berbatov, are recognized.", "latex": null, "type": "figure"}, "FIGREF10": {"text": "Entities and their relations around \"John McCormick\" in Wikidata.", "latex": null, "type": "figure"}, "FIGREF11": {"text": "Augusto Pinochet (2) Jack Straw (3) Ricardo Lagos Passage: Tottenham won 2-0 at Hapoel Tel Aviv in UEFA Cup action on Thursday night in a defensive display which impressed Spurs skipper Robbie Keane. ... Keane scored the first goal at the Bloomfield Stadium with Dimitar Berbatov, who insisted earlier on Thursday he was happy at the London club, heading a second. The 26-year-old Berbatov admitted the reports linking him with a move had affected his performances ... Spurs manager Juande Ramos has won the UEFA Cup in the last two seasons ... Question: Tottenham manager Juande Ramos has hinted he will allow *** to leave if the Bulgaria striker makes it clear he is unhappy. Choices: (1) Robbie Keane (2) Dimitar Berbatov", "latex": null, "type": "figure"}, "FIGREF12": {"text": "These stories and questions are entirely Passage: Library of Congress Has Books for Everyone (WASHINGTON, D.C., 1964) -It was 150 years ago this year that our nation's biggest library burned to the ground. Copies of all the wriuen books of the time were kept in the Library of Congress. But they were destroyed by fire in 1814 during a war with the British. That fire didn't stop book lovers. The next year, they began to rebuild the library. By giving it 6,457 of his books, Thomas Jefferson helped get it started. The first libraries in the United States could be used by members only. But the Library of Congress was built for all the people. From the start, it was our national library. Today, the Library of Congress is one of the largest libraries in the world. People can find a copy of just about every book and magazine printed. Libraries have been with us since people first learned to write. One of the oldest to be found dates back to about 800 years B.C. The books were written on tablets made from clay. The people who took care of the books were called \"men of the written tablets.\" Question1: Who gave books to the new library?", "latex": null, "type": "figure"}, "FIGREF13": {"text": "The Hanging Gardens, in Mumbai, also known as Pherozeshah Mehta Gardens, are terraced gardens . . . They provide sunset views over the Arabian Sea... Mumbai (also known as Bombay, the official name until 1995) is the capital city of the Indian state of Maharashtra. It is the most populous city in India... The Arabian Sea is a region of the northern Indian Ocean bounded on the north by Pakistan and Iran, on the west by northeastern Somalia and the Arabian Peninsula, and on the east by India ... Question: (Hanging gardens of Mumbai, country, #BLANK#) Candidate answers: a)Iran, b)India, c)Pakistan, d)Somalia, ... Answer: (b)India", "latex": null, "type": "figure"}, "FIGREF15": {"text": "3.2). Letting the t-th hidden state of the passage be h t , the state is a contextual embedding of the t-th token and can be viewed as a vector concatenation h t = [s(\u03a6 t ), s(c t )] where \u03a6 t is a property (or statement or predicate) being stated of a particular constant symbol c t . Here s(\u03a6 t ) and s(c t ) are unknown emergent embeddings of \u03a6 t and c t respectively. A logician might write this as h t = \u03a6 t [c t ]. Furthermore, the question can be interpreted as having the form \u03a8[x] where the problem is to find a constant symbol c such that the passage implies \u03a8[c]. Assuming 36 h t = [s(\u03a6 t ), s(c t )], h q = [s(\u03a8), 0], and e(c) = [0, s(c)], we can rewrite Equation (", "latex": null, "type": "figure"}, "FIGREF16": {"text": "(c) is the subset of the positions where the constant symbol (entity identifier) c occurs. Note that if we identify \u03b1 t with s(\u03a6 t ), s(\u03a8) and assume that s(c), s(c t ) is either 0 or 1 depending on whether c = c t , then Equations (3.3) and (3.4) agree. In explicit reference readers, the hidden state h t need not carry a pointer to c t as the restriction on t is independent of learned representations.", "latex": null, "type": "figure"}, "FIGREF17": {"text": "13). We first present the Stanford Reader as a paradigmatic aggregation reader and the Attention Sum Reader as a paradigmatic explicit reference reader.Aggregation ReadersStanford Reader. The Stanford Reader[2]  computes a bidirectional LSTM[45] representation of both the passage and the question.h = biLSTM(e(p)). (3.5) h q = [fLSTM(e(q)) |q| , bLSTM(e(q)) 1 ].(3.6) In Equations (3.5) and (3.6), e(p) is the sequence of word embeddings e(w i ) for w i \u2208 p and similarly for e(q). The expression biLSTM(s) denotes the sequence of hidden state vectors resulting from running a bidirectional LSTM on the vector sequence s. We write biLSTM(s) i for the i-th vector in this sequence. Similarly fLSTM(s) and bLSTM(s) denote the sequence of vectors resulting from running a forward LSTM and a backward LSTM respectively and [\u00b7, \u00b7] denotes vector concatenation. The Stanford Reader, and various other readers, then compute a bilinear attention over the passage which is used to construct a single weighted vector representation of the passage.", "latex": null, "type": "figure"}, "FIGREF18": {"text": "o (a) is the \"output embedding\" of the answer a. On the CNN/Daily Mail dataset the Stanford Reader learns an output embedding for each of the roughly 550 entity identifiers used in the dataset. For datasets in which the answer might be any word in V, output embeddings must be trained for the entire vocabulary. The reader is trained with log-loss \u2212 log P (a|p, q, A) where a is the correct answer. At test time the reader is scored on the percentage of problems where\u00e2 = a. Memory Networks. Memory Networks [8, 43] use Equations (3.7) and (3.9) but have more elaborate methods of constructing \"memory vectors\" h t not involving LSTMs. Memory networks use Equations (3.7) and (3.9) but replace Equation (3.8) with P (\u00b7|p, q, A) = P (\u00b7|p, q) = softmax w\u2208V e o (w) o. (3.10) Note that Equation (3.10) trains output vectors over the whole vocabulary rather than just those items occurring in the choice set A. This is empirically significant in non-anonymized 39 datasets such as CBT and WDW where choices at test time may never have occurred as choices in the training data. Attentive Reader. The Stanford Reader was derived from the Attentive Reader [1]. The Attentive Reader uses \u03b1 t = softmax t MLP([h t , h q ]) instead of Equation (3.7). Here MLP(x)", "latex": null, "type": "figure"}, "FIGREF19": {"text": "3.11) is similar to Equation(3.10)  in that it leads to the training of output vectors for the full vocabulary rather than just those items appearing in choice sets in the training data. As in memory networks, this leads to improved performance on non-anonymized datasets.Explicit Reference ReadersAttention Sum Reader. In the Attention Sum Reader[3], h and q are computed with Equations (3.5) and (3.6) as in the Stanford Reader but using GRUs rather than LSTMs. The attention \u03b1 t is computed similarly to Equation (3.7) but using a simple inner product \u03b1 t = softmax t h t h q rather than a trained bilinear form. Most significantly, however, Equations(3.8) and (3.9) are replaced by the following where t \u2208 R(a, p) indicates that a reference to the candidate answer a occurs at the position t in p. P (a|p, q, A) = t\u2208R(a,p) \u03b1 t . (3.12) a = argmax a t\u2208R(a,p) \u03b1 t . (3.13)", "latex": null, "type": "figure"}, "FIGREF20": {"text": "question embeddings h q for different values of are computed with different GRU model parameters. Here h h q abbreviates the sequence h 1 h q , h 2 h q , . . . h |p| h q . Note that for K = 1 we have only h 1 q and h 1 as in the Attention Sum Reader. An attention is then computed over the final layer h K with \u03b1 t = softmax t (h K t ) h K q in the Attention Sum Reader. This reader uses Equations (3.12) and (3.13).", "latex": null, "type": "figure"}, "FIGREF21": {"text": "state vector h t as a concatenation [s(\u03a6 t ), s(a t )] where \u03a6 t is a property being asserted of entity a t at the position t in the passage. Here s(\u03a6 t ) and s(a t ) are emergent embeddings of the property and entity respectively, we also think of the vector representation q of the question as having the form [s(\u03a8), 0] and the vector embedding e o (a) of an entity as having the form [0, s(a)]. Remember that the vector embeddings have no semantics as discussed, and they are considered as pointers or semantics-free constant symbols.", "latex": null, "type": "figure"}, "FIGREF22": {"text": "that if e o (a) s(a) was different for each candidate answer a then answers would be biased toward constant symbols where this product was larger. This contradicts the anonymization of entity identifiers, and then all constant symbols must be equivalent.", "latex": null, "type": "figure"}, "FIGREF23": {"text": "e o (a) h t in those cases where t \u2208 R(a, p). The third row shows that this inner product falls off significantly just one word before or after the position of the answer word.", "latex": null, "type": "figure"}, "FIGREF24": {"text": "1 shows that the output vectors e o (a) for different entity identifiers a are nearly orthogonal. The orthogonality of the output vectors is required by Equation (3.19) provided that each output vector e o (a) is in the span of the hidden state vectors h t,p for which t \u2208 R(a, p). Intuitively, the mean of all vectors h t,p with t \u2208 R(a, p) should be approximately equal to e o (a). Empirically this will only be approximately true. Theoretically, Corollary A would suggest that the vector embedding of the constant symbols should have the number of dimensions at least as large as the number of distinct constants. However, it is sufficient that e o (a) s(a ) is small for a = a to make the neural readers work in practice, and this also allows the vector embeddings of the constants to have dimension much smaller than the number of constants. We have experimented with twosparse constant symbol embeddings where the number of embedding vectors in dimension d is 2d(d \u2212 1) (d choose 2 times the four ways of setting the signs of the non-zero coordinates).", "latex": null, "type": "figure"}, "FIGREF25": {"text": "is equivalent to h q e o (a) = 0. Experimentally, however, we cannot expect h q e o (a) to be exactly zero and Equation (3.23) seems to provides a more experimentally meaningful test. The fourth and fifth rows of Table 3.1 is an empirical evidence for Corollary B. The fourth row measures the cosine of the angle between the question vector h q and the hidden state h t averaged over passage positions t at which some entity identifier occurs. The fifth row measures the cosine of the angle between h q and e o (a) averaged over the entity identifiers a.", "latex": null, "type": "figure"}, "FIGREF26": {"text": "in a passage instead of anonymized entity identifiers in the CNN/Daily Mail dataset. Plot of e o (a i ) e o (a j ) from the Stanford Reader trained on the CNN dataset, where rows range over i values and columns range over j values. Offdiagonal values have mean 25.6 and variance 17.2 while diagonal values have mean 169 and variance 17.3.", "latex": null, "type": "figure"}, "FIGREF27": {"text": "then re-train the reader. This jump might be explained by the output embeddings e o (a) to 46 be learned. The output embeddings are semantic word embeddings when the dataset is nonanonymized, but they are semantic-free entity identifiers when the dataset is anonymized.This suppression of semantics may facilitate the separation of the hidden state vector space H into a direct sum S \u2295 E with s(\u03a6) \u2208 S and e o (a), s(a) \u2208 E.One-Hot Pointer Reader. Here, we implement the one-hot pointer to the Stanford Reader. We modify the input embedding and the output softmax of the Stanford Reader.For the input embedding of a passage, let i t be the index of a candidate answer in the choice list if the candidate answer is referred to the t-th token in the passage, otherwise zero. We define an one-hot pointer e (i t ) as an one-hot vector of the index if i t = 0, otherwise the zero vector, i.e., e (0) = 0. Note that a passage in WDW has at most five candidate answers, and we can use a five-dimensional one-hot vector to represent them. Then, we concatenate e (i t ) as additional features to the word embedding e(w t ) for token w t in the passage:e(w t ) = [e(w t ), e (i t )]. (3.24) Then, we replace the input embedding e(w t ) with\u0113(w t ) in the Stanford Reader. For the output softmax, we take the output softmax over some elements of o instead of all elements as follows: p(i|d, q) = softmax i\u2208A [0, e (i)] o, (3.25) where \"0\" stands for a sufficient number of zeroes in order to make the dimensions match and o is computed by Equation (3.7).Even though not shown here, in preliminary experiments, we also tried a fixed set of \"pointer vectors\"-vectors distributed widely on the unit sphere so that for i = j we have that e (i) e (j) is small-instead of one-hot vectors in a case where a choice list has a large number of candidate answers. This reader yields similar performance to the one hot pointer reader while permitting smaller embedding dimensionality.Linguistic Features. We also add linguistic features to each input embeddings; whether the current token occurs in the question; the frequency of the current token in the passage; the position of the token's first occurrence in the passage as a percentage of the passage length;and whether the text surrounding the token matches the text surrounding the placeholder in the question.", "latex": null, "type": "figure"}, "FIGREF28": {"text": "(3.3), and the contextual and question embeddings could be decomposed into a property and candidate answer symbol. For a given passage and question, an aggregation reader computes a score for each token in the passage, which is an inner product between the contextual embedding of the token and the embedding of the question. Then, the aggregation reader predicts the answer by the sum of all contextual embeddings weighted by the score for each token as Equation(3.2). On the other hand, an explicit reference reader used explicit reference information that explicitly gives tokens referring to each candidate answer. For each candidate, the explicit reader computes the sum of scores of tokens referring to the candidate answer as Equation(3.4).", "latex": null, "type": "figure"}, "FIGREF29": {"text": "1 and Figure 4.2 show the distribution of the number of paragraphs for each passage and the length of each paragraph, respectively. Wikihop is closely related to Wikireading, another relation and entity centered reading comprehension dataset created from Wikipedia and Wikidata. Wikipedia is a free online encyclopedia hosted by the Wikimedia Foundation that consists of more than 6 million articles 1 . Wikidata is a collaboratively edited knowledge base hosted by the Wikimedia Foundation 1 https://en.wikipedia.org/wiki/English_Wikipedia Paragraph1: The Hanging Gardens, in Mumbai, also known as Pherozeshah Mehta Gardens, are terraced gardens . . . They provide sunset views over the Arabian Sea . . . Paragraph2: Mumbai (also known as Bombay, the official name until 1995) is the capital city of the Indian state of Maharashtra. It is the most populous city in India . . . Paragraph3: The Arabian Sea is a region of the northern Indian Ocean bounded on the north by Pakistan and Iran, on the west by northeastern Somalia and the Arabian Peninsula, and on the east by India . . . Query: (Hanging gardens of Mumbai, country, ?) Answer candidates: {Iran, India, Pakistan, Somalia, . . . }", "latex": null, "type": "figure"}, "FIGREF30": {"text": "location\", and most entities in Wikidata and entries in Wikipedia are linked to each other. Each instance of Wikireading consists of a passage, question, and answer, and it is from a Wikidata tuple, i.e., each question is a relation in the Wikidata tuple, the passage is the Wikipedia article describing the subject entity, and the answer is the object entity. Wikihop is a reading comprehension dataset constructed from Wikireading, and its passages are carefully selected for multi-hop reading comprehension. The paragraphs are selected on a bipartite graph whose left nodes are entities in Wikidata, and right nodes are paragraphs in Wikipedia. A left entity node is connected to a right paragraph node if and only if its entity is mentioned in the paragraph. Paragraphs on the path between two entities that have a relation in the tuples in Wikidata are used as a passage in a question. The question consists of an entity and the relation on the tuple, and the answer is another entity on the tuple. The paragraphs on the path are used as the passage because the path is likely", "latex": null, "type": "figure"}, "FIGREF31": {"text": "The length of each paragraph in Wikihop.", "latex": null, "type": "figure"}, "FIGREF32": {"text": "The number of paragraphs for each passage in Wikihop.the reasoning chain to achieve the relation between the two entities. Additionally, unlikeWikireading, Wikihop provides a list of candidate answers for each question that helps to avoid the ambiguity of the answer. Thus, Wikihop provides questions that likely require multi-hop reading comprehension, where their answers are inferred from multiple sentences in the passage.", "latex": null, "type": "figure"}, "FIGREF33": {"text": "(4.1) shows, these 55 embeddings scales quadratically with the sequence length.Additionally, a pre-trained Transformer has a limitation on the maximum length of sequences that can be processed due to the number of pre-defined position embeddings. The self-attention structure of Transformer does not have any mechanisms that specify the position of tokens except the position embeddings. A position embedding is a trainable parameter, and each position embedding and a corresponding token embedding are paired and injected into the self-attention layer. Again, the self-attention layer has a geometry free structure; thus, the position embeddings are only geometrical information that Transformer can take. In pre-training, a specific number of position embeddings are used; however, the number might not be enough for some downstream tasks where the pre-trained Transformer needs to read longer sequences.", "latex": null, "type": "figure"}, "FIGREF34": {"text": "limited by the number of pre-trained position embeddings. Reformer[51] introduced locality sensitive hashing to compute the attention. The locality sensitive hashing provides a subset of all tokens in the sequence that likely dominates the attention score. Thus Reformer reduces the quadratic computational complexity. Longformer[52] employs the idea of a convolutional network where each convolutional unit takes only tokens around it. As the convolutional unit does, Longformer computes attentions for each token over several tokens around it. Additionally, Longformer computes a global attention (attention over all tokens in the sequence) for some special tokens so that they claim the global attention helps to take 56 account of a global context and long dependency.", "latex": null, "type": "figure"}, "FIGREF35": {"text": "Explicit reference on the Transformer encoder.", "latex": null, "type": "figure"}, "FIGREF36": {"text": "for para k \u2208 para 0 , para 1 , ... Encode(\u00b7; \u03a6) is a parameterized Transformer that encode a sequence of tokens into a sequence of context aware embeddings, whose parameters are denoted by \u03a6.", "latex": null, "type": "figure"}, "FIGREF37": {"text": "Figure 5.1 The process-structure-property-performance reciprocity", "latex": null, "type": "figure"}, "FIGREF38": {"text": "e j \u2208 bool be the relation between entities e i and e j . The subgraph of PSPP knowledge graph is a set of PSPP charts, e.g., {(e i , e j , r e i ,e j )|e i , e j \u2208 E \u2282 E}. Here r e i ,e j = True if entities are connected in the chart and r e i ,e j = False otherwise (seeFig. 5.1). Let S = {s 0 , s 1 , ...} be the sentences in the scientific articles, and then sentences mentioning entities e i and e j be S e i ,e j \u2282 S. In the task, we find all relations among the entities, i.e., {r e i ,e j |\u2200 e i , e j \u2208 E}.", "latex": null, "type": "figure"}, "FIGREF39": {"text": "The entities are collected from two resources; Scripta Materialia 3 and scientific articles. Scripta Materialia is a journal with a keyword list for helping identify the topic of each article. The keyword list has five sections; 1) Synthesis and Processing; 2) Characterization; 3) Material Type; 4) Properties and Phenomena; and 5) Theory, Computer Simulations, and Modeling. We used keywords in 1) Synthesis and Processing for processing, keywords in 3) Material Type for structure, and keywords in 4) Properties and Phenomena for property. Additional structures are collected from nouns phrases in scientific articles. These noun phrases consisting of multiple NNs (singular nouns, or mass nouns), are collected from a corpus described in Section 5.4 by using Stanford CoreNLP [38], then each noun phrase is classified into structure if it does not contain any words in the keyword list. The phrase containing a keyword is classified as the class of the keyword. For instance, Fig. 5.2 lists two sentences with noun phrases.Here 'phrase_transition' is classified as a structural entity, but 'hardness_distribution' is classified as a property entity, as 'hardness' is in the keyword list. We collected such additional structures because the number of structural entities is significantly greater than those of processing and property entities, and the keyword list is", "latex": null, "type": "figure"}, "FIGREF40": {"text": "Sentences containing noun phrases.", "latex": null, "type": "figure"}, "FIGREF41": {"text": "Structure of the CNN model. The convolutional layers embed a sentence, and the max pooling and two fully connected layers give a binary probability distribution with a sigmoid function.", "latex": null, "type": "figure"}, "FIGREF42": {"text": "is trained on a naive distant supervised approach, where the objective function is maximized for each sentence, max \u03a6 (e i ,e j )\u2208E train s\u2208Se i ,e j log P (r e i ,e j |s), (5.11)", "latex": null, "type": "figure"}, "FIGREF43": {"text": "trained on weakly labeled sentences and predicted a binary relation for a given entity pair as the CNN model did. The models used the bag-of-words feature that indicates whether a word is in a set of sentences. The feature is represented by a sparse binary vector, where an element is one if the corresponding word is in the sentences and zero otherwise. We also explored stop words removal and n-gram features in Fig. 5.4 and Fig. 5.5; however, the effect was limited. Note that the radial basis function (RBF) kernel was used in all SVM models.", "latex": null, "type": "figure"}, "FIGREF44": {"text": "Precision-recall curve of the logistic regression model. The features are 'bag of words', 'bag of words + stop word removal' and 'bag of unigram + bigram + trigram'and low recall if a system returns only a small number of high confidence predictions, and low precision and high recall if a system returns many low confidence predictions. Typically, these are balanced by a hyper-parameter (confidence) of system prediction. Thus, the trajectory of precision and recall pairs is computed with various values of the hyper-parameter, and is called a precision-recall curve.", "latex": null, "type": "figure"}, "FIGREF45": {"text": "Precision-recall curve of the SVM model. The features are 'bag of words', 'bag of words + stop word removal' and 'bag of unigram + bigram + trigram' A test data corresponded with a training data, unaware of the relationships in the test data (Section 5.4). A model was trained on the corresponding training data and scored a pair in the test data to avoid letting the model know the true relationships during training.", "latex": null, "type": "figure"}, "FIGREF46": {"text": "Precision-recall curve over the relationship data of the CNN model5.5 ResultsFigures 5.4 and 5.5 show the precision-recall curves for the baseline models. These figures show various feature representation schemes, such as stop words and n-grams (Section 5.4) on the logistic and SVM models. The logistic model performed well on low recall space,i.e., most confidently predicted positive entity pairs were actually positively related. On the contrary, the performance of the SVM model was poorer in the space but better overall than the logistic model. In both models, the effects of the feature representation schemes were limited.", "latex": null, "type": "figure"}, "FIGREF47": {"text": "6  shows the precision-recall curve of our CNN model. The precision was one when the recall was about 0.4, i.e., roughly speaking half the positive entity pairs were perfectly identified. The performance of the CNN model was superior to that of the baseline models.", "latex": null, "type": "figure"}, "FIGREF48": {"text": "Figure 5.7 The end-to-end demo system. a) Desired properties and a base material were selected. b) A sample of the generated PSPP design chart. The desired properties were toughness and creep strength, and 'steel' was selected as base material. c) A sentence describing the relation between toughness and carbon content.", "latex": null, "type": "figure"}, "FIGREF49": {"text": "developed Propnet consisting of 115 material properties and 69 relationships, and Str\u00f6tgen et al. [85] proposed the Bosche Materials Science Knowledge Base consisting of 40K relational facts for solid oxide fuel cells. Unlike we find mentions, tokens referring an entity, by using heuristic string matching, recently mention-level annotations are available in some tasks for the general scientific domain. For example, SemEval 2017 ScientificIE [86] and SemEval-2018: \"Semantic Relation Extraction and Classification in Scientific Papers\" [87] consists of three tasks; a) mention identification b) mention classification c) mention-level relation extraction, and each mention, the class of each mention, and their relations are labeled in the training data. Additionally,", "latex": null, "type": "figure"}, "FIGREF50": {"text": "... \u03b4c = r\u03c3c/\u03c4 is the characteristic or critical whisker length , f and r ... \u03c4 is the matrix shear strength ... 3 34.2/P ... toughness (\u03b4kcb) and grain ... dvpwhere , d is the matrix ..treatment, the increase of grain size was not obvious because of the heat resistance introduced by ... .2 ) after aging ... .3 ) grain refining, size reduction of ... 6 26.0/N solution strengthening and precipitation strengthening respectively, ..., \u03b4h\u2212p was the yield strength ... 7 24.7/N ...dislocation density in lath martensite matrix due to the high content of element ... 100 steel delayed the recovery process during tempering ..... the effect of ingot grain refinement on the mechanical properties of al profiles which are manufactured through hot working ... 10 -14.1/N ... refining the prior austenitic grain size ... long context ... the mechanical strength and cleavage resistance ... 11 -16.4/N ... enhanced solid solution strengthening and composition homogenization is larger than ... 12 -18.7/N ... as the solution treatment temperature increases to ..., the transformation ... and the formation of rim o phase ... 13 -23.4/N ... during the aging treatment , the rim o phase at the margin of \u03b12 grains become ...", "latex": null, "type": "figure"}, "TABREF0": {"text": "Answer[ID] is an answer selected from the candidate answers. Answer[span] is an answer identified by a span. . . . . . . . . . . . . . . . . . . . . . . . . 8 Table 2.1 Sample reading comprehension problems from our dataset. . . . . . . . 17 Table 2.2 A sample question from Remedia Reading Comprehension Story and Questions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Table 2.3 A sample question from MCTest. . . . . . . . . . . . . . . . . . . . . 20 Table 2.4 A sample question from the CBT dataset. . . . . . . . . . . . . . . . 22 Table 2.5 A sample question from CNN/Daily Mail dataset. . . . . . . . . . . . 23 Table 2.6 A sample question from SQuAD dataset. . . . . . . . . . . . . . . . . 23 Table 2.7 A sample question from Wikihop dataset. . . . . . . . . . . . . . . . 26 Table 2.8 Notable reading comprehension datasets since the 1990s. . . . . . . . 27 ix Table 2.9 Performance of suppressed baselines. * Random performance is computed as a deterministic function of the number of times each choice set size appears. Many questions have only two choices and there are about three choices on average. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Table 2.10 Dataset statistics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Table 2.11 System performance on test set. Human performance was computed by two annotators on a sample of 100 questions. Result marked I is from", "latex": null, "type": "table"}, "TABREF1": {"text": "computed for the Stanford Reader. . . . . . . . . . . . . . . . . . . . . . . . 45", "latex": null, "type": "table"}, "TABREF2": {"text": "1 Sample multi-hop reading comprehension question [5]. . . . . . . . . . 53 Table 4.2 The performance on the development and test data. The performance on the test data is computed by the leader board system of Wikihop. *Training and development data are anonymized. Note that no anonymized test data is provided. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 The model of independent paragraph reads each paragraph independently, and the model of oracle paragraphs takes solely paragraphs mentioning the correct answer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 Table 5.1 Samples of entities obtained by the linguistic rules . . . . . . . . . . . 73 Table 5.2 Entities in the relationship data . . . . . . . . . . . . . . . . . . . . . 78 Table 5.3 Relations in the relationship data . . . . . . . . . . . . . . . . . . . . . 78 Table 5.4 Hyper-parameters of the CNN model . . . . . . . . . . . . . . . . . . . 80 Table 5.5 Sample representative sentences scored by the CNN model. Label P indicates that the entities are positively related in the test relationship data and label N indicates a negative relation. Entities in each sentence are underlined. The score is the v r z 2 of each sentence. . . . . . . . . . . . . . . . . . . . . . . 89 Table 5.6 Source articles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90", "latex": null, "type": "table"}, "TABREF3": {"text": "Table 1.1 shows an example of a reading comprehension question from Who-did-What[7]. Here a machine selects the most appropriate answer to fill the blank in the question from the choice list. To solve the question, the machine needs to understand syntax, including the part-of-speech tags of each token, syntactic and dependency structures; thus, it finds tokens referring candidate answers;", "latex": null, "type": "table"}, "TABREF4": {"text": "Passage: Tottenham won 2-0 at Hapoel Tel Aviv in UEFA Cup action on Thursday night in a defensive display which impressed Spurs skipper Robbie Keane. ... Keane scored the first goal at the Bloomfield Stadium with Dimitar Berbatov, who insisted earlier on Thursday he was happy at the London club, heading a second. The 26-year-old Berbatov admitted the reports linking him with a move had affected his performances ... Spurs manager Juande Ramos has won the UEFA Cup in the last two seasons ... Question: Tottenham manager Juande Ramos has hinted he will allow *** to leave if the Bulgaria striker makes it clear he is unhappy.", "latex": null, "type": "table"}, "TABREF5": {"text": "An example of reading comprehension question. question inTable 1.2, the candidate answers are all viruses mentioned in the passage, and the correct answer is (a)COVID-19. Each dataset has a different algorithm to pick these candidate answers. For example, bAbI[8]  picked all nouns in the passage for the candidate answers, candidate answers in CNN/Daily Mail dataset[1] are all entities in", "latex": null, "type": "table"}, "TABREF6": {"text": ".2, the answer is \"COVID-19\" (string). The evaluation is not trivial and different for each dataset of this style.Reading comprehension tasks are closely related to other question answering tasks because they are essentially question answering problems over a passage, a relatively short text.Thus, reading comprehension tasks and other question answering tasks share many common characteristics in their problem formulation, approaches and evaluation. However, it is worth noting that the goal of reading comprehension tasks is different from the goal of other question answering tasks.Passage: Pregnant women may be at higher risk for severe infection with COVID-19 based on data from other similar viruses, like SARS and MERS, but data for COVID \u2212 19Question: We are lacking for the data of #BLANK# to evaluate the risk of pregnant woman.", "latex": null, "type": "table"}, "TABREF7": {"text": "The process of forming a problem starts with the selection of a question article from the English Gigaword corpus. The question is formed by deleting a person named entity from Passage: Britain's decision on Thursday to drop extradition proceedings against Gen. Augusto Pinochet and allow him to return to Chile is understandably frustrating ... Jack Straw, the home secretary, said the 84-year-old former dictator's ability to understand the charges against him and to direct his defense had been seriously impaired by a series of strokes. ... Chile's president-elect, Ricardo Lagos, has wisely pledged to let justice run its course. But the outgoing government of President Eduardo Frei is pushing a constitutional reform that would allow Pinochet to step down from the Senate and retain parliamentary immunity from prosecution. ...", "latex": null, "type": "table"}, "TABREF8": {"text": "Sample reading comprehension problems from our dataset.", "latex": null, "type": "table"}, "TABREF9": {"text": "Passage: James the Turtle was always getting in trouble. Sometimes he'd reach into the freezer and empty out all the food. Other times he'd sled on the deck and get a splinter. His aunt Jane tried as hard as she could to keep him out of trouble, but he was sneaky and got into lots of trouble behind her back. One day, James thought he would go into town and see what kind of trouble he could get into. He went to the grocery store and pulled all the", "latex": null, "type": "table"}, "TABREF10": {"text": "3 A sample question from MCTest.", "latex": null, "type": "table"}, "TABREF11": {"text": ").CBT vs. 84% for WDW. The 16% error rate for humans on WDW seems to be largely due to noise in problem formation introduced by errors in named entity recognition and parsing.Reducing this noise in future versions of the dataset should significantly improve human performance. Another difference compared to CBT is that WDW has shorter choice lists on average. Random guessing achieves only 10% on CBT but 32% on WDW. The reduction in the number of choices seems likely to be responsible for the higher performance of an", "latex": null, "type": "table"}, "TABREF12": {"text": "5 A sample question from CNN/Daily Mail dataset.", "latex": null, "type": "table"}, "TABREF13": {"text": "6 A sample question from SQuAD dataset.", "latex": null, "type": "table"}, "TABREF14": {"text": "7 A sample question from Wikihop dataset. the dataset might not require general reading comprehension skills.WikiHop[5]  is a reading comprehension dataset aiming for multihop reading comprehension. Multihop reading comprehension is a reading comprehension task where the question cannot be solved by any single sentence in the given passage, but it can be solved by information written in multiple sentences. We call the reading comprehension skill at getting together the information written in multiple sentences as the multihop inference. Similar to Wikireading, each question of Wikihop consists of a subject entity and relation type, but the passage is a set of paragraphs from multiple Wikipedia articles to encourage the multihop inference. Additionally, each question provides candidate answers so that it is multiple-choice question answering task. We describe the detail of the dataset in Section 4.Deep Read datasetSentence selection 3rd to 6th grade material 60 stories \u00d7 5 questionsTable 2.8 Notable reading comprehension datasets since the 1990s.", "latex": null, "type": "table"}, "TABREF15": {"text": "9. The suppression removed 49.9% of the questions.Table 2.10 shows statistics of our dataset after suppression. We split the final dataset into train, validation, and test by taking the validation and test to be a random split of the most recent 20,000 problems as measured by question article date. In this way there is very little overlap in semantic subject matter between the training set and either validation or test. We also provide a larger \"relaxed\" training set formed by applying less baseline suppression (a larger value of k in the optimization). The relaxed training set then has a slightly different distribution from the train, validation, and test sets which are all fully suppressed.", "latex": null, "type": "table"}, "TABREF16": {"text": "Table 2.9 Performance of suppressed baselines. * Random performance is computed as a deterministic function of the number of times each choice set size appears. Many questions have only two choices and there are about three choices on average.Table 2.10 Dataset statistics.", "latex": null, "type": "table"}, "TABREF17": {"text": "11 shows the performance of each system on the test data. For the Attention and Stanford Readers, we anonymized the WDW data by replacing named entities with entity IDs as in the CNN/Daily Mail dataset. We see consistent reductions in accuracy when moving from CNN to our dataset. The Attentive and Stanford Reader drop by up to 10% and the Attention Sum and Gated-Attention readers drop by up to 17%. The ranking of the systems also changes. In contrast to the Attentive/Stanford readers, the Attention Sum/Gated-Attention readers explicitly", "latex": null, "type": "table"}, "TABREF18": {"text": "This dataset is different in a variety of ways from existing large-scale cloze datasets and provides a significant extension to the training and test data for machine comprehension.", "latex": null, "type": "table"}, "TABREF19": {"text": "", "latex": null, "type": "table"}, "TABREF21": {"text": "", "latex": null, "type": "table"}, "TABREF22": {"text": "3.2). In the table, the Stanford Reader achieves just better than 45% on WDW while the Attention Sum Reader can get near 60%. On the other hand, the performance of the Stanford Reader jumps to near 65% when we anonymize WDW and", "latex": null, "type": "table"}, "TABREF23": {"text": "", "latex": null, "type": "table"}, "TABREF24": {"text": "At a very high level, our analysis and experiments support a central role for reference resolution in reading comprehension. Automating reference resolution in neural models, and demonstrating its value on appropriate datasets, would seem to be an important area for future research.", "latex": null, "type": "table"}, "TABREF25": {"text": ".1. In this example the question asks in what country the Hanging Gardens of Mumbai are. Paragraph1 says that the Hanging Gardens of Mumbai are gardens located in Mumbai, and Paragraph2 says thatMumbai is located in India that is a country (Mumbai is a capital city of India). Either of these paragraphs is not enough to infer the answer, India, but both paragraphs are required to infer it. Thus such questions require reading comprehension systems to solve semantic relations over the entire passage, including coreference and inference that is likely difficult to solve. Naturally, the passage consisting of multiple passages is relatively longer than that in other datasets consisting of a single paragraph.", "latex": null, "type": "table"}, "TABREF26": {"text": "Sample multi-hop reading comprehension question", "latex": null, "type": "table"}, "TABREF27": {"text": "The performance on the development and test data. The performance on the test data is computed by the leader board system of Wikihop. *Training and development data are anonymized. Note that no anonymized test data is provided.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>System </td><td>Dev accuracy (%) </td><td>Test accuracy (%)\n</td></tr><tr><td>GA w/C-GRU [60] </td><td>56.0 </td><td>59.3\n</td></tr><tr><td>HDE [59] </td><td>68.1 </td><td>70.9\n</td></tr><tr><td>CFC [57] </td><td>*72.1 </td><td>70.6\n</td></tr><tr><td>DynSAN [47] </td><td>70.1 </td><td>71.4\n</td></tr><tr><td>Entity-GCN [53] </td><td>*71.6 </td><td>71.2\n</td></tr><tr><td>BERT-Para [55] </td><td>72.2 </td><td>76.5\n</td></tr><tr><td>Longformer-base [52] </td><td>75.0 </td><td>-\n</td></tr><tr><td>Longformer-large [52] </td><td>- </td><td>81.9\n</td></tr><tr><td>Our model </td><td>*77.4 </td><td>-\n</td></tr></table></body></html>"}, "TABREF29": {"text": "", "latex": null, "type": "table"}, "TABREF30": {"text": ".1 Samples of entities obtained by the linguistic rules", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Process Structure water quenching carbon dioxide element modeling grain distribution peak temperature particle size distribution rolling texture matrix phase deformation mode \u03b2 titanium alloy microwave sintering \u03b2 grain size plasma sintering solution strength discharge machining pore size </td></tr></table></body></html>"}, "TABREF31": {"text": "Entities in the relationship data", "latex": null, "type": "table"}, "TABREF32": {"text": "Hyper-parameters of the CNN model", "latex": null, "type": "table"}, "TABREF33": {"text": "5 shows some representative sentences scored by the CNN model. A representative sentence is the highest scored sentence in a sentence set S e i ,e j for each entity pair, i.e., a representative sentence is s = argmax s\u2208Se i ,e j P (r = True|s), and it is the sentence that most likely describes the positive relation of the entity pair in the sentence set. The sentence and score indicate the grounds for the decision of the CNN model. The highly scored representative sentences seem to describe the desired relations (sentences 4 and 8) and, interestingly, relations described in the equation were also discovered by the model(sentences 2, 3, and 6). This implies that some important relations tend to be described in an equation. This result also indicates that the relations in which we are interested are significantly different from typical relations in other NLP tasks like 'has_a', 'is_a'.", "latex": null, "type": "table"}, "TABREF34": {"text": "Table 2.8 Notable reading comprehension datasets since the 1990s.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Dataset </td><td>Answer type </td><td>Text resource </td><td>Data size\n</td></tr><tr><td>Deep Read dataset </td><td>Sentence selection </td><td>3rd to 6th grade material </td><td>60 stories \u00d7 5 questions\n</td></tr><tr><td>MCTest </td><td>Multiple choice </td><td>Fictional story </td><td>2640 questions\n</td></tr><tr><td>CNN/Daily Mail </td><td>Multiple choice </td><td>News article </td><td>1.4M questions\n</td></tr><tr><td>Children Book Test </td><td>Multiple choice </td><td>Children Book </td><td>687K questions\n</td></tr><tr><td>WDW </td><td>Multiple choice </td><td>News article </td><td>206K questions\n</td></tr><tr><td>WikiHop </td><td>Multiple choice </td><td>Wikipedia and Wikidata </td><td>51K questions\n</td></tr><tr><td>SQuAD </td><td>Span prediction </td><td>Wikipedia </td><td>100K questions\n</td></tr><tr><td>HotpotQA </td><td>Span prediction </td><td>Wikipedia </td><td>16K-91K questions\n</td></tr><tr><td>TriviaQA NarrativeQA </td><td>Free-form answer Free-form answer </td><td>Wikipedia and Web-page Book and movie script </td><td>96K questions\n47K questions\n</td></tr><tr><td>Wikireading </td><td>Free-form answer </td><td>Wikipedia </td><td>13M questions\n</td></tr></table></body></html>"}, "TABREF35": {"text": "Table 2.9 Performance of suppressed baselines. \u2217 Random performance is computed\nas a deterministic function of the number of times each choice set size appears. Many\nquestions have only two choices and there are about three choices on average.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>Accuracy\n</td></tr><tr><td>Baseline </td><td>Before After\n</td></tr><tr><td>First person in passage </td><td>0.60 </td><td>0.32\n</td></tr><tr><td>Most frequent person </td><td>0.61 </td><td>0.33\n</td></tr><tr><td>n-gram </td><td>0.53 </td><td>0.33\n</td></tr><tr><td>Unigram </td><td>0.43 </td><td>0.32\n</td></tr><tr><td>Random\u2217 </td><td>0.32 </td><td>0.32\n</td></tr></table></body></html>"}, "TABREF36": {"text": "Table 2.11 System performance on test set. Human performance was computed\nby two annotators on a sample of 100 questions. Result marked I is from Hermann\net al. [1], results marked II are from Chen et al. [2], result marked III is from Kadlec\net al. [3], and result marked IV is from Dhingra et al. [4].", "latex": null, "type": "table", "html": "<html><body><table><tr><td>System </td><td>WDW </td><td>CNN\n</td></tr><tr><td>Word overlap </td><td>0.47 </td><td>-\n</td></tr><tr><td>Sliding window </td><td>0.48 </td><td>-\n</td></tr><tr><td>Distance </td><td>0.46 </td><td>-\n</td></tr><tr><td>Sliding window + Distance </td><td>0.51 </td><td>-\n</td></tr><tr><td>Semantic features </td><td>0.52 </td><td>-\n</td></tr><tr><td>Attentive Reader </td><td>0.53 </td><td>0.63I\n</td></tr><tr><td>Attentive Reader (relaxed train) </td><td>0.55\n</td><td>\u00a0</td></tr><tr><td>Stanford Reader </td><td>0.64 </td><td>0.73II\n</td></tr><tr><td>Stanford Reader (relaxed train) </td><td>0.65\n</td><td>\u00a0</td></tr><tr><td>Attention Sum Reader </td><td>0.57 </td><td>0.70III\n</td></tr><tr><td>Attention Sum Reader (relaxed train) </td><td>0.59\n</td><td>\u00a0</td></tr><tr><td>Gated-Attention Reader </td><td>0.57 </td><td>0.74IV\n</td></tr><tr><td>Gated-Attention Reader (relaxed train) Human Performance </td><td>0.60\n84/100 </td><td>0.75+II\n</td></tr></table></body></html>"}, "TABREF37": {"text": "Table 3.1 Statistics to support Equations (3.19) and (3.23). These statistics are\ncomputed for the Stanford Reader.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>\u00a0</td><td>\u00a0</td><td>CNN Dev </td><td>\u00a0</td><td>CNN Test\n</td><td>\u00a0</td></tr><tr><td>\u00a0</td><td>\u00a0</td><td>samples </td><td>mean </td><td>variance </td><td>samples </td><td>mean </td><td>variance\n</td></tr><tr><td>&gt;ht, eo(a)\n&gt;ht, eo(a)\n</td><td>t \u2208 R(a, p) </td><td>222,001 </td><td>10.66 </td><td>2.26 </td><td>164,746 </td><td>10.70 </td><td>2.45\n</td></tr><tr><td>t \u2208 /R(a, p) </td><td>93,072,682 </td><td>-0.57 </td><td>1.59 </td><td>68,451,660 </td><td>-0.58 </td><td>1.65\n</td></tr><tr><td>&gt;ht\u00b11, eo(a)\n</td><td>t \u2208 R(a, p) </td><td>443,878 </td><td>2.32 </td><td>1.79 </td><td>329,366 </td><td>2.25 </td><td>1.84\n</td></tr><tr><td>Cosine(hq, ht), </td><td>\u2203a t \u2208 R(a, p) </td><td>222,001 </td><td>0.22 </td><td>0.11 </td><td>164,746 </td><td>0.22 </td><td>0.12\n</td></tr><tr><td>Cosine(hq, eo(a)), </td><td>\u2200a </td><td>103,909 </td><td>-0.03 </td><td>0.04 </td><td>78,411 </td><td>-0.03 </td><td>0.04\n</td></tr></table></body></html>"}, "TABREF38": {"text": "Table 3.2 Accuracy on Who-did-What dataset. Each result is based on a single\nmodel. Results for neural readers other than NSE are based on replications of those\nsystems. All models were trained on the relaxed training set which uniformly yields\nbetter performance than the restricted training set. The first group of models are\nexplicit reference models and the second group are aggregation models. + indicates\nanonymization with better reference identifier.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Who-did-What Validation (%) </td><td>Test (%)\n</td></tr><tr><td>Attention Sum Reader 59.8 </td><td>58.8\n</td></tr><tr><td>Gated-Attention Reader 60.3 </td><td>59.6\n</td></tr><tr><td>NSE 66.5 </td><td>66.2\n</td></tr><tr><td>Gated-Attention + Linguistic Features+ 72.2 </td><td>72.8\n</td></tr><tr><td>Stanford Reader 46.1 </td><td>45.8\n</td></tr><tr><td>Attentive Reader with Anonymization 55.7 </td><td>55.5\n</td></tr><tr><td>Stanford Reader with Anonymization 64.8 </td><td>64.5\n</td></tr><tr><td>One-Hot Pointer Reader 65.1 </td><td>64.4\n</td></tr><tr><td>One-Hot Pointer Reader + Linguistic Features+ 69.3 </td><td>68.7\n</td></tr><tr><td>Stanford with Anonymization + Linguistic Features+ 69.7 </td><td>69.2\n</td></tr><tr><td>Human Performance - </td><td>84\n</td></tr></table></body></html>"}, "TABREF39": {"text": "Table 4.3 The model of independent paragraph reads each paragraph indepen-\ndently, and the model of oracle paragraphs takes solely paragraphs mentioning the\ncorrect answer.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>System </td><td>Dev accuracy (%)\n</td></tr><tr><td>Independent paragraphs </td><td>69.4\n</td></tr><tr><td>Oracle paragraphs </td><td>96.9\n</td></tr><tr><td>Our model </td><td>77.4\n</td></tr></table></body></html>"}, "TABREF40": {"text": "Table 5.3 Relations in the relationship data", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Relationship type </td><td>Positive </td><td>Negative\n</td></tr><tr><td>Process \u2194 </td><td>Structure 14 </td><td>49\n</td></tr><tr><td>Structure \u2194 Property </td><td>10 </td><td>31\n</td></tr></table></body></html>"}}, "back_matter": []}